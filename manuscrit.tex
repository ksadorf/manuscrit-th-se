\documentclass[symmetric,justified,marginals=raggedouter]{tufte-book}

%\hypersetup{colorlinks}% uncomment this line if you prefer colored hyperlinks (e.g., for onscreen viewing)

%%
% If they're installed, use Bergamo and Chantilly from www.fontsite.com.
% They're clones of Bembo and Gill Sans, respectively.
%\IfFileExists{bergamo.sty}{\usepackage[osf]{bergamo}}{}% Bembo
%\IfFileExists{chantill.sty}{\usepackage{chantill}}{}% Gill Sans

%\usepackage{microtype}

%%
% For nicely typeset tabular material
\usepackage{booktabs}

%%
% For graphics / images
\usepackage{graphicx}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
\graphicspath{{graphics/}}

% The fancyvrb package lets us customize the formatting of verbatim
% environments.  We use a slightly smaller font.
\usepackage{fancyvrb}
\fvset{fontsize=\normalsize}

%%
% Prints argument within hanging parentheses (i.e., parentheses that take
% up no horizontal space).  Useful in tabular environments.
\newcommand{\hangp}[1]{\makebox[0pt][r]{(}#1\makebox[0pt][l]{)}}

%%
% Prints an asterisk that takes up no horizontal space.
% Useful in tabular environments.
\newcommand{\hangstar}{\makebox[0pt][l]{*}}

%%
% Prints a trailing space in a smart way.
\usepackage{xspace}

%%
% Some shortcuts for Tufte's book titles.  The lowercase commands will
% produce the initials of the book title in italics.  The all-caps commands
% will print out the full title of the book in italics.
\newcommand{\vdqi}{\textit{VDQI}\xspace}
\newcommand{\ei}{\textit{EI}\xspace}
\newcommand{\ve}{\textit{VE}\xspace}
\newcommand{\be}{\textit{BE}\xspace}
\newcommand{\VDQI}{\textit{The Visual Display of Quantitative Information}\xspace}
\newcommand{\EI}{\textit{Envisioning Information}\xspace}
\newcommand{\VE}{\textit{Visual Explanations}\xspace}
\newcommand{\BE}{\textit{Beautiful Evidence}\xspace}

\newcommand{\TL}{Tufte-\LaTeX\xspace}

% Prints the month name (e.g., January) and the year (e.g., 2008)
\newcommand{\monthyear}{%
  \ifcase\month\or January\or February\or March\or April\or May\or June\or
  July\or August\or September\or October\or November\or
  December\fi\space\number\year
}


% Prints an epigraph and speaker in sans serif, all-caps type.
\newcommand{\openepigraph}[2]{%
  %\sffamily\fontsize{14}{16}\selectfont
  \begin{fullwidth}
  \sffamily\large
  \begin{doublespace}
  \noindent\allcaps{#1}\\% epigraph
  \noindent\allcaps{#2}% author
  \end{doublespace}
  \end{fullwidth}
}

% Inserts a blank page
\newcommand{\blankpage}{\newpage\hbox{}\thispagestyle{empty}\newpage}

\usepackage{units}

% Typesets the font size, leading, and measure in the form of 10/12x26 pc.
\newcommand{\measure}[3]{#1/#2$\times$\unit[#3]{pc}}

% Macros for typesetting the documentation
\newcommand{\hlred}[1]{\textcolor{Maroon}{#1}}% prints in red
\newcommand{\hangleft}[1]{\makebox[0pt][r]{#1}}
\newcommand{\hairsp}{\hspace{1pt}}% hair space
\newcommand{\hquad}{\hskip0.5em\relax}% half quad space
\newcommand{\TODO}{\textcolor{red}{\bf TODO!}\xspace}
\newcommand{\ie}{\textit{i.\hairsp{}e.}\xspace}
\newcommand{\eg}{\textit{e.\hairsp{}g.}\xspace}
\newcommand{\na}{\quad--}% used in tables for N/A cells
\providecommand{\XeLaTeX}{X\lower.5ex\hbox{\kern-0.15em\reflectbox{E}}\kern-0.1em\LaTeX}
\newcommand{\tXeLaTeX}{\XeLaTeX\index{XeLaTeX@\protect\XeLaTeX}}
% \index{\texttt{\textbackslash xyz}@\hangleft{\texttt{\textbackslash}}\texttt{xyz}}
\newcommand{\tuftebs}{\symbol{'134}}% a backslash in tt type in OT1/T1
\newcommand{\doccmdnoindex}[2][]{\texttt{\tuftebs#2}}% command name -- adds backslash automatically (and doesn't add cmd to the index)
\newcommand{\doccmddef}[2][]{%
  \hlred{\texttt{\tuftebs#2}}\label{cmd:#2}%
  \ifthenelse{\isempty{#1}}%
    {% add the command to the index
      \index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2}}% command name
    }%
    {% add the command and package to the index
      \index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2} (\texttt{#1} package)}% command name
      \index{#1 package@\texttt{#1} package}\index{packages!#1@\texttt{#1}}% package name
    }%
}% command name -- adds backslash automatically
\newcommand{\doccmd}[2][]{%
  \texttt{\tuftebs#2}%
  \ifthenelse{\isempty{#1}}%
    {% add the command to the index
      \index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2}}% command name
    }%
    {% add the command and package to the index
      \index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2} (\texttt{#1} package)}% command name
      \index{#1 package@\texttt{#1} package}\index{packages!#1@\texttt{#1}}% package name
    }%
}% command name -- adds backslash automatically
\newcommand{\docopt}[1]{\ensuremath{\langle}\textrm{\textit{#1}}\ensuremath{\rangle}}% optional command argument
\newcommand{\docarg}[1]{\textrm{\textit{#1}}}% (required) command argument
\newenvironment{docspec}{\begin{quotation}\ttfamily\parskip0pt\parindent0pt\ignorespaces}{\end{quotation}}% command specification environment
\newcommand{\docenv}[1]{\texttt{#1}\index{#1 environment@\texttt{#1} environment}\index{environments!#1@\texttt{#1}}}% environment name
\newcommand{\docenvdef}[1]{\hlred{\texttt{#1}}\label{env:#1}\index{#1 environment@\texttt{#1} environment}\index{environments!#1@\texttt{#1}}}% environment name
\newcommand{\docpkg}[1]{\texttt{#1}\index{#1 package@\texttt{#1} package}\index{packages!#1@\texttt{#1}}}% package name
\newcommand{\doccls}[1]{\texttt{#1}}% document class name
\newcommand{\docclsopt}[1]{\texttt{#1}\index{#1 class option@\texttt{#1} class option}\index{class options!#1@\texttt{#1}}}% document class option name
\newcommand{\docclsoptdef}[1]{\hlred{\texttt{#1}}\label{clsopt:#1}\index{#1 class option@\texttt{#1} class option}\index{class options!#1@\texttt{#1}}}% document class option name defined
\newcommand{\docmsg}[2]{\bigskip\begin{fullwidth}\noindent\ttfamily#1\end{fullwidth}\medskip\par\noindent#2}
\newcommand{\docfilehook}[2]{\texttt{#1}\index{file hooks!#2}\index{#1@\texttt{#1}}}
\newcommand{\doccounter}[1]{\texttt{#1}\index{#1 counter@\texttt{#1} counter}}

% Generates the index
\usepackage{makeidx}
\makeindex


\usepackage{indentfirst}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Customization %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\setcounter{tocdepth}{1}
\setcounter{secnumdepth}{1}

\renewcommand\contentsname{\normalfont \huge Table des matières}

\titlecontents{chapter}%
    [0em]% distance from left margin
    {\vspace{1\baselineskip}\begin{fullwidth}Chapitre }% above (global formatting of entry)
    {\contentslabel{0em} \hspace{1em} \huge $\vert$ \Large}% before w/ label (label = ``Chapter 1'')
    {\hspace{1em}}% before w/o label
    {\hfill\qquad\thecontentspage}% filler and page (leaders and page num)
    [\end{fullwidth}]% after
\titlecontents{section}% FIXME
    [0em] % distance from left margin
    {\vspace{0\baselineskip}\begin{fullwidth} \rmfamily\itshape} % above (global formatting of entry)
    {\hspace*{6em}\contentslabel{2em}} % before w/label (label = ``2.6'')
    {\hspace*{7em}} % before w/o label
    {\normalfont\hfill\qquad\thecontentspage} % filler + page (leaders and page num)
    [\end{fullwidth}] % after

\usepackage{enumitem}
\setlist{leftmargin=20mm}

\usepackage{tikz}
\usetikzlibrary{calc}

\newcommand\tikzmark[1]{%
  \tikz[overlay,remember picture] \coordinate (#1);}
  
 \renewcommand\labelitemi{--}

\usepackage{multirow}

\makeatletter
    \newcommand{\vast}{\bBigg@{3}}
    \newcommand{\Vast}{\bBigg@{3.5}}
    \newcommand{\vastt}{\bBigg@{4}}
    \newcommand{\Vastt}{\bBigg@{11}}
    %%
    %% Size from smallest to largest:
    %%\[ ( \big( \Big( \bigg( \Bigg( \vast( \Vast( \vastt( \Vastt(\]
\makeatother

\usepackage{dpfloat}

\begin{document}

% Front matter
\frontmatter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Titre %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\author{Quentin Lobbé}
\title{\nohyphenation{Archives et Fragments Web}}
\cleardoublepage
{  
  \begin{fullwidth}%
  \thispagestyle{empty} 
  \setlength{\parskip}{\baselineskip}
  \begingroup
  \vspace*{10em}
  \par\noindent\Large{Quentin Lobbé}
  \vspace*{-1em}
  \par\noindent\Huge\textbf{Archives et Fragments Web}
  \par\noindent\nohyphenation\Large{Désagréger les archives Web pour mener une exploration temporelle de traces numériques des migrations}
  \endgroup
  \vfill  
  \par\noindent\nohyphenation Université Paris-Saclay, École doctorale des sciences et technologies de l'information et de la communication.  Thèse pour l'obtention du doctorat de Télécom ParisTech et de l'Université Paris-Saclay.    
  \end{fullwidth}%
}

\blankpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Info Thèse %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\newpage
\begin{fullwidth}
~\vfill
\thispagestyle{empty}
\setlength{\parskip}{\baselineskip}

\par\noindent Thèse présentée par \textbf{\thanklessauthor}\\
LTCI, Télécom ParisTech, Université Paris Saclay \& Inria. Paris, France.\\
quentin.lobbe@telecom-paristech.fr

\par\noindent Sous la direction de :\\
\textbf{Pierre Senellart}, professeur à l'École Normale Supérieure\\
\textbf{Dana Diminescu}, professeure à Télécom ParisTech

\par\noindent Soutenue publiquement à Paris le 9 novembre 2018, devant un jury composé de :\\
\textbf{Bruno Bachimont} (Rapporteur), enseignant-chercheur à l'Université Technologique de Compiègne\\
\textbf{Marc Spaniol} (Rapporteur), professeur à l'Université de Caen Basse-Normandie\\
\textbf{Anat Ben-David}, professeure à l'Open University of Israel\\
\textbf{Dominique Cardon}, professeur associé à Sciences Po Paris\\
\textbf{Bruno Defude}, directeur adjoint de la recherche et des formations doctorales à Télécom SudParis

\par\textit{last modified \monthyear}
\end{fullwidth}
  
\thispagestyle{empty}%
\clearpage%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Remerciements %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

~\vfill
\noindent
\par\noindent Il me demanda de chercher la première page.\\
\noindent Je posais ma main gauche sur la couverture et ouvris le volume de mon pouce serré contre l'index. Je m'efforçais en vain : il restait toujours des feuilles entre la couverture et mon pouce. Elles semblaient sourdre du livre.\\
- Maintenant cherchez la dernière.\\
\noindent Mes tentatives échouèrent de même; à peine pus-je balbutier d'une voix qui n'était plus ma voix :\\
- Cela n'est pas possible.\\
\noindent Toujours à voix basse le vendeur me dit : \\
- Cela n'est pas possible et pourtant cela \textit{est}. Le nombre de pages de ce livre est exactement infini. Aucune n'est la première, aucune n'est la dernière.
\\~\\
\noindent\textit{Jorge Luis Borges - Le livre de sable} 
\vfill
\indent
\newpage
\begingroup
\vspace*{8em}
\huge $\vert$ \huge Remerciements
\vspace*{4em}
\par\normalsize Ici je remercie plein de gens

\par Beaucoup de gens

\par Mais vraiment
\endgroup
\vfill

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Tables %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\tableofcontents

\listoffigures

\listoftables

\mainmatter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Chapitre 1 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction}

\section{Introduction générale}

Ici l'intro de la thèse.

\section{Mise en garde}

\subsection{Penser le passé depuis le présent}

Ici on fait un rapide détour par l'historiographie et les difficultés à parler du passé depuis le présent.

\subsection{Conservation différentielle et nature des archives Web}

Ici on parle de la raréfaction de la matière Web à mesure que l'on remonte le temps et également à mesure que le web fournit du contenu.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Chapitre 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Du Web aux Représentations en Ligne des Diasporas}
\label{chap:2}

\section{Retour aux origines du Web}
\label{sec:2_web}

\section{Le migrant connecté}

\section{Le Web, espace de communication et d'organisation}

The Web is the main publishing application of the Internet.
As such, it consists mainly of the combination of three standards, the URI (Berners-Lee 1994) defining a naming space for object on the Internet, 6 HTTP (Fielding et al. 1999) defining a client–server interaction protocol using hyperlinks at its core, and HTML (Berners-Lee and Connolly 1995) an SGML DTD that defines the layout rendering of pages in browsers.
The implementation of these three standards enables any computer connected to the Internet to be- come a publishing system.

But the fact that it is actionable on the Web changes the way references are used by fragmenting content to smaller addressable pieces and overall favoring transversal navigation and access to content which in return, deeply changes the nature of writing as well as reading (Aarseth 1997; Landow 1997; Bolter 2001).

Géopolitique de l'hypertexte

Le web est un digital cultural artifact (Lyman and Kahle 1998)

 the Web does, to a large extend, re-use previous forms of publishing 12 (Crowston and Williams 1997; Eriksen and Ihlström 2000; Shepherd and Polanyi 2000), it also invents new ones.
 
This characterization of the Web as a distributed hypermedia openly and permanently authored at a global scale entails that Web archiving can only achieve preservation of limited aspects of a larger and living cultural artifact.

the interconnectedness of content is a major quality of the Web that raises issues when it comes to archiving.

\section{L'Atlas e-Diasporas}
\label{sec:2_atlas}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Chapitre 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Archiver le Web}

\noindent Face à la disparition totale ou partielle des sites Web recensés par l'atlas e-Diasporas (Chapitre \ref{chap:2}), il a été décidé de lancer une campagne d'archivage afin de préserver cet héritage numérique et d'anticiper, par là même, la tenue de futures recherches. Sans cette initiative, mes travaux de thèse n'auraient pas pu exploiter et questionner les traces d'un Web aujourd'hui passé. 

De part la nature même du médium, le Web demande de penser et de mettre en place un archivage particulier, basé sur des techniques de collectage dédiées. Dans ce chapitre, nous évoquerons la genèse de l'archivage du Web qui, au tournant des années 2000, a connu un essor mondial, mobilisant nombre d'acteurs et d'institutions. Nous introduirons ensuite, d'un point de vue technique, les principales méthodes de sélection, collecte et stockage des corpus à archiver. Nous présenterons enfin les contours des archives e-Diasporas à proprement parler. Ses particularités et ses caractéristiques. Sa durée et son étendue. \\

\noindent Bien que cette thèse se concentre sur l'exploration d'archives Web déjà existantes, il nous semble important d'évoquer la façon dont ces dernières sont constituées en amont afin de mieux saisir les biais analytiques (Chapitre \ref{chap:4}) qui motiveront la présentation de notre principale contribution (Chapitre \ref{chap:5}). Ce faisant, les éléments que nous nous apprêtons à présenter s'appuieront principalement sur l'ouvrage de J.~Masanes : "\textit{Web Archiving}" \citep{masanes_web_2006} qui reste encore aujourd'hui une référence.

\section{Vingt ans d'archivage du Web}
\label{sec:3_20ans}

\noindent En Octobre 2016, se tenait à la Bibliothèque Nationale de France (BNF) une grande conférence anniversaire réunissant, pour les 20 ans de l'archivage du Web\footnote{\url{http://www.bnf.fr/fr/professionnels/anx_journees_pro_2016/a.jp_161122_23_archivage_web.html}}, les acteurs français de la pratique. Alors qu'était évoqués les conditions du partage du dépôt légal du Web national entre la BNF et l'INA, il a été rappelé qu'à l'origine chacune des deux institutions souhaitait se voir attribuer la pleine gestion de ce dépôt. L'INA mettait en avant ses compétences techniques acquises en archivant les flux audiovisuels nouvellement introduits dans le paysage culturel. La BNF, pour sa part, s'appuyait sur son expérience pluricentenaire de préservation du patrimoine\footnote{\url{http://multimedia.bnf.fr/video/prof/161123_10_dl_web.mp4}}. 

Cette querelle initiale et son dénouement (la cotutelle du dépôt légale) sont à l'image de l'histoire même de l'archivage du Web: la conjugaison d'une tradition longue de sauvegarde des savoirs et d'un ensemble de techniques de collecte nouvellement pensées pour cet objet complexe qu'est le Web, le tout porté par une poignée de pionniers.

\subsection{Préserver la mémoire collective}

\noindent L'archivage du Web s'inscrit dans la tradition longue des techniques d'élaboration et de conservation de la mémoire collective. Tradition qui remonte aux origines même de l'humanité où technique et mémoire se trouvaient étroitement liées. 

A. Leroi-Gourhan fait émerger, de l'étude de séries d'objets (silex taillés, percuteurs, harpons, etc) et de figures préhistoriques (gravures et peintures des grottes ornées), une ligne de rencontre entre technique et mémoire \citep{leroi-gourhan_geste_1964}. Le préhistorien décrit la technique comme un système évolutif, soumis aux lois générales de la technologie et apparaissant comme transversal à des cultures parfois diverses et éloignées\footnote{
Leroi-Gourhan associe les formes animales des grottes ornées à des signes, réalisant des couplages basés sur l'observation (comptages et statistiques) de dizaines de cavités. Il cherche à établir une échelle évolutive des styles pariétaux, transversale aux premiers âges de l'Europe de l'Ouest \citep{leroi-gourham_art_1984}}. La technique est chargée, en elle même, de l'histoire passée des continuités, ruptures et transformations technologiques dont elle est l'aboutissement à un instant t.   

Avec Leroi-Gourhan, la technique devient mémoire. Elle peut en être chargée et/ou être conçue à dessein de la conserver. Involontairement, le silex taillé porte en lui la trace de l'homme qui l'a élaboré. Lorsque le tailleur finit par mourir, son geste continue à s'extérioriser à travers l'outil qui demeure. Précieux indice pour celui qui vient à sa suite ou pour l'archéologue qui, des millénaires après, saura grâce à cet objet assembler les traces fragmentées d'une pratique passée. Mais l'homme aurait aussi très bien pu choisir, en conscience, d'inscrire son expérience individuelle sur des supports de mémoire dédiés. L'écriture est ainsi l'une des premières techniques de la mémoire, utilisée par l'humanité depuis le néolithique. L'écriture est en cela une \textit{mnémotechnologie} \citep{stiegler_leroi-gourhan:_1998}. 

Poursuivant son évolution, l'humanité développe plus avant les techniques de transmission des savoirs pour sélectionner et agréger ses expériences individuelles en une mémoire collective. Des espaces et des structures voient le jour, appuyés par divers pouvoirs politiques ou religieux, avec le double objectif de préserver et d'administrer l'héritage collectif. J. Derrida décrit ainsi le geste d'archiver comme un "\textit{geste de pouvoir}" \citep{derrida_trace_2014}. Choisir ce que l'on garde ou non dans les archives ne peut être que le fruit d'une hégémonie, d'une hiérarchie et "\textit{d'un certain nombre d'opérations de pouvoir}" rendues légitimes par une institution. L'État est ainsi caractérisé par \textit{sa capacité d'accumuler, contrôler et exploiter la mémoire collective} \citep{stiegler_etat_1991}, capacité dont on retrouve divers incarnations au court de l'histoire :

\begin{itemize}[leftmargin=*]  
\item Au IVe millénaire av. J.C, les tablettes d'argiles étaient accumulées par les mésopotamiens pour constituer les premières bibliothèques
\item Entre 535 et 555, Cassiodore pense le Monastère de Vivarium comme un lieu de transmission où, pour la première fois, seraient associés culture savante et christianisme
\item François Ier crée le dépôt légal\footnote{"\textit{Nous avons délibéré de faire retirer, mettre et assembler en notre librairie toutes les livres dignes d'être vues qui ont été ou qui seront faites, compilées, amplifiées, corrigées et amendées de notre tems}", extrait de l'ordonnance royale \citep{dougnac_depot_1960}} en France, par l'ordonnance royale du 28 décembre 1537, à des fins de préservation culturelle mais également de contrôle politique
\end{itemize}

\noindent Les siècle passent et les archives s'adaptent à la transformation des supports de mémoire et à l'émergence de formes nouvelles d'enre\-gistrement. Avec l'arrivée des technologies analogiques\footnote{Cinématographie, photographie, radiodiffusion, etc}, il faut désormais capter et archiver des flux d'images et de sons, ce qui conduira en France à la création de l'Institut National de l'Audiovisuel (INA) en 1974. L'apparition du numérique\footnote{Bases de données, logiciels, interfaces, etc} marque la dernière étape de ce cheminement en ouvrant la voie à un renouveau des formes de lecture et d'étude des archives. L'accès à distance de documents numérisés facilite leur consultation, mais il devient également possible de les qualifier, de les annoter ou de les mettre en relation, et ce, de manière large voire exhaustive \citep{borgman_digital_2000} :

\begin{itemize}[leftmargin=*]  
\item En 1971, le projet Gutenberg commence à collecter des copies numéri\-ques (recopiées et tapées \textit{à la main}) d'ouvrages du domaine public
\item Le Thesaurus Linguae Graecae cherche, depuis 1972, à numériser la plupart des textes littéraires rédigés en grecs ancien et toujours subsistants
\end{itemize}

\noindent Mais si le numérique permet aujourd'hui de revisiter des ressources anciennement archivées, il est aussi créateur d'objets nativement numé\-riques tout autant porteurs d'un héritage à préserver. Le web en est la parfaite illustration.

\subsection{Un héritage numérique}

\noindent L'archivage du Web débute à la fin des années 90, et plus précisément en 1996 lorsque se développent les premières initiatives de préservation du Web, soit 4 années à peine après la publication de la première page sur la toile~(Section \ref{sec:2_web}). La National Library of Australia est ainsi à l'initiative du projet Pandora\footnote{\url{http://pandora.nla.gov.au/}} qui vise à archiver les publications en ligne australiennes sur la base d'une collecte sélective et continue de sites Web australiens. La Swedish Royal Library, quant à elle, lance le projet Kulturarw3\footnote{\url{https://web.archive.org/web/20040206225053/https://www.kb.se/kw3}} qui s'essaye à une collecte "\textit{intégrale}" et espacée dans le temps des sites du Web suédois \citep{arvidson_kulturarw3_2000}.

Mais c'est avec la création d'Internet Archive par B. Kahle la même année \citep{kahle_preserving_1997}, que s'écrit véritablement la première page de l'histoire des archives du Web. Ingénieur et activiste, Kahle s'inspire de la Bibliothèque d'Alexandrie pour motiver la création d'une organisation à but non lucratif afin de rendre accessible au plus grand nombre le passé du Web\footnote{\url{https://archive.org/}}. Utilisant un crawler développé pour le compte de son autre société Alexa Internet, Kahle revendiquait, dans les premières années de la collecte, être capable d'archiver au moins une fois tous les deux mois chacun des sites de l'ensemble du Web \citep{mohr_introduction_2004}. La revente d'Alexa au groupe Amazon en 1999 va lui permettre de pérenniser financièrement Internet Archive, qui depuis ce temps n'a eu de cesse d'archiver le Web.

Ces pionniers de l'archivage sont rapidement suivis par la Finlande en 1997, le Danemark en 1998 et d'autres pays nordiques rassemblés autour du projet NWA\footnote{Nordic Web Archive} \citep{hallgrinsson_nordic_2003}. En 2003, la publication par l'UNESCO de la \textit{Charte sur la conservation du patrimoine numérique} \citep{unesco_charter_2003} marque un nouveau tournant pour l'archivage du Web en reconnaissant la valeur universelle d'une telle démarche et l'urgence face à la disparition potentiel de tout, ou d'une partie, de l'héritage numérique mondial : \\

\begin{fullwidth}
"\textit{Le  patrimoine  numérique  mondial  risque  d'être  perdu  pour  la  postérité.  Les  facteurs  qui  peuvent  contribuer  à  sa  perte  sont  l'obsolescence  rapide  du  matériel  et  des  logiciels  qui  servent  à  le  créer,  les  incertitudes  concernant  les  financements,  la  responsabilité  et  les  méthodes  de  la  maintenance  et  de  la  conservation et l'absence de législation favorable à sa préservation. L'évolution des attitudes n'a pas suivi celle des technologies. L'évolution numérique a été trop rapide et trop coûteuse pour que les pouvoirs publics et les institutions élaborent en temps voulu et en connaissance de cause des stratégies de conservation. La menace qui plane sur le potentiel économique, social, intellectuel et culturel du patrimoine, pierre angulaire de l'avenir, n'a pas été pleinement saisie.}" 

\noindent(Charte sur la conservation du patrimoine numérique, Article 3)\\
\end{fullwidth}

\noindent Pour de nombreuses bibliothèques nationales, la charte de l'UNESCO fait l'effet d'un accélérateur (Figure \ref{fig:date-initiative}). Les institutions sont encouragées dès 2003 à archiver leur Web national \citep{gomes_survey_2011}. Mais notons ici que la notion de Web national reste discutable \citep{abiteboul_first_2002}, il s'agira souvent de crawler le Web en fonction d'une extension de nom de domaine donnée (.fr, .jp, .uk, etc), extension qui ne couvre pas exhaustivement l'ensemble des sites associés à un domaine national précis, elle est plutôt à considérer comme une borne inférieure de celui-ci \citep{koehler_analysis_1999}. Le cas des corpus de l'Atlas e-Diasporas en est un très bon contre-exemple, nombre de sites migrants possédant une extension générique (.com, .net) ou correspondant au pays d'accueil plutôt qu'au pays d'origine \citep{leclerc_cyberespace_2012}.  

\begin{figure}%
  \includegraphics[width=\linewidth]{graphics/date-initiative}
  \caption{Évolution cumulée du nombre d'initiatives d'archivage du Web par année de création (source de données \citep{gomes_survey_2011} et Wikipédia \url{https://en.wikipedia.org/wiki/List_of_Web_archiving_initiatives})}
  \label{fig:date-initiative}
\end{figure} 

\noindent Ces nouveaux acteurs de l'archivage du Web peuvent être classés suivant la terminologie introduite par J. Masanes \citep{masanes_web_2006}, entre initiatives publiques ou privées, poursuivant un but lucratif ou non. L'accès aux corpus archivés peut être entièrement public ou restreint et limité, en ligne ou physique (machine de consultation dans une bibliothèque). Par exemple, Internet Archive est une initiative à but non lucratif, avec un accès public à l'ensemble de ses corpus, en ligne depuis 2001 et physique depuis 2002\footnote{La Wayback Machine est officiellement lancée en 2001. Avant cette date les corpus d'Internet Archives n'étaient pas accessibles au public. En 2002, une copie intégrale des archives est consultable à la Bibliotheca Alexandrina, en Égypte}. 

Une autre manière de catégoriser ces initiatives est de regarder la nature des corpus archivés. Nous avons déjà évoqué les corpus territoriaux censés capturer les contours d'un Web national. Cette notion peut être également transposée à plus fine échelle : celle d'une région ou d'une ville \citep{boudrez_archiving_2002}. Un corpus d'archive peut être conçu pour cibler une thématique donnée, souvent  centrée sur des événements politiques \citep{voerman_archiving_2002,schneider_building_2003} : élections, référendums, etc. Certaines initiatives s'affranchissent même de barrières géographiques devenues contraignantes en préservant des sites de domaines nationaux étrangers \citep{gomes_introducing_2009}.

Enfin, il est possible de voir les corpus d'archives Web par rapport à l'utilisation que l'on en fait. Certains sont ouvertement tournés vers la consultation publique (Internet Archives, à nouveau), d'autres sont constitués à des fins universitaires (on pense au corpus japonais WARP\footnote{\url{http://warp.da.ndl.go.jp/search/}} de la National Diet Library). La British Library, de son côté, fait de ses archives Web une utilisation détournée, voire cachée, en chargeant la version passée d'une page Web de son site si cette dernière n'est momentanément ou définitivement plus accessible à un visiteur\footnote{\url{https://www.bl.uk/collection-guides/uk-web-archive}}. L'un des plus gros corpus d'archives Web reste en revanche celui détenu par Google qui permet d'accéder depuis le cache\footnote{\url{https://fr.wikipedia.org/wiki/Mémoire_cache}} de son moteur de recherche à une version précédemment crawlée d'une page. Notons, pour terminer ce tour d'horizon, que même si elles sont nombreuses de part le monde, les initiatives d'archivage du Web sont historiquement et géographiquement le fait d'états occidentaux (Figure \ref{fig:map-initiative}). Les continents sud Américain et Africain (hormis la la Bibliotheca Alexandrina) sont absents de ce paysage, rendant encore plus précieux les corpus transnationaux archivés par l'Internet Archives et l'Atlas e-Diasporas.  


\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/map-initiative}
  \caption{Carte des initiatives d'archivage du Web par pays et années de création (source de données \citep{gomes_survey_2011} et Wikipédia \url{https://en.wikipedia.org/wiki/List_of_Web_archiving_initiatives})}
  \label{fig:map-initiative}
\end{figure*} 


\noindent En vingt années d'existence, les archives du Web ont agrégé autour d'elles une communauté de chercheurs et d'ingénieurs participant à sa promotion. L'International Internet Preservation Consortium (IIPC) est fondé en 2003 dans l'idée de proposer des rapports et des suivis réguliers de l'archivage\footnote{\url{http://internetmemory.org/images/uploads/Web\_Archiving\_Survey.pdf}} et des workshops sont organisés (l'IWAW, International Web Archiving Workshops). Mais le fait est de constater que la dynamique visible à la fin des années 2000 est en train de se tasser. En 2017, une seule et unique initiative a vu le jour (en Belgique autour du projet Promise\footnote{\url{https://promise.hypotheses.org/}}). Les vastes projets de recherche et d'exploitation des archives Web que sont ARCOMEM \citep{risse_arcomem_2014}, LAWA \citep{spaniol_tracking_2012} et LIWA \citep{denev_sharc:_2009} n'ont pas trouvé de successeurs et, en 2018, l'Internet Memory Foundation a annoncé stopper ses activités d'archivage. Enfin, rappelons que la position centrale d'Internet Archive dans cet écosystème ne la met pas à l'abri d'une possible disparition. En 2016, B. Kahle prévoyait (notamment en réaction à l'élection de D. Trump à la tête des États Unis) de déplacer une nouvelle copie intégrale d'Internet Archive au Canada\footnote{\url{http://blog.archive.org/2016/11/29/help-us-keep-the-archive-free-accessible} \url{-and-private/}}. Et n'oublions pas que, jusqu'à présent, la survie d'Internet Archive est et reste étroitement liée à son fondateur, la question de sa succession et de la pérennité du corpus après sa mort devra être rapidement abordée.  

Mais alors que les institutions semblent s'en détourner, le futur des archives Web viendra peut être de ceux que M. Graham, directeur de la WayBack Machine, nomme les "\textit{rogue archivists}"\footnote{\url{https://youtu.be/33_fnPwaEM0}}. S'inscrivant dans les pas d'A. Schwartz\footnote{En 2011, Schwartz hacke la base de données de l'éditeur JSTOR afin de "\textit{libérer}" plusieurs millions d'articles scientifiques payants, dont une part importante appartenait au domaine public (voir \textit{The Internet Own Boy} réalisé par B. Knappenberger en 2014 \url{https://archive.org/details/TheInternetsOwnBoyEsp}). Schwartz est à l'origine de contributions considérables, à la fois sur des aspects techniques du Web (développeur du format de flux RSS), mais également sur la question plus politique de l'accès universel aux connaissances.}, les rogues archivits sont des activistes et libristes militants s'appropriant politiquement la question des archives. Soit qu'ils considèrent le Web et son contenu comme un commun de l'Humanité \citep{coriat_retour_2015}, soit qu'ils voient dans les archives un moyen de faire perdurer la mémoire de minorités opprimées \citep{de_kosnik_rogue_2016}. Ils administrent de manière autonome certains des 7000 crawlers qui alimentent quotidiennement Internet Archive. Encore mineure, au regard des volumes globaux d'archives Web, il est néanmoins possible de déceler la trace de leurs contributions dans les travaux d'A. Ben David \citep{ben-david_internet_2018} qui révèle que les archives du Web Nord Coréen (présentes dans le corpus d'Internet Archive) n'ont pas été collectées par des crawlers institutionnels mais par des crawlers indépendants, non assujettis à la géopolitique des proxys.       

\subsection{Le cas des archives Françaises}

\noindent En France, l'archivage du Web est l'aboutissement singulier de dix années d'expérimentations techniques et de construction d'un cadre législatif inédit. Aujourd'hui, deux institutions se partagent le périmètre du dépôt légal du Web : la Bibliothèque Nationale de France (BNF) et l'Institut National de l'Audiovisuel (INA). 

Crée par François 1er, le \textit{dépôt légal} est l'obligation pour tout éditeur (imprimeur, producteur, importateur, etc) de déposer chaque document dont il a la charge (en France) à la BNF ou auprès de l'organisme le plus adapté à la nature particulière de ce document. Tout ce qui se publie et s'édite en France est donc directement collecté par la BNF. L'INA, quant à elle, administre les archives radio et télé. Elle fut initialement créée pour en faire une exploitation commerciale et destinée aux professionnels de l'audiovisuel. L'État français est l'un des premiers états au monde a avoir posé la question des conditions de la mémoire culturelle et patrimoniale du Web. Le Web devait rentrer dans le périmètre du dépôt légal et c'est ainsi que furent posée les bases d'un futur \textit{dépôt légal du Web}.

Les tractations commencent officiellement en 2001. En s'appuyant sur la directive européenne 2001/29/EC\footnote{\url{https://en.wikipedia.org/wiki/Copyright\_Directive}}, dite \textit{Information Society Directive}, l'Assemblée Nationale ouvre au débat la discussion du \textit{Projet de loi sur la société de l'information}\footnote{\url{http://www.assemblee-nationale.fr/11/projets/pl3143.asp}}. Cette loi vise à adapter le droit français aux NTIC en matière de libertés de communication, de commerce en ligne, mais également de droit d'auteur. De ces débats découle, en 2006, l'adoption de la loi DADVSI\footnote{\url{https://www.legifrance.gouv.fr/affichTexte.do?cidTexte=JORFTEXT000000266350}} relative \textit{au droit d'auteur et aux droits voisins dans la société de l'information} qui définit le cadre légale des archives Web à venir :\\

\begin{fullwidth}

"\textit{Les logiciels et les bases de données sont soumis à l’obligation de dépôt légal dès lors qu’ils sont mis à disposition d’un public par la diffusion d’un support matériel, quelle que soit la nature de ce support. Sont également soumis au dépôt légal les signes, signaux, écrits, images, sons ou messages de toute nature faisant l’objet d’une communication au public par voie électronique.}" (Loi DADVSI, Article 21)\\

\end{fullwidth}

\noindent La BNF et l'INA souhaitant toutes deux se voir confier le plein contrôle du dépôt légal de Web par l'État, les équipes de J. Masanès (BNF) et de T. Drugeon (INA) se lancent l'une comme l'autre dans la course à l'archivage dès tournant des années 2000, c'est à dire bien en amont de tout arbitrage politique. Comme nous le verrons dans la section suivante (Section \ref{sec:3_constituer}), la masse de travail à mettre en place pour débuter une collecte est considérable, l'histoire de l'archive du Web en France est donc tout autant l'aboutissement d'une volonté politique que le fruit d'années de recherches et développements. D'un point de vue purement technique, les deux institutions suivent des directions divergentes : la BNF et J.Masanès s'associent à la définition du format d'archivage WARC, l'INA et T. Drugeon créent le format DAFF et développent un crawler indépendant\footnote{Nous discuterons dans la section \ref{sec:3_constituer} des aspects techniques des divers formats d'archivage}. L'INA lance sa collecte de sites Web de manière expérimentale en 2009 alors que l'État s'oriente vers un partage du dépôt légale : une solution à deux têtes. Le cadre de cette partition est défini par le décret du 19 Décembre 2011\footnote{\url{https://www.legifrance.gouv.fr/affichTexte.do?cidTexte=JORFTEXT000025002022&categorieLien=id}} qui établit que :

\begin{itemize}[leftmargin=*]  
\item La BNF archivera l'ensemble du domaine national français et d'outre-mer au moins une fois par an. Par là, sont identifiés tous les sites Web en .fr ainsi qu'une liste blanche de sites en .com, .org, .net, etc "\textit{édités par des personnes physiques ou morales domiciliées en France}".
\item L'INA archivera un sous ensemble thématique du Web français centré sur les sites dit \textit{médias} (sites des services des médias audiovisuels, Web TV et Web radios, programmes radio et télé, professionnel de l'audiovisuel, etc). La fréquence de collecte sera variable et adaptée à la nature même des mises à jour de ces sites (chaque jour, semaine, mois, etc)
\end{itemize}

Si le périmètre de la collecte de la BNF reste classique au regard de ses consœurs mondiales, le corpus archivé par l'INA, lui, est tout à fait singulier. La collecte se concentre sur un jeu de 14.000 sites Web médias (sélectionnés manuellement). Seulement $30\%$ d'entre eux ont une extension .fr contre $50\%$ en .com \citep{drugeon_technical_2005}. L'INA intègre à son corpus des sources vidéos (de youtube et dailymotion dès 2010), des flux RSS, des Tweets (depuis 2014), etc. Les deux institutions offrent également la possibilité à des chercheurs de constituer des corpus tiers et portés sur une thématique précise : l'ANR Web90 est montée en partenariat avec la BNF\footnote{\url{https://web90.hypotheses.org/}} autour des premières années du Web français \citep{schafer_web_2016}, l'INA réalise une collecte dédiée aux attentats de Paris fin 2015\footnote{\url{https://asap.hypotheses.org/173\#more-173}}. L'INA est enfin la seule des deux institutions à avoir encore aujourd'hui une équipe technique dédiée à la recherche et à l'exploitation de ses archives Web.

Les corpus de la BNF et de l'INA se veulent donc complémentaires. Ils appartiennent à la catégorie des initiatives publiques mais n'offrant qu'un accès physique aux contenus archivés : il n'y a pas de portail en ligne de consultation des archives. Le chercheur doit se déplacer dans l'un des 31 centres locaux de l'INA ou, s'il est a Paris, il reste possible d'accéder aux deux corpus depuis la BNF.

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/ticket-bnf}
  \vspace*{0.2cm}  
  \caption{Demande d'accréditation pour accéder aux zones de consultations des archives Web à la BNF (site François Mitterand)}
  \label{fig:ticket-bnf}
\end{marginfigure} 

\begin{center}
	***
\end{center}

\noindent Je reprend ici, le \textit{je} pour parler d'une expérience personnelle. Courant 2017, je me suis rendu à la BNF (site François Mitterrand) afin d'y consulter les archives de l'INA et de la BNF et tester les modalités d'accès aux corpus. Ce récit ne vaut pas généralité, mais doit être pris comme un témoignage d'une exploration d'une heure trente dans la bibliothèque avant de trouver les archives. Je pensais tout d'abord (en me fiant aux indications du site Web) qu'il était possible de consulter les archives depuis le réseaux Wifi du lieu. Après divers échecs, les bibliothécaires m'ont progressivement fait passer d'interlocuteur en interlocuteur jusqu'à finalement me faire accéder (moyennant une demande d'accréditation, Figure \ref{fig:ticket-bnf}) à l'une des salles du Rez-de-Jardin de la BNF (Figure \ref{fig:map-bnf}). Là les archives de l'INA ne sont consultables que depuis une poignée de postes labellisés \textit{Inathèque}. Les archives de la BNF, elles, sont accessibles depuis l'ensemble des  machines de la zone. Une fois connecté, un moteur de recherche classique nous permet de faire des recherches par URL (pour la BNF) et plein texte (pour l'INA), il n'est en revanche pas possible de sauvegarder ses recherches ou de les exporter d'une quelconque manière. Je me suis donc servi de mon téléphone pour photographier les pages Web qui m'intéressaient. Je ne cherche pas ici à pointer du doigt ou accuser. Au contraire j'ai été étonnamment surpris de voir que la consultation des archives Web à Paris relève d'une véritable expédition et je remercie les bibliothécaires d'avoir finalement su me guider. Mais s'il ne faut pas faire grief de leur méconnaissance, force est de constater que nous ne devons pas être très nombreux à consulter les archives Web là bas et que celles ci ne sont pas particulièrement mise en avant après du public et du personnel.

\begin{center}
	***
\end{center}

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/bnf}
  \caption{Localisation (en bleu) des postes de consultation des archives Web à la BNF (Rez-de-Jardin, site François Mitterand)}
  \label{fig:map-bnf}
\end{marginfigure} 
  
\noindent Les conditions d'accès restreintes aux corpus de l'INA et de la BNF ne jouent pas en la faveur d'une démocratisation de leur exploitation. Auprès du grand public d'une part, mais également vis à vis de potentiels chercheurs voulant questionner les archives. S'il reste tout à fait possible de venir étudier une liste prédéfinie d'URL et de sites (en lecture seule), les modalités d'accès n'encouragent pas à l'exploration des corpus ni à la possibilité de mener des recherches larges et/ou automatisées (il n'y a pas d'API par exemple). 

\section{Sélectionner, collecter et explorer des corpus}
\label{sec:3_constituer}

\noindent Archiver le Web a rapidement été considéré comme une nécessité. Mais l'enthousiasme des premières années a soulevé nombre d'opposi\-tions \citep{masanes_web_2006} : 1) la qualité des documents publiés sur le Web ne serait pas suffisante pour justifier leur sauvegarde, 2) le Web n'a pas besoin d'être collecté puisque par construction il se préserve lui même, 3) archiver le Web est une opération trop compliquée à mettre en place. 

La première remarque, qui témoigne du conservatisme des éditeurs face a l'arrivée de contenus amateurs, issus du Web et publiés hors de leurs monopoles, s'est atténuée avec le temps et surtout, rappelons le, grâce à l'impacte de la charte de l'UNESCO \citep{unesco_charter_2003}. La seconde objection, ne résiste pas non plus à la réalité du Web. Le Web est un milieu volatile qui, dans une certaine mesure, peut effectivement être considéré comme \textit{self preserving} mais dont une partie importante du contenu reste éphémère. Enfin, nous présenterons les aspects techniques et méthodologiques déployés pour rendre possible l'archivage du Web et explorer ses versions passées.

\subsection{Un objet éphémère et multiforme}  
 
\noindent À l'époque où le Web ne contenait qu'une poignée de pages et de sites, la question de son auto-préservation\footnote{traduit ici de l'anglais "\textit{self preserving}" \citep{spinellis_decay_2003}} fut posée. Le Web s'archivait-il déjà de lui même ? 

Lorsqu'un nouveau contenu est publié, il est possible de reléguer en bas de page les élément plus anciens, à la manière d'une pile. Les \textit{content management system} (CMS\footnote{Wordpress, Drupal, Joomla!, etc}), apparus avec l'essor des blogs, avaient ainsi pour vocation de stabiliser ce processus de création et suppression de contenus en ligne, en reléguant les publications passées dans une section d'\textit{archives} dédiée. Rien n'empêche également un site Web d'être copié dans son intégralité puis redéployé sur de nouveaux serveurs pour le préserver. Le numérique facilite et rend possible la reproduction à l'infini de ses objets. Ainsi, une page Web aurait théoriquement pu ne jamais disparaitre de la toile. Mais il fut montré, dès le début des années 2000, que malgré ces possibilités, le Web restait un milieu hautement instable. La disparition de contenu est un phénomène inhérent au Web. 

La durée de vie d'un site Web, peut être calculée par rapport à la mesure de sa \textit{half life}, soit la durée qu'il faut pour que la moitié de son contenu (ici ramené au nombre de pages\footnote{La \textit{half life} peut être appliquée à d'autres objets numériques comme des bases de données ou des documents scannés. Cette mesure est elle même dérivée des techniques de datation des atomes.}) disparaisse du Web \citep{koehler_longitudinal_2004}. Dès 1999, la \textit{half life} moyenne d'un site Web est ramenée à 50 jours \citep{cho_evolution_1999}, estimation qui doit être relativisée par rapport au contexte de publication du site et à la nature de ses pages \citep{mcdonnell_cataloging_1999,fetterly_large-scale_2003}. De même, 80\% des pages Web collectées entre 2003 et 2004, par les archives du Web japonais ont été effacée en moins d'un an du Web vivant \citep{toyoda_whats_2006}. Pour préserver le Web il faut donc intervenir et archiver avant qu'une page ne soit détruite. Ainsi, avant toute collecte, l'archiviste doit prendre en compte les changements susceptibles d'intervenir sur une page ciblée, afin de minimiser la perte d'information.

Les changements subis par une page Web au cours de son existence sont multiples \citep{douglis_at&t_1998, adar_web_2009}, allant de la modification de son contenu jusqu'à une évolution de la structure des liens qui la relie au reste du site. La fréquence de changement d'une page peut être estimée et prédite en s'appuyant sur des versions précédemment archivées \citep{chawathe_meaningful_1997,khoury_efficient_2007}. Il est possible d'affiner l'estimation de cette fréquence en catégorisant les changements par types (structurels, sémantiques ou cosmétiques) \citep{yadav_change_2007}. Plutôt que de considérer chaque page indépendamment les unes des autres, les changements peuvent être détectés à l'échelle d'un site ou d'un réseau. On s'appuyera alors sur la présence de liens hypertextes \citep{liu_webcq-detecting_2000} ou sur des relations hierarchiques plus marquées \citep{lim_automated_2001}. Dans la suite de ce manuscrit, nous nous limiterons à considérer comme changements les seuls actes de création, de modification ou de suppression d'une partie ou de l'ensemble d'une page Web.

Il faut finalement attendre qu'il soit archivé, pour pouvoir considérer le Web comme un support d'informations \textit{self preserving}. Ce n'est qu'une fois les corpus d'archives rendus accessibles depuis le Web lui même \citep{brugger_website_2009}, que l'on peut considérer qu'il garde en lui la trace (mesurée et mesurable) de ses états passés.\\

\noindent Par ailleurs et à la différence d'autres types de documents à archiver, une page Web possède ce que J. Masanès \citep{masanes_web_2006} nomme une double cardinalité. 

\begin{figure}
  \includegraphics[width=\linewidth]{graphics/web-ressource}
  \caption{La double cardinalité d'une ressource Web, d'après \citep{masanes_web_2006}}
  \label{fig:web-ressource}
\end{figure} 

\noindent La cardinalité est le nombre d'instances en circulation d'un artéfact donné : un musée conservera des pièces uniques et originales, une librairie mettra à disposition de ces visiteurs des copies. La cardinalité donne toute sa valeur à un objet archivé et influence les techniques de préservation. Jusqu'à l'invention de l'imprimerie, pour archiver un livre il fallait le copier à la main. L'original était conservé dans un lieu donné et les copies envoyées vers d'autres bibliothèques \citep{canfora_vanished_1990}. L'original se perdant parfois, la copie (rectifiée ou annotée) devenait à défaut œuvre de référence. Mais la notion d'original disparait avec l'imprimerie. Le livre dans sa forme est stabilisée, les bibliothèques possédant toute la même version d'un ouvrage devenu reproductible à l'identique \citep{febvre_apparition_2013}. Ainsi  Au contraire du livre, sites et pages Web ont la singularité de présenter une double cardinalité : 

\begin{enumerate}[leftmargin=*]  
\item les fichiers sources, hébergés sur un serveur donné 
\item l'infinité d'accès possibles à cette source
\end{enumerate}

\noindent D'une machine à l'autre ou d'un écran à l'autre, une page Web sera toujours vue différemment \citep{bon_apres_2014}. Soit que la taille de l'écran (ordinateur, smartphone, tablette, etc) aura modifié son aspect, soit que la qualité de la connexion à Internet n'aura pas permis de tout charger, ou encore que l'historique de navigation\footnote{Nous discuterons plus en détail de cette problématique dans le Chapitre \ref{chap:6}} aura influencé l'affichage de la page à nos yeux. C'est pour cela que J. Masanès propose de parler de \textit{ressource Web} pour nommer tout objet Web susceptible d'être archivé. Une ressource Web est un document unique dont la source peut être identifiée précisément mais interprétée d'une infinité de manière possibles. Depuis son navigateur, derrière son écran. Pour l'archiviste se pose alors la question de quoi archiver ? L'originale ou toutes les interprétations d'une même page ? Arrivé à ce point, archiver le Web revient donc à prendre en compte l'ensemble des états successifs d'une ressource Web afin de ne rien rater. Une fois la fréquence d'archivage décidée, la collecte peut être opérée du point de vue de la source ou du point de vue de l'internaute naviguant derrière son écran. Où l'archiviste choisira-il de se positionner, lui, et ses outils de collecte~?

\subsection{Sélection}

\noindent Toute collecte sur le Web débute par le choix d'un point d'entrée clairement identifié. L'archiviste ne peut se permettre de dériver au hasard du Web pour trouver les sites qui l'intéressent. La \textbf{sélection} désigne donc l'ensemble des techniques mises en place pour définir ce ou ces points d'entrée. S'agit-il d'un site Web précis ? D'une liste de pages Web ? D'un masque ou d'un pattern d'URL à satisfaire ? À quelle profondeur débuter l'archivage ? Doit-on commencer par collecter la page principale d'un site, la \textit{front page}, ou un sous ensemble de pages contenant un mot clé donné ?     

Le principal critère de sélection définissant le périmètre d'un corpus à archiver reste l'extension d'URL. Les .fr, .uk et autres .ma définissent le cadre grossier d'un domaine national sur le Web (Section \ref{sec:3_20ans}). La sélection s'opère alors en validant un masque d'URL ou une heuristique prédéfinie\footnote{Dans un script cherchant à sélectionner les sites du domaine français, on ne conservera que les noms de domaine dont l'extension valide l'expression régulière suivante : $*.fr\$$}. Il est également possible de dessiner les contours d'une archive en partant d'une liste initiale de sites Web, appelés sites sources et liés à une thématique précise. Il faut pour cela faire appelle, en amont de toute collecte, à des experts (sociologues, historiens, etc). L'INA, par exemple, a procédé par expertise pour identifier les 14.000 sites média de son périmètre d'archivage. 

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/degree}
  \caption{Graphe dont les nœuds sont labellisés par degré. En théorie des graphes, le degré $deg(v)$ d'un nœud $v$ correspond au nombre de liens incidents (entrant ou sortant) à ce nœud.}
  \label{fig:degree}
\end{marginfigure} 

Lors d'une collecte plus large, les archivistes peuvent s'appuyer sur des indices traduisant la valeur d'un site ou d'une page visitée. Le degré d'un site Web (Figure \ref{fig:degree}) permet ainsi d'estimer son autorité au sein d'un environnement hypertexte \citep{abiteboul_first_2002}. Une autre stratégie consiste à identifier les sites sources par rapport aux habitudes de navigation des internautes. Quels sites sont fréquemment visités ? Quelles pages en particulier ? Il s'agit alors d'exploiter les requêtes adressées à un moteur de recherche en ligne \citep{pandey_user-centric_2005}. Ou encore, de se baser sur les \textit{access patterns} \citep{alnoamany_access_2013} afin de privilégier, au sein d'un même site, l'archivage de séquences de pages fréquemment visitées. Le système de sélection d'Internet Archives, en particulier, utilise cette dernière méthode \citep{kimpton_year-by-year:_2006}. Dans la même veine, un historique de navigation personnel pourra faire office de liste de primo-candidats à archiver \citep{dumais_stuff_2016}. Cette fonctionnalité a récemment été ajoutée aux archives du Web danois\footnote{Une version bêta du système danois est disponible ici : \url{https://github.com/netarchivesuite/solrwayback}}. Mais précisons, qu'aucune stratégie de sélection ne prévaut sur une autre. Elles sont d'ailleurs souvent combinées pour former une chaine complexe en amont de tout collectage (Section \ref{sec:3_edias}). 

Par ailleurs, il est possible d'opérer une sélection par \textit{crowd sourcing} en faisant appel à des archivistes tiers. C'est toute l'idée du service payant \textit{Archive-it}\footnote{\url{http://www.archive-it.org}}, lancé en 2006 par Internet Archive, permettant à tout un chacun de se constituer des corpus d'archives Web. Quelques 230 millions d'URLs ont ainsi été recueillies entre 2006 et 2007 avant d'être reversées dans le fond d'archives principales de la Wayback Machine. Dans l'idée de démocratiser encore d'avantage leur exploitation, les archives portugaises offrent la possibilité à chaque internautes de suggérer une liste de pages ou de sites Web (pas nécessairement appartenant au domaine portugais .pt) à ajouter aux  collectages\footnote{\url{http://sobre.arquivo.pt/en/collaborate/suggest/}}. L'utilisateur devient ainsi acteur de la préservation du Web.

Terminons en soulignant que le choix d'archiver une page plutôt que l'ensemble d'un site (et inversement) n'est pas trivial. À quelle échelle doit on archiver ? L'arbitrage est souvent décidé au cas par cas et peut faire l'objet de compromis. Même si l'on demande à Internet Archive de sauvegarder une page précise, le système remontera toujours à la front page du site afin d'en archiver la racine  \citep{kimpton_year-by-year:_2006}. Ce genre de mécanisme permet d'amender et d'enrichir les points d'entrées après chaque collecte. La découverte de nouveaux sites appelant à réévaluer sans cesse la liste d'origine.    

\subsection{Collecte}

\noindent Par \textbf{collecte} nous désignons l'ensemble des techniques visant à transformer une page du Web vivant en une page archivée. Comme nous l'indiquions précédemment le Web peut, sous certains aspects, être considéré comme \textit{self preserving}. Une fois archivés, l'ensemble des éléments collectés restent accessible depuis le Web. Le Web contient en lui même les traces de son passé. Lorsque l'on scanne une pellicule, image par image pour archiver un film, on fait subir à ce support de mémoire une transformation. De l'analogique au numérique. Dans le cas d'une ressource Web, la transformation induite par la collecte est minime. Il s'agit grossièrement de venir prélever les fichiers d'origines d'une page, sans les altérer et de les dater avant de les réintroduire dans les archives Web.     

Or, le protocole HTTP qui régit les règles de communication sur le Web, entre client (l'internaute) et serveur (la page), n'autorise qu'un accès unitaire aux ressources Web. Il n'est possible d'accéder au Web qu'une page à la fois. La page Web (identifiée par une URL unique) est en cela l'unité de consultation de base du Web. La collecte doit donc s'effectuer page après page et non par lot, telle que :   

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/collecte}
  \caption{Archivage du Web vivant page après page, de $p_1$ à $p_3$}
  \label{fig:collecte}
\end{figure*} 

\noindent Il existe ainsi trois grandes familles de techniques d'archivage du Web, qui témoignent du déplacement progressif des outils de collecte du serveur vers le client. 

La première, nommée \textit{serveur-side-archiving}, consiste à collecter les ressources directement depuis le serveur hébergeant un site ou une page Web. Cette technique est plutôt employée par les producteurs de données eux même, s'ils souhaitent archiver l'ensemble des ressources d'une de leur plateforme Web par exemple. Mais les moyens à mettre en place sont considérables car, à la différence d'une simple copie, le \textit{serveur-side-archiving} induit la réplication du site (les fichiers HTML, CSS, etc d'origine) et de l'entièreté de son environnement de dévelop\-pement et d'hébergement. On utilisera plutôt cette méthode pour archiver le Web dit profond\footnote{À opposer à la portion visible du Web (ce que l'on voit derrière son écran), le Web profond désigne tout élément qui n'est pas accessible directement depuis un crawler : les formulaires, bases de données, etc \citep{lawrence_accessibility_2000}. Une discussion sur l'exploration d'archives du Web profond sera menée au chapitre \ref{chap:7}}.

La seconde approche, dite \textit{transaction-archiving}, se situe à la frontière entre serveur et client. Il s'agit ici de positionner l'outil de collecte au niveau du système d'entrées/sorties (IO) du serveur hébergeant le site Web ciblé \citep{fitch_web_2003}. Ce que l'on archivera sera le couple [requête, réponse] du client au serveur, soit la demande d'un internaute cherchant à visiter une URL donnée et la page Web telle que retournée par le serveur. Cette forme d'archive dessine une vision non exhaustive d'un site Web mais néanmoins fidèle à la réalité du flot d'internautes qui le parcourent. En capturant la trace des pages effectivement visitées et la manière toujours unique dont celles-ci sont affichées à l'écran des utilisateurs, cette technique est la seule qui intègre directement l'humain et ses gestes dans les archives Web\footnote{Ce type d'archive fera l'objet d'une exploration dédiée au chapitre \ref{chap:7}, où nous interrogerons les logs de navigation Web de la Bibliothèque du Centre Pompidou}.  

La dernière famille, connue sous l'appellation de \textit{client-side-archiving}, est aussi la plus rependue. Ayant acté qu'une ressource Web pouvait être visualisée d'une infinité de manière possible par le client (l'internaute), l'archiviste choisit ici de placer son outil de collecte en lieu et place de l'utilisateur. L'outil devient client et cherche à reproduire les interactions d'un internaute pour accéder au contenu ciblé~: la page Web à archiver. Tout l'enjeu est donc de définir et de contrôler l'exhaustivité de ces interactions pour construire une copie fidèle d'une page ou d'un site. 

Comme la collecte doit être menée page par page, programmée à l'avance et conduite à échelle large, les archivistes du Web se sont inspirés des \textit{crawlers} développés pour les moteurs de recherche \citep{pant_crawling_2004} à la fin des années 1990. Un \textbf{crawler} est un robot programmé pour parcourir un site ou un ensemble de sites, une page à la fois, en capturant au passage l'ensemble de ses fichiers d'origine. Un crawler, pour bien fonctionner, doit respecter des règles de politesse~: éviter les dénis de services (DNS, Serveurs HTTP), les blacklistages officiels (robots.txt, sitemap.xml, etc.) et officieux (\textit{cloaking}, pièges à robot)\footnote{Ce manuscrit n'étant pas spécifiquement centré sur la question des crawlers, il est possible d'en apprendre d'avantage en se référant aux cours de C. Maussang (\url{https://frama.link/FrFrZ5EC}) ou en se tournant vers des ouvrages dédiés \citep{chakravarthy_webvigil:_2002,mitchell_web_2015}}. Dans le cadre spécifique des archives du Web, un crawler doit en plus intégrer les contraintes temporelles évoquées précédemment. Il a pour mission de capter l'ensemble des changements intervenants sur une page ou un site cible. Enfin, nous appelons \textbf{crawl} une campagne d'archivage menée par un crawler et (par abus de langage) le résultat même de cette campagne. 

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/crawl}
  \caption{Différentes stratégies adoptées par un crawler $c$ pour collecter les pages $\{p_1,...p_n\}$ d'un même site}
  \label{fig:crawl}
\end{figure*}

\noindent L'aspect d'un corpus d'archives Web est directement le fait du crawler qui a mené la collecte. Un crawl peut ainsi être conduit de plusieurs manières. Une première possibilité revient à entreprendre une collecte en profondeur d'abord (\textit{depth-first}, Figure \ref{fig:crawl} (a)). Le crawler capturera en priorité les pages filles de la page sur laquelle il se trouve. Un autre approche consiste à travailler en largeur d'abord (\textit{breadth-first}, Figure \ref{fig:crawl} (b)). Le crawler privilégiera cette fois les pages sœurs. Mais ces techniques sont lentes et il faudra prévoir un temps considérable pour parcourir l'entièreté d'un site, or l'archiviste cherchera au contraire à minimiser le temps de capture. Aussi, on peut envisager l'instauration d'une limite en profondeur pour ne pas archiver des pages trop éloignées de la racine du site (Figure \ref{fig:crawl} (c)).

En pratique, l'archiviste optera plutôt pour un compromis entre largeur et profondeur. Avec la démocratisation des moteurs de re\-cherches en ligne, l'internaute n'est plus obligé de passer par la front page d'un site pour en consulter le contenu. Les profils de navigation se diversifient rapidement \citep{holscher_web_2000}. Pour identifier les pages pertinentes, les crawler doivent donc intégrer à leurs programmations divers indicateurs topologiques ou sémantiques. Le pageRank \citep{page_pagerank_1999} ou le degré entrant d'un site (Figure \ref{fig:degree-in}) traduisent tous deux l'importance d'une page crawlée \citep{cho_efficient_1998}. Ces mesures peuvent être enrichie au regard de l'historique du crawl ou d'une possible hiérarchie entre pages \citep{baeza-yates_crawling_2005}. 

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/degree-in}
  \caption{Graphe dont les nœuds sont labellisés par degré entrant. En théorie des graphes, le degré $deg^-(v)$ d'un nœud $v$ correspond au nombre de liens incidents entrant à ce nœud.}
  \label{fig:degree-in}
\end{marginfigure} 

Nous le verrons, lorsque dans la chapitre \ref{chap:4} nous changerons de point de vue, passant de l'archiviste à l'explorateur d'archives, la cohérence est une notion fondamentale. Un crawl doit garantir une forme de cohérence topographique et temporelle vis à vis du corpus qu'il cherche à constituer. Sur ce point, il faudra poser la question de l'ordonnancement des sites les uns par rapport aux autres. Certains sites, larges ou volatiles, feront l'objet d'une collecte rapide (\textit{short-term scheduling}) qui mobilisera toutes les ressources du crawler.  Pour les autres, le crawl s'inscrira dans le temps long (\textit{long-term scheduling}) et pourra prendre plusieurs jours \citep{castillo_scheduling_2004}. Ne crawler que lorsque qu'un site est le moins susceptible de subir des changement peut aussi garantir une cohésion temporel au corpus \citep{saad_coherence-oriented_2011}. S'appuyant sur toutes ces réflexions, les équipes d'Internet Archive présentent en 2004 un crawler open-source, l'Heritrix \citep{mohr_introduction_2004} capable de s'adapter à divers type de collecte : large (\textit{broad crawling}), en continue (\textit{continuous crawling}) ou focalisée (\textit{focused crawling}). Heritrix reste encore aujourd'hui le crawler le plus répandu pour l'archivage du Web. 

Face à l'évolution du Web, les crawler s'adaptent et archivent de nouveaux objets : allant des sites Flash\footnote{\url{https://fr.wikipedia.org/wiki/Adobe_Flash}} aux vidéos Youtube ou Dailymotion \citep{pop_archiving_2010}. Mais alors que les contenus publiés incorporent de plus en plus d'éléments dynamiques, se syndiquer à un flux RSS devient une stratégie à part entière pour collecter de l'information en continue \citep{oita_archiving_2010}. Des librairies sont développées pour interpréter les portions de code utilisant du Javascript\footnote{\url{https://github.com/ariya/phantomjs}} et les crawlers commencent à se spécialiser pour archiver certains réseaux sociaux. Les interfaces de programmation applicative (API) s'imposent comme des sources de données auxquelles il convient de se connecter. Si certaines APIs ouvertes permettent de crawler l'entièreté d'une plateforme\footnote{Voir la crawl de Github réalisé en 2010 par F. Cunny et Linkfluence (\url{http://www.visualcomplexity.com/vc/project.cfm?id=785})}, d'autres plus limitées obligent les archivistes à faire preuve d'inventivité. Ainsi, Internet Archive possède, depuis Mars 2016, un compte Facebook \textit{charlie.archivist} dont la timeline est régulièrement archivée\footnote{\url{https://web.archive.org/web/20170914234842/https://www.facebook.com/charlie.archivist}}. 

Terminons ce descriptif des techniques de collecte, par une présentation plus poussée du crawler de l'INA. Développé par les équipes de T. Drugeon \citep{drugeon_technical_2005}, ce crawler fut en charge (de 2010 à 2014) de l'archivage des sites Web de l'Atlas e-Diasporas. 

\begin{figure}%
  \includegraphics[width=\linewidth]{graphics/crawl-ina}
  \caption{Fonctionnement général du système de collecte de l'INA}
  \label{fig:crawl-ina}
\end{figure}

\noindent Pour commencer (Figure \ref{fig:crawl-ina}), un ordonnanceur général (\textit{scheduler}) gère la liste des sites sources auxquels une fréquence de collecte a été associée. En se basant sur cette fréquence, l'ordonnanceur choisit les sites à archiver en priorité et leur dédie à chacun un crawler (\textit{site crawler}). Plusieurs centaines de crawlers peuvent être ainsi lancés en parallèle. Ces crawlers procèdent à une récolte en largeur d'abord, mais sans sortir du périmètre du site qui leur est alloué. Une fois les pages visitées, le contenu est indexé et stocké sur fichiers. À ce niveau, si des liens hypertextes sortants sont détectés dans une page, les sites pointés par ces derniers sont conservés afin de potentiellement venir enrichir la liste des sites sources. La fréquence de collecte est mise à jour entre deux crawls successifs. Soit en analysant les informations venues de l'agrégateur de flux RSS, soit en comparant l'évolution d'une page archivée d'une version à l'autre. Cette architecture, fait du crawler de l'INA un outil extrêmement réactif, adapté à la nature même des sites médias et, de fait, très efficace lorsqu'il s'agit de constituer rapidement des corpus portant sur un événement singulier\footnote{Voir la collecte réalisée pour les attentats de Paris en 2015 (\url{https://asap.hypotheses.org/173})}.

\subsection{Stockage}

\noindent Le \textbf{stockage} représente l'ensemble des techniques d'enregistrement d'une ressource Web crawlée. Ainsi, parallèlement à son crawler, l'INA développe son propre format de fichier destiné au stockage des archives Web : le Digital Archive File Format (DAFF). L'INA prend ainsi le contre pied du reste de la communauté qui, elle, continue de s'en tenir au format Web ARChive (WARC) pour sauvegarder la grande majorité des corpus existants.

C'est en 1996, s'inspirant du format de compression et d'archivage ARC (popularisé à la fin des années 80), qu'Internet Archive définie le ARC\_IA\footnote{\url{https://www.loc.gov/preservation/digital/formats/fdd/fdd000235.shtml}}. L'idée étant de combiner plusieurs ressources collectées en un seul et même fichier avant de les compresser pour en réduire la taille sur disque.

Mais l'ARC\_IA évolue rapidement, suivant les avancées des techniques de crawl, et ce, jusqu'à atteindre sa version actuelle : le WARC. Devenu le format standard d'archivage Web en 2009\footnote{\url{https://www.iso.org/standard/44717.html}}, un fichier d'ar\-chives WARC peut être vu comme la concaténation de plusieurs enregistrements (ou blocs), chaque enregistrement correspondant à une ressource Web crawlée\footnote{Une page, une image, ... Chez Internet Archive, toute objet associé à une URL unique sera archivé comme ressource Web}. Les informations contenues dans un bloc WARC sont de deux natures : des \textit{meta données} et des \textit{données}. Les méta données (stockées dans le \textit{header} du bloc) couvrent toutes les informations relatives au crawl : date de collecte, taille de la ressource, URL de la ressource, ID du bloc, ... Ces méta données sont directement suivies des données à proprement parler : soit l'enregistrement brut des fichiers .HTML, .CSS, etc collectés. Ainsi, chaque fois qu'une page Web est archivée (qu'elle ait évolué ou non depuis le précédent crawl) un bloc est ajouté au fichier WARC courant (Figure \ref{fig:daff-warc} (a)). 

Directement liés au WARC, il est possible d'extraire d'un bloc deux sous-formats spécialement dédiés à l'exploitation des corpus : les WAT et WET. Un fichier WAT (Web Archive Transformation) ne contient que des méta données. Contrairement au WET (Web Extracted Text) et à ses dérivés (LGA ou WANE\footnote{\url{https://webarchive.jira.com/wiki/spaces/ARS/pages/90997507/Datasets+Available}}) qui, eux, ne stockent que des éléments de texte issus de la partie données d'un bloc WARC. Ces fichiers WAT et WET répondent à l'une des principales critiques lancées à l'encontre du format WARC, pourtant hégémonique : le WARC introduit de la redondance dans les stocks d'archives Web.

En effet, entre deux crawls successifs, un bloc WARC sera invariablement crée (que la ressource Web collectée ait évolué ou non). Une page Web stable dans le temps, verra ainsi son contenu archivé autant de fois qu'elle aura été crawlée, conduisant à une consommation d'espace de stockage considérable. C'est donc en partant de l'intuition selon laquelle méta données et données devraient être stockées séparément (pour ne pas surcharger les corpus) que l'INA a développé le format DAFF.  

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/daff-warc}
  \caption{Différences entre les formats WARC (a) et DAFF (b)}
  \label{fig:daff-warc}
\end{figure*}

\noindent Une archive DAFF est en réalité l'association de deux fichiers complémentaires : un fichier de méta données et un fichier de données. Comme pour le WARC, chaque fichier est une suite de blocs correspondant à une ressource Web crawlée. Le fichier de méta données contient divers champs relatifs à la collecte (voir Table \ref{tab:daff}). Le fichier de données, quant à lui, ne renferme que deux champs : un identifiant unique et le contenu (HTML, CSS, etc) de la page Web archivée. Avec le DAFF, données et méta données sont stockées séparément. Ainsi, d'un crawl à l'autre, si la page Web visitée n'a pas évolué, alors son contenu ne sera pas re-téléchargé : seul un nouveau bloc de méta données sera ajouté pour témoigner du passage du crawler. Chaque bloc de données est donc associé à un ou plusieurs blocs de méta données (le champ \textit{content} des méta données correspondant au champ \textit{id} des données). Ce mécanisme permet, d'une part de ne pas dupliquer inutilement le contenu d'une page Web archivée et, d'autre part, de pouvoir pratiquer rapidement divers calculs statistiques sur le seul fichier de méta données. Celui étant pas nature plus léger qu'un WARC complet, donc moins long à traiter.

Notons que pour tester si une page a évolué depuis sa précédente visite, le crawler de l'INA compare la valeur des champs \textit{id} des blocs de données concernés. En effet l'\textit{id} est une clé SHA-256 résultant du hachage\footnote{En cryptographie, le hachage consiste à une donnée de taille arbitraire, une image (ou clé) de taille fixe et unique} du contenu même de la page Web archivée. On dira donc d'une page Web qu'elle s'est transformée si et seulement si les deux clés successives sont différentes. La nature de ce changement ne sera en revanche pas connue, celui-ci pouvant aller de la refonte entière de la page à la simple suppression d'une virgule.        

\begin{table*}
  \label{tab:daff}
  \begin{tabular}{lrl}
    \toprule
    Méta données& Champ & Description\\
    \midrule  
    \multirow{5}{*}{\emph{obligatoire} \vastt\{ }&id&identifiant unique du bloc\\
    &url&url associée à la ressource\\
    &date&date de téléchargement (Timesone GMT, ISO 8601)\\
    &content&identifiant unique du bloc de données associé\\     
    &status&statut de retour du crawler (ok, request\_error, server\_error, etc)\\
    \midrule     
    \multirow{12}{*}{\emph{facultatif   } \Vastt\{ }&crawl\_session&identifiant unique de la campagne de crawl\\
    &charset&encodage de la ressource\\
    &type&MIME Type (identifiant du format de donnée de la ressource)\\
    &corpus&nom du corpus d'archives associé\\
    &ip&adresse ip associée au crawl\\
    &level&profondeur du crawl\\
    &page&la ressource est elle une page Web (0|1)\\
    &client\_country&nationalité associée à la page\\
    &length&taille du bloc de données associé\\
	&active&la ressource était elle active au moment du crawl\\
	&client\_lang&langue associée à la ressource\\
	&referer\_url&url précédemment visitée par le crawler\\
    \midrule
    Données& Champ & Description\\
    \midrule 
    &id&clé SHA-256 unique\\
    &content&contenu (HTML, CSS, etc) de la ressource\\    	
    \bottomrule
\end{tabular}
  \bigskip
  \caption{Ensemble des champs disponibles dans les fichiers de méta données et de données DAFF}
\end{table*} 

\noindent Outre le stockage sous formats WARC et DAFF, J. Masanès \citep{masanes_web_2006} rappelle qu'il existe des méthodes alternatives de sauvegarde des archives. Associées aux stratégies de collecte situées côté serveur (\textit{serveur-side-archiving}) on trouvera les formes dites de \textit{local file system served archives} qui consistent à transformer un site Web archivé en une copie locale de l'ensemble de ses ressources. Ainsi les URIs absolues, permettant (sur le Web) de naviguer d'une page à l'autre, seront transformées en URIs relatives à l'intérieur du fac-similé. Très couteuse, cette méthode nécessite de transformer en profondeur la nature des pages archivées et devient vite ingérable à mesure qu'augmente le nombre de collectes.

Enfin, il reste toujours possible de copier un site, page après page, sous format PDF ou image (capture d'écran ou vidéo). Bien que facile à mettre en place (techniquement parlant) cette stratégie ne passera pas non plus à l'échelle\footnote{En 2013, K. Goldsmith imprime littéralement plusieurs centaines de milliers de pages Web en soutient à A. Schwartz, remplissant l'équivalent d'une pièce de 1,100 $\mathrm{m}^2$} et aura pour conséquence d'arracher les sites et pages Web archivés à leur environnement hypertexte d'origine.  

Pour terminer, les corpus d'archives Web répartis dans le monde se comptent par centaines. Alors que le Web vivant continue son expansion, le volume du Web archivé ne cesse de croitre. En 2017, la BNF avait archivé 18,000 millions de pages Web (soit environ 370TB) tandis que l'INA plafonnait à 43,000 millions de pages pour un total avoisinant les 420TB. Et depuis sa création, l'Internet Archive a collecté à elle seule pas moins de 650,000 millions de ressources Web soit 40,000TB de données. Ainsi, face à ces corpus qui s'amassent et à la nécessité de les exploiter, les archivistes du Web ont du développer des outils dédiés à leur exploration.

\subsection{Fouille}

\noindent Par \textbf{fouille}, nous désignons les stratégies d'interrogation et de requêtage des archives Web. Ainsi, pour permettre aux chercheurs d'ana\-lyser le résultat des collectes, les archivistes déploient des dispositifs techniques \textit{au dessus} des corpus existants. 

Comme nous l'évoquions en section \ref{sec:3_20ans}, la fouille est tributaire des modalités d'accès aux données qui, pour $50\%$ des initiatives \citep{costa_survey_2013}, passent par la mise en place d'un portail en ligne. Mais cela ne signifie pas pour autant que les archives sont entièrement accessibles. Su ce point, $38\%$ des initiatives restreignent la consultation de leurs corpus : soit que l'analyse doit se faire localement (INA, BNF, etc), soit que les archives ne sont pas intégralement mises à disposition du public (The Library of Congress, Australia's Web Archive, etc). Contrairement à Internet Archive et aux Portuguese Web Archives qui proposent un plein accès, en ligne, à leurs collectages.  

Les dispositifs de fouille\footnote{Souvent désignés par \textit{search strategies} ou simplement \textit{search} dans la littérature}, déployés par dessus les archives, reprennent l'architecture générale de la plupart des système de moteurs de recherche \citep{grainger_solr_2014,hatcher_lucene_2004}. Les archives sont ainsi indexées puis mises à disposition d'un serveur de \textit{search} qui les rend interrogeables\footnote{Nous développerons le processus d'indexation des archives Web plus en détail dans la section \ref{sec:4_moteur}}. L'indexation définie l'étape de transformation d'un document texte en une liste de mots ou d'ensembles de mots, cette étape est nécessaire à toute construction d'un moteur de recherche. On appelle index la structure de données obtenue après indexation. 

Côté utilisateur, la recherche se traduit par une interface Web, dans laquelle il est possible de rentrer une requête (un texte, un ensemble de mots clé, des filtres, etc), puis de consulter les résultats correspondants sous la forme d'une liste ou d'un histogramme (Figure \ref{fig:ia-search}). Tout l'enjeu pour ces moteurs d'exploration d'archives est de proposer la meilleur technique de recherche possible pour fouiller efficacement un corpus d'archives Web. Ou comment, partant de la requête d'un chercheur, proposer avec justesse un ensemble de page archivées qui satisfasse ses interrogations ?

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/IA-search}
  \caption{Interface de search de la WayBack Machine (\url{https://web.archive.org/web/*/yabiladi})}
  \label{fig:ia-search}
\end{marginfigure} 

Dans ce domaine, la recherche dite plein texte (\textit{full-text}) est ce vers quoi tendent toutes les initiatives d'archive du Web \citep{costa_characterizing_2011,costa_evaluating_2012}. En recherche d'information, le full-text revient à faire correspondre les mots d'un document ou d'un ensemble de documents avec les critères fournis par un utilisateur (des mots clés, une phrase, etc). Popularisée sur le Web par AltaVista, c'est la recherche telle que nous l'expérimentons quotidiennement en interrogeant Google. Mais encore aujourd'hui, son application aux moteurs d'exploration d'archives Web reste limitée. Si l'INA, les portugais d'Arquivo.pt ou encore The Web Archive of Catalonia\footnote{\url{https://www.padicat.cat/en}} proposent d'étendre cette fonctionnalité à l'ensemble de leurs corpus \citep{stack_full_2006}, à la BNF, au contraire, le full-text n'est applicable que sur les seules URLs des pages archivées. C'est à dire qu'à une requête utilisateur donnée, le moteur de la BNF ne pourra faire correspondre qu'une recherche stricte par URL, sans regard pour le contenu même des pages. En 2016, Internet Archive ajoute à la Wayback Machine un système full-text basé sur le titre des pages Web collectées\footnote{Si et seulement si le titre est présent dans la balise \textit{head} du HTML de la page (\url{http://blog.archive.org/2016/10/24/beta-wayback-machine-now-with-site-search/}, \url{https://www.w3schools.com/html/html_head.asp})}, avant cette date seule une recherche stricte par URL était proposée. Selon M. Costa \citep{costa_survey_2013}, $67\%$ des initiatives d'archivage du Web proposaient en 2013 une recherche full-text complète, limitée ou dégradée. 

La taille importante des corpus ou la difficulté des archivistes à définir quelles doivent être les éléments d'une page Web à indexer, peuvent expliquer que le full-text soit si compliqué à mettre en place. Mais des alternatives existent : on peut ainsi envisager une recherche par catégories \citep{holzmann_tempas:_2016}, par entités nommées \citep{spaniol_tracking_2012} ou en se basant sur des tendances issues des réseaux sociaux \citep{risse_arcomem_2014}. Une solution originale consiste à ne pas construire de moteur d'exploration mais à s'adosser à des systèmes existants. Les créateurs du projet Memento\footnote{\url{http://mementoweb.org/about/}}, par exemple, intègrent les corpus d'Internet Archive directement à l'intérieur d'un navigateur Web. Ce faisant, la fonction de recherche à proprement parlé est assuré par le navigateur. 

Créée en 2001, la Wayback Machine est le plus emblématique des systèmes de fouille d'archives Web \citep{tofel_waybackfor_2007}. L'ensemble des re\-ssources archivées y sont indexées page par page (bloc WARC par bloc WARC). Les indexes sont ensuite répartis sur les quelques 2500 serveurs de stockage du data center principal d'Internet Archive. Un ordonnanceur central envoie les requêtes utilisateurs à l'ensemble des serveurs avant d'agréger les résultats. Il existe un système de cache destiné à améliorer les temps de réponse de la Wayback Machine, ainsi l'attente variera en fonction de la popularité de chaque requête. Le moteur des Portuguese Web Archives propose, lui, d'indexer les archives d'abord par date de téléchargement puis page par page \citep{costa_survey_2013}. Si l'utilisateur associe à sa requête une années ou un intervalle de temps précis, le résultat de ses recherches lui sera plus rapidement retourné. Les moteurs d'exploration peuvent aussi être entièrement décentralisés. A. Anand décrit Everlast \citep{anand_everlast:_2009} comme un système de fouille \textit{peer-to-peer} où chaque élément du réseau est à la fois serveur et client, plus scalable donc. 

Bénéficiant d'une distribution open source, la Wayback Machine est aujourd'hui réutilisée ou sert de base\footnote{Dans son intégralité ou élément par élément : Héritrix pour la partie crawl, NutchWAX pour la partie recherche full-text} à l'architecture de $62\%$ des initiatives d'archivage du Web \citep{costa_survey_2013}. Mais force est de constater que les systèmes existants n'encouragent pas particulièrement à l'exploration des archives, à la découverte des corpus, ou plus basiquement à leur exploitation à grande échelle (en terme de quantité de pages ou de temporalité). Il est de plus difficile de s'évader du cadre stricte imposé par des interfaces invariablement semblables, proposant une expérience des archives Web toujours identique identique.

La Wayback Machine est redoutablement efficace lorsqu'il s'agit de rechercher une version précise d'une page déjà connue. Notons aussi qu'elle propose une API\footnote{\url{https://archive.org/help/wayback_api.php}} pour accélérer les traitements. Mais sans la possibilité de filtrer à priori le contenu des pages, tout le dispositif de fouille sera à développer du côté de l'explorateur qui bien souvent n'a pas les compétences ou les moyens techniques pour y arriver. Et si l'INA offre de meilleures fonctionnalités de recherche (plein text complet, n-gram, etc ...), le fait que l'on ne puisse accéder aux corpus que depuis des lieux dédiés reste un frein majeur à toute analyse. Ainsi, une asymétrie se dessine rapidement lorsque l'on parcourt la littérature basée sur les archives Web. Beaucoup de travaux portent sur la constitution en amont de corpus d'archives (sélection, collecte, etc), très peu en revanche se lancent dans l'exploitation ou le questionnement de corpus existants. Ces derniers, bien que précieux à juste titre, se \textit{limitent}\footnote{Pas forcément en conscience, mais nous pensons que les outils d'exploration jouent un rôle quand il s'agit de définir de la portée de ces travaux} soit à l'analyse de versions passées de sites identifiés à priori \citep{schafer_web_2016,gebeil_les_2016}, soit à l'extraction d'éléments singuliers d'un contenu archivé : des images \citep{ben-david_internet_2018} ou des liens hypertextes \citep{weltevrede_where_2012}.

Les archives Web ont été construites pour inscrire la mémoire du Web sur un support durable et préserver notre héritage numérique. Offrir ainsi la possibilité aux chercheurs de demain d'interroger, de questionner et de critiquer le Web qui nous est contemporain. Mais plus on archive et plus la taille de ce Web passé grandit, laissant parfois les chercheurs seuls face à des corpus trop larges et trop vastes pour être explorer sans méthode et stratégie clairement définies. Les archives Web doivent rester une matière vivante. Prenons garde à ce qu'elles ne deviennent pas des capsules temporelles\footnote{\url{https://en.wikipedia.org/wiki/Westinghouse\_Time\_Capsules}} que l'on enterre dans l'espoir, qu'un jour, peut être, quelqu'un se décide à les rouvrir. 

\section{Les archives Web de l'Atlas e-Diasporas}
\label{sec:3_edias}

\noindent Parallèlement au travail de cartographie présenté en section \ref{sec:2_atlas}, les chercheurs pilotant la construction l'Atlas e-Diasporas prennent la décision d'archiver l'ensemble des sites Web déjà répertoriés. Tout autant pour les préserver des assauts du temps \citep{khouzaimi_e-diasporas_2015} que pour permettre la tenue de recherches futures : se donner la possibilité d'un retour arrière, analyser les évolutions et transformations subies par ces réseaux. Déjà associée à la collecte des sites, l'INA se voit confier la charge de l'archivage. Toutes les e-Diasporas seront concernées par cette campagne de sauvegarde, mais  nous nous attarderons ici sur la seule description de la section marocaine de l'Atlas. 

L'archivage du corpus marocain débute en Mars 2010 et se termine en Septembre 2014 après une collecte patiente et continue. Le collectage couvre l'ensemble des 156 sites de l'e-Diasporas marocaine. La fréquence de collecte associée à chaque site est définie en amont par les chercheurs. Celle ci est sera au final soit hebdomadaire (pour $56\%$ des sites), soit mensuelle (pour les $44\%$ restants). La majorité des sites archivés à la semaine sont les plus fréquements mis à jours : des blogs, des portails communautaires ou des médias. Les archives sont stockées suivant le format DAFF, vu comme l'union d'un fichier de méta données (\textit{metadata-r-00006.daff} : 13GB) et d'un fichier de données (\textit{data-r-00006.daff} : 151GB). Ces fichiers représentent un total de 17,043,833 ressources collectées, parmi lesquelles nous comptons 16,897,787 pages Web ($99\%$), 145,301 images, 700 vidéos et 44 enregistrements audio. Dans le chapitre \ref{chap:6} nous explorerons les sites \textit{yabiladi.com} et \textit{larbi.org} dont une présentation détaillée est donnée par la table \ref{tab:detail-archive}. Cette table introduit un premier élément de comparaison entre les archives e-Diasporas et leurs équivalents chez Internet Archive.

\begin{table}
  \label{tab:detail-archive}
  \begin{tabular}{lrr}
    \toprule
    &larbi.org&yabiladi.com\\
    \midrule
    Nombre d'archives (e-Diasporas)  & 78,311 & 2,683,928\\
    Nombre d'archives (Internet Archive) & 24,537 & 887,981\\
    \midrule
    Début de l'archivage  (e-Diasporas) & Mars 2010 & Mars 2010\\
    Début de l'archivage  (Internet Archive) & Oct. 2002 & Fev. 2001\\
    \midrule
    Fin de l'archivage  (e-Diasporas) & Sept. 2014 & Sept. 2014\\
    Fin de l'archivage  (Internet Archive) & Sept 2018 & Sept 2018\\    
  \bottomrule
\end{tabular}
  \bigskip
  \caption{Décompte des archives Web des sites \textit{yabiladi.com} et \textit{larbi.org}}
\end{table} 


\noindent Si la fréquence d'archivage (telle que mise en place par l'INA) semble plus élevée du côté d'e-Diasporas, la durée de collecte est importante chez Internet Archive. L'idée ici n'est pas de prouver qu'un corpus est mieux qu'un autre, mais de saisir les particularités de chacun. Un corpus d'archives Web n'est jamais parfait, bien au contraire, et nous émettons ici l'hypothèse que c'est par une approche mixte, en conjuguant diverses sources de données que nous maximiserons la précision scientifique des explorations à venir. Ainsi, sur la période 2010-2014 et dans les cas précis de \textit{yabiladi.com} et \textit{larbi.org}, la capture réalisée pour e-Diasporas semble plus fidèle. Il faudra en revanche l'associer à Internet Archive lorsque nous chercherons à remonter au delà de 2010. 

Mais essayons maintenant d'étendre cette comparaison à l'ensemble des sites du corpus marocain. Voyons comment ces sites ont été archivés par différentes initiatives. Comme beaucoup de ces observations devrons être faites à la main (notamment à la BNF), nous nous limitons tout d'abord aux seules front pages (pages racines) de chaque site Web de l'e-Diasporas marocaine. Pour chacune de ces pages, nous consultons successivement les archives e-Diasporas (produites par l'INA), les archives de la BNF et les archives d'Internet Archive, puis nous notons leurs dates de premier et dernier crawl afin de se donner une idée de l'étendue des collectes. Les résultats sont présentés et agrégés par les figures \ref{fig:date-crawl-ina} (pour l'INA), \ref{fig:date-crawl-bnf} (pour la BNF) et \ref{fig:date-crawl-ia} (pour Internet Archive). Le nom de domaine des sites est inscrit en ordonnée, le temps en abscisse. Chaque ligne est divisée en années puis en mois (1 mois = un tiret). Si un tiret est colorié c'est qu'il se trouve entre les dates de premier et de dernier crawl du site correspondant.

\iffalse

\begin{figure*}[hbtp]%
  \includegraphics[width=\linewidth]{graphics/date-crawl-ina}
  \caption{Préservation des sites de l'e-Diaspora marocaine par l'INA}
  \label{fig:date-crawl-ina}
\end{figure*}

\begin{figure*}[hbtp]%
  \includegraphics[width=\linewidth]{graphics/date-crawl-bnf}
  \caption{Préservation des sites de l'e-Diaspora marocaine par la BNF}
  \label{fig:date-crawl-bnf}
\end{figure*}

\begin{figure*}[hbtp]%
  \includegraphics[width=\linewidth]{graphics/date-crawl-ia}
  \caption{Préservation des sites de l'e-Diaspora marocaine par Internet Archive}
  \label{fig:date-crawl-ia}
\end{figure*}

\fi

L'intuition précédente est confirmée par ces trois figures : les corpus vu depuis Internet Archive et (dans une moindre mesure) depuis la BNF couvrent naturellement une plus grande étendue temporelle que la collecte de l'INA, limitée aux seules années 2010-2014. En revanche, leurs collectages sont incomplets, les sites marocains ne sont pas tous archivés. C'est assez naturel au regard du périmètre d'archivage de la BNF notamment, qui ne doit théoriquement couvrir que les sites du domaine français, ici la BNF aura archivé par effet de bord des sites marocains en .com ou .org ce qui nous amène à relativiser la notion de domaine Web national telle que présenté plus tôt (Section \ref{sec:3_20ans}). Ce qui frappe, enfin, est la cohérence générale de notre corpus tel qu'il est présenté par l'INA. L'ensemble des sites sont archivés, collectés au moins une fois entre 2010 et 2014 et forment un ensemble thématiquement homogène. 

Mais ces figures ne doivent pas non plus nous induire en erreur, nous ne voyons pas le détail des collectes : ni ce qu'il s'est passé entre les dates de premier crawl et de dernier crawl, ni le comportement du crawler vis à vis de pages éloignées de la racine des sites. Or, nous le découvrirons dans le chapitre suivant, archiver est avant tout une question de choix et de sélections, ce qui posera nombre de problèmes à l'explorateur d'archives Web. 

\begin{center}
	*
\end{center}

\noindent Au cours de ce chapitre, nous nous sommes attaché à décrire la genèse de l'archivage du Web comme technique de préservation d'un nouvel héritage numérique. L'idée étant de comprendre la nature des corpus que nous manipulerons dans la suite de manuscrit. Au tournant des années 2000, de nombreuses initiatives privées et publiques se sont emparées du sujet, déployant en un temps record (à l'échelle du Web) des moyens humains et techniques. Néanmoins cette dynamique semble aujourd'hui s'essouffler et force est de constater que, même si les corpus grandissent toujours plus, peu de chercheurs se sont déjà aventurés dans les archives. Le Web passé reste un terrain en partie inexploré. 

Pour se saisir du Web, il aura fallu aux pionniers du Web inventer et déployer de nouvelles méthodes de collecte et de stockage. C'est choix techniques façonnent et régissent les archives Web telles que nous les découvrons aujourd'hui. Détacher du Web vivant, l'archive Web se consultent dans des lieux sanctuarisés, souvent à la main et à travers des outils qui, malgré eux, réduisent les archives Web à de simples documents. La sensation du Web comme environnement n'est pas restituée dans les archives. L'exploration y est forcément ciblée, réduite à une URL ou un mot clé. 

Cependant, comme pour le Web vivant, l'unité d'exploration des archives reste la page Web. WARC et DAFF sont deux formats construits au dessus des pages qu'ils capturent et dotent, par là même, d'une nouvelle temporalité. Une fois sur fichier chaque version d'une même page se voit associée à une date de téléchargement. Cette date devient dès lors le seul marqueur temporel par lequel nous pouvons explorer les archives Web. Dans le chapitre suivant, nous chercherons ainsi à comprendre ce que cette datation implique pour l'explorateur.    

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Chapitre 4 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\begin{minipage}[t,leftmargin=5em]{1.5\linewidth}%
\begin{adjustwidth}{-0.5cm}{}
\chapter{Traces Discrétisées et Temporalité Figée} 
\label{chap:4}
\end{adjustwidth}
\end{minipage}
\hfill

\noindent Au cours de ce chapitre, nous amorcerons un changement du point de vue, glissant du regard de l'archiviste vers celui de l'explorateur d'archives Web. 

Un explorateur d'archive est une personne ayant l'intention de découvrir ou d'étudier un corpus d'archives Web donné (fini ou toujours en construction). Son geste pourra tout autant être motivé par une question de recherche précise que par sa seule curiosité. Ce faisant, l'explorateur devra démêler les traces d'un Web passé pour faire émer\-ger une information ou un savoir de cette masse de données. 

Dans un premier temps, nous déconstruirons la structure des ar\-chives Web pour en saisir les règles et la grammaire interne. En effet, selon N. Brügger \citep{brugger_website_2009}, le Web archivé n'est déjà plus le Web, c'est un autre espace, un autre environnement. Lorsqu'un site du Web vivant est sélectionné, stocké et collecté, il subit une série de transformations qui forcent les archivistes à recréer une partie du système d'information du Web. Pour explorer les archives, il faut se détacher des automatismes acquis en parcourant le Web vivant. Sur ce point, nous présenterons ici certaines propriétés et certains biais inhérents au Web archivé qu'il faudra prendre en considération avant toute analyse. 

Pour l'explorateur, les archives Web se présentent d'abord comme des traces discrétisées du Web vivant, arrachées à un flux d'information en continu ou à un territoire en expansion. La discrétisation du Web par les archives est le fruit d'une sélection, mais surtout d'un ensemble de destructions, comme le souligne J. Derrida \citep{derrida_trace_2014}. Archiver c'est avant tout détruire ce que l'on ne peut conserver.

Par ailleurs, la collecte propulse les ressources archivées dans une nouvelle temporalité. Les pages du Web passé n'appartiennent plus au temps du Web vivant mais au temps des archives : une temporalité faite d'instantanés figés et sans possibilité d'extension. Il n'y a pas de continuité absolue entre deux versions d'une même page archivée. D'un crawl à l'autre tout peut changer (Section \ref{sec:3_constituer}). Ainsi, il nous faudra discuter des phénomènes de leurres et de cécité des collectes\footnote{\textit{crawl blindness} en anglais}, de la notion de cohérence entre pages et de la présences de contenus sur-archivés qui peuvent être source de nombreux biais d'analyse. 

Passé ces mises en gardes, nous décrirons le développement de notre propre moteur d'exploration d'archives Web. Un moteur adapté au format DAFF et suffisamment flexible pour être le support de nos futures expérimentations. Nous détaillerons notre chaine d'extraction et d'enrichissement des archives, ainsi que la pièce maitresse de tout système de fouille : le schéma d'indexation et ses implications. 

Enfin, nous constaterons que les archives Web ne sont pas des traces directes du Web vivant, mais plutôt les traces directes des crawlers. Nous donnerons ainsi des exemples d'artéfacts de crawl, présents dans les archives de l'Atlas e-Diasporas et qui, à nos yeux, sont des freins majeurs à toute exploration large des corpus. Ce sera l'occasion de porter un regard critique sur les archives telles que nous les connaissons et d'ouvrir la voix à une exploration fragmentée du Web passé.\\


\section{Détruire pour mieux archiver}
\label{sec:4_derrida}

\noindent J.L. Borges ouvre la seconde partie de son recueil de nouvelles \textit{Fictions} \citep{borges_fictions_1974} par un court texte intitulé \textit{Funes ou la mémoire}. Il y fait le compte rendu concis de la rencontre entre son narrateur et le mystérieux Irénée Funes, personnage ayant la capacité de ne rien oublier, jamais. Funes a une mémoire prodigieuse : \\

\begin{fullwidth}
"\textit{En effet, non seulement Funes se rappelait chaque feuille de chaque arbre de chaque bois, mais chacune des fois qu'il l'avait vue ou imaginée. Il décida de réduire chacune de ses journées passées à quelque soixante-dix mille souvenirs, qu'il définirait ensuite par des chiffres. Il en fut dissuadé par deux considérations : la conscience que la besogne était interminable, la conscience qu'elle était inutile. Il pensa qu'à l'heure de sa mort il n'aurait pas fini de classer tous ses souvenirs d'enfance.}" --- \citep[p.~116-117]{borges_fictions_1974}\\
\end{fullwidth}

\noindent L'esprit de Funes est engorgé de souvenirs d'une infinie précision, enregistrés en continus. Mais la mémoire pour fonctionner, nous dit Borges, a besoin d'oublier, de sélectionner et de généraliser. C'est en substance la thèse soutenue par J. Derrida qui décrit le geste de l'archiviste comme un geste de pouvoir : le pouvoir de choisir ce qui doit être préserver ou non. L'archivage est le résultat d'une sélection féroce qui doit détruire avant de sauver : "\textit{Il n'y a pas d'archives sans destruction, on choisit, on ne peut pas tout garder.}" \citep[p.~60]{derrida_trace_2014}. C'est ainsi que l'organisation légitime de l'héritage collectif revient aux seuls archivistes qui définissent au présent la mémoire de demain\footnote{Internet Archive décide, suite à l'élection de D. Trump en 2016, de créer une nouvelle copie de ses corpus d'archives Web et de les déplacer au Canada (\url{https://frama.link/hgBbtPp6})}, en classifiant et hiérarchisant dans les bibliothèques les traces de nos expériences passées. Ce faisant, pour Derrida "\textit{l'archive commence là où la trace s'organise, se sélectionne}" \citep[p.~61]{derrida_trace_2014}, car toute expérience finit tôt ou tard par s'effacer, il en va de sa nature même. Ainsi, pour maintenir le lien qui nous renvoie à ce qui n'est plus là, il faut archiver nos traces avant qu'elle ne disparaissent. 

Le Web vivant est tout autant un flux continu d'information qu'un territoire en perpétuelle expansion (Section \ref{sec:2_web}). Pour en archiver les traces, il faut procéder par \textbf{discrétisation}, c'est à dire : diviser une forme continue en une ou plusieurs valeurs individuelles. Les systèmes de stockage WARC et DAFF (Section \ref{sec:3_constituer}) réduisent le Web en un ensemble discret de pages archivées. Or, depuis le lancement d'Alta Vista en 1995\footnote{Alta Vista fut le plus important moteur de recherche pré-Google, capable d'indexer une grande partie des pages du Web et de les rendre accessibles via des requêtes plein-texte (\url{https://en.wikipedia.org/wiki/AltaVista})}, la page Web est considérée comme valeur élémentaire d'indexation, de fouille et d'exploration de la toile. Il en va de même pour les archives Web pour qui la page demeure l'unité de base de toute collecte. Ainsi, dans la suite de ce manuscrit, nous schématiserons une campagne de crawl comme telle : 

Un site Web archivé consiste en \textit{n} pages Web numérotées $\{p_1$,...,$p_n\}$. Un corpus d'archives Web est le résultat d'un ou plusieurs crawls successifs $\{c_1$,...,$c_l\}$. Nous appelons crawl $c_i$ le processus de collecte des pages Web $\{p_1$,...,$p_n\}$ d'un site Web donné. Le temps nécessaire au téléchargement des pages est supposé négligeable. Nous appelons $t_i(p_j)$ la date de téléchargement de la page $p_j$ au cours du crawl $c_i$. La première date de téléchargement d'une page $p_j$ est, enfin, notée $\min\limits_{i} t_i(p_j)$. La figure \ref{fig:discret} illustre cette mécanique pour les pages $p_1, p_2, p_3$. 

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/discretisation}
  \caption{Archivage des pages $p_1, p_2, p_3$ au cours du crawl $c_i$}
  \label{fig:discret}
\end{figure*}

\section{Un temps sans durée}
\label{sec:4_temporalite}

\noindent Au cours d'une collecte, les pages archivées sont propulsées dans une nouvelle temporalité. Elles n'ap\-partiennent plus au temps présent du Web vivant, mais au temps figé des archives passées. Comment dès lors capturer le temps présent ? Cette question nous ramène à Saint Augustin, dont l'expression du  présent à partir de l'instant influence encore aujourd'hui la pensée occidentale. Pour Saint Augustin, le présent est une suite infinie de points élémentaires, des instantanés sans étendue :\\ 

\begin{fullwidth}
"\textit{(...) Et cette même heure se compose elle-même de parcelles fugitives. Tout ce qui s'en détache, s'envole dans le passé; ce qui en reste est avenir. Que si l'on conçoit un point dans le temps sans division possible de moment, c'est ce point-là seul qu'on peut nommer présent. Et ce point vole, rapide, de l'avenir au passé, durée sans étendue; car s'il est étendu, il se divise en passé et avenir. Ainsi, le présent est sans étendue.}" ---  \citep[livre XI, chap. XV, 20, p.~195]{saint_augustin_confessions_1993}\\
\end{fullwidth}

\noindent Le présent se déploie sous nos yeux comme un temps insaisissable qui, à peine éprouvé, cesse déjà d'exister pour se diluer dans le passé. La seul manière de le capturer reste donc de le diviser et de le réduire à ses plus petits éléments. Ainsi en va-t-il des archives Web qui sont, par construction, des instantanées du Web vivant : une suite de blocs DAFF régulièrement collectés et associés à des date de téléchargement. 

Mais dans le temps des archives il n'y pas de durée. Toute page collectée n'a d'étendue temporelle que sa seule date de téléchargement. Sur ce point, l'un des enjeux de l'exploration sera justement de réinstaller de la durée dans les corpus archivés. Les phénomènes que nous souhaitons observer et étudier ont besoin d'être rapportés à une durée. Que l'on parle de l'évolution lente d'une communauté de bloggeurs ou de l'éruption soudaine d'un événement dans un forum de discussion, il faudra à chaque fois pouvoir en éprouver l'étendue dans le temps. 

Pour réintégrer de la durée dans les archives, nous nous proposons de discuter de la notion de \textbf{persistance}. Une page archivée sera dite persistante si d'une version à l'autre, son contenu reste inchangé. Dans la formalisme DAFF, les données des pages sont identifiées par des clés SHA-256 (Section \ref{sec:3_constituer}). Ces clés sont des signatures uniques construites à partir du contenu même des pages archivées. Ainsi, en comparant les deux clés SHA-256 de deux versions successivement crawlées d'une même page, il est possible de savoir si cette page a évolué ou non. Par se procéder, nous pouvons identifier des chaînes de persistance entre différents collectages.

Chaque chaîne de persistance s'ouvre sur une date de \textit{dernière modification}. Nous appelons ainsi $\mu_i(p_j)$ la date de \textit{dernière modification} d'une page $p_j$ au cours d'un crawl $c_i$, avec $\mu_i(p_j) \leq t_i(p_j)$. Par définition, au sein d'un même crawl, la date de dernière modification d'une page précédera toujours (ou sera égale à) sa date de téléchargement. La figure \ref{fig:last_modified} donne à voir des chaînes de persistance entre les multiples captures de la page $p_1$.

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/last_modified}
  \caption{Chaînes de persistance entre captures (bleu) et dates de dernière modification (rouge) pour la page $p_1$}
  \label{fig:last_modified}
\end{figure*}

\noindent Intuitivement, il devient alors possible de dire d'une page archivée qu'elle n'a pas évolué depuis telle ou telle collecte, qu'elle a duré dans le temps. De plus, grâce aux chaînes de persistance, la datation des corpus d'archives Web s'affine, se fait plus précise. Une page ne sera plus maintenant seulement rapportée à sa seule date de téléchargement mais également à sa date de dernière modification, potentiellement bien antérieure. La table \ref{tab:datation_1} propose ainsi une échelle de datation, utile pour évaluer la précision historique d'un élément du Web passé dont l'unité d'analyse reste, pour le moment, la page Web :\\

\begin{table}
\hspace{2em}%
  \label{tab:datation_1}
  \begin{tabular}{lll}
    \toprule
    Unité & Nature de la date &\\
    \midrule
    page&lancement du crawl & \tikzmark{start}\\
    page&téléchargement &\\
    page&dernière modification & \tikzmark{end}\\         
  \bottomrule
\end{tabular}
  \bigskip
  \caption{Échelle de datation d'une page Web archivée}
\end{table} 

\begin{tikzpicture}[overlay,remember picture]
\draw[->] let \p1=(start), \p2=(end) in ($(\x1,\y1)+(0.8,0.2)$) -- node[label=right:précision historique] {} ($(\x1,\y2)+(0.8,0)$);
\end{tikzpicture}

\noindent En nous appuyant sur cette grammaire, nous souhaitons maintenant discuter de trois biais majeurs dont il faut prendre connaissance avant de débuter toute exploration.

\subsection{Cécité de crawl}

\noindent À ce que nous appelons cécité de crawl\footnote{\textit{Crawl blindness} en anglais} correspondent l'ensemble des changements subis par une page Web mais non captés par le crawler ou, tout au moins, mal daté par ce dernier. C'est une notion assez intuitive, dont nous donnons une illustration avec la figure \ref{fig:crawl_blind}. Dans cet exemple, une page $p_1$ subit quatre évolutions successives $e_1, e_2, e_3, e_4$ correspondant respectivement à : la publication d'une image accompagné d'un texte ($e_1$ puis $e_2$), la publication d'une seconde image directement suivie par sa suppression ($e_3$ puis $e_4$). Aux yeux de l'explorateur seul le résultat de $e_2$ restera gravé dans les archives (points bleus). Jamais il n'aura connaissance de l'état $e_1$ ni même de l'existence de $e_3$.     

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/crawl_blind}
  \caption{Cécité de crawl pour une page $p_1$}
  \label{fig:crawl_blind}
\end{figure*}

\noindent Ces loupés sont essentiellement dus à la difficulté de calibrer un crawler vis à vis de la fréquence de mise à jour d'un site (Section \ref{sec:3_constituer}).

\subsection{Cohérence entre pages}

\noindent Dans les archives discrétisées du Web, deux pages collectées ne sont pas forcément cohérentes l'une envers l'autre. Prenons l'exemple de deux pages du Web vivant connectées deux à deux par un lien de citation hypertexte. L'une citant l'autre. Depuis la présentation de l'Atlas e-Diasporas (Section \ref{sec:2_atlas}), nous savons à quel point la nature de ces liens est importante aux yeux des sociologues et historiens.

Mais qu'en est-il, si dans les archives la capture de ces sites est espacée de plusieurs mois ou de plusieurs années ? Ce lien a-t-il encore du sens ? Peut on dire que ces pages sont toujours cohérentes entre elles ? Sur ce point, M. Spaniol \citep{spaniol_data_2009} propose une définition générale de la \textbf{cohérence} entre deux pages archivées, ainsi :

\begin{itshape}
\begin{enumerate}[leftmargin=*]  
\item Une page est toujours cohérente avec elle même
\item L'intervalle d'invariance $[\mu_i(p_j),\mu_i(p_j)^*]$ de la page $p_j$ est borné par la date de dernière modification $\mu_i(p_j)$ par rapport à $ t_i(p_j)$ et le prochain changement $\mu_i(p_j)^*$ subit par $p_j$ directement après $t_i(p_j)$
\item Deux pages ou plus sont cohérentes si il existe un seul point dans le temps (ou un intervalle) $t_{\mathrm{coherence}}$ tel que l'on puisse trouver une intersection non vide des intervalles d'invariance de toutes ces pages :
\end{enumerate}
\[
	\forall p_j, \exists t_{\mathrm{coherence}}:t_{\mathrm{coherence}} \in \bigcap^n_{i=j}[\mu_i(p_j),\mu_i(p_j)^*] \neq \emptyset
\]
\end{itshape}

\noindent La cohérence, telle qu'énoncée ici, est une cohérence absolue. Il suffit d'un unique chevauchement d'invariance, même en dix années de collecte, pour dire de deux pages qu'elles sont cohérentes.

Or l'explorateur d'archive est avant tout un observateur, son point de vue est situé : dans l'espace (une URL donnée) autant que dans le temps (une date, un intervalle). Au cours d'une exploration, nous serons plus souvent amené à nous demander si deux pages sont cohérentes par rapport à notre point d'observation $t_i(p_j)$ plutôt que dans le cas général. Cette focalisation du regard est ce que M. Spaniol nomme \textbf{cohérence par observation}\footnote{\textit{Observable coherence} en anglais et dans la littérature \citep{spaniol_data_2009}} et qu'il définit comme suit :\\

\begin{itshape}
\noindent Deux pages ou plus sont cohérentes par observation, si il existe un seul point dans le temps $t_{\mathrm{coherence}}$ tel que l'on puisse trouver une intersection non vide d'intervalles couvrant respectivement la date de téléchargement $t_i(p_j)$ et la date de dernière modification correspondante $\mu_i(p_j)$ (avec $\mu_i(p_j) \leq t_i(p_j)$) :
\[
	\forall p_j, \exists t_{\mathrm{coherence}}:t_{\mathrm{coherence}} \in \bigcap^n_{i=j}[\mu_i(p_j),t_i(p_j)] \neq \emptyset
\]
\end{itshape}

\noindent La figure \ref{fig:coherence} illustre pour deux points d'observation successifs, la notion de cohérence par observation. Dans le premier cas $p_1$ et $p_2$ sont effectivement cohérentes. Dans le second, les intervalles d'invariance ne se chevauchant malheureusement pas, il n'est pas possible de dire de $p_1$ et $p_2$ qu'elles sont cohérentes. 

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/coherence}
  \caption{Cohérence par observation entre les pages $p_1$ et $p_2$}
  \label{fig:coherence}
\end{figure*}

\subsection{Contenus dupliqués}

\noindent L'une des particularités du formalisme DAFF est justement de ne pas dupliquer dans les archives des ressources Web qui n'auraient pas évolué (Section \ref{sec:3_constituer}). Seules les pages ayant subi une transformation sont ainsi re-collectées. Néanmoins, d'un crawl à l'autre, il est possible qu'une partie du contenu de la page soit similaire à la version précédemment capturée et ce malgré les divers changements qu'elle aurait pu subir. On pense notamment aux pages d'accueil des sites d'actualités ou des forums qui présentent des informations publiées sous la forme de listes où chaque nouvel élément est inséré en en-tête. Mécaniquement certains éléments peuvent se retrouver à plusieurs enregistrés dans les archives comme le présente la Figure \ref{fig:duplicate}.

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/duplicate}
  \caption{Contenu d'une page (en rouge) collecté plusieurs fois}
  \label{fig:duplicate}
\end{figure*}

\noindent Cela peut poser de lourds biais d'analyse si l'on cherche par exemple à connaitre la distribution d'un mot clé extrait des pages archivées. Ce dernier pourra, du fait de la structure même du corpus, être artificiellement sur-représenté dans les résultats. 

\section{Construire un moteur d'exploration d'archives Web}
\label{sec:4_moteur}

\noindent Ces différents biais maintenant présentés, nous pouvons nous tourner vers la description de l'architecture de notre moteur d'exploration d'archives Web. Nos corpus étant en DAFF, il n'a pas forcément été possible de réutiliser des éléments open-source issus d'autres moteurs (quasiment tous conçus pour accueillir du WARC). De fait cette section est intéressante pour qui souhaite mettre en place ou comprendre dans le détail la mécanique d'une tel système. Notre architecture suit néanmoins la structure classique d'une chaîne d'extraction, d'analyse et de visualisation de données à grande échelle \citep{marz_big_2015}. La figure \ref{fig:architecture} donne à voir une illustration de son fonctionnement.  Il s'agira donc ici d'une description plutôt orientée ingénierie dont la majeure partie a fait l'objet d'une publication démonstration\footnote{Lobbé, Q. (2018), \textit{Revealing Historical Events out of Web Archives}, TPDL 2018}.

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/architecture}
  \caption{Architecture de notre moteur d'exploration d'archives Web}
  \label{fig:architecture}
\end{figure*}

\subsection{Extraction et enrichissement}

\noindent Nous suivons ici l'hypothèse que notre moteur doit rendre accessible les archives Web d'une seule e-Diaspora à la fois. Nous prendrons comme exemple les archives du corpus marocain.
 
La première étape consiste à extraire les informations contenues dans les fichiers DAFF. Rappelons que chaque corpus est divisé en deux fichiers DAFF : les données d'une part (\textit{data}) et les méta données (\textit{metadata}) d'autre part. Pour ce faire, nous commençons par adapter une librairie JAVA fournie par les équipes de l'INA (\textit{dlweb-commons}) qui cherche à transférer les archives des fichiers DAFF vers le système de stockage d'Hadoop\footnote{Voir \url{https://hadoop.apache.org/} et \url{https://fr.wikipedia.org/wiki/Hadoop}}, le Hadoop Distributed File System (HDFS). Le HDFS est un système de fichiers qui permet de manipuler de larges volumes de données, de manière distribuée (ie : réparti entre plusieurs machines) et relativement scalable (ie: pouvant supporter une forte montée en charge). Le format DAFF a ceci de limitant, qu'il reste pensé pour le stockage et non pour la manipulation des données. Filtrer un fichier DAFF par URL ou date de téléchargement n'est, par exemple, pas trivial.

Une fois chargées dans le HDFS, nos data et metadata sont envoyées dans un pipeline de traitement nommé Spark\footnote{\url{https://spark.apache.org/}}. Spark permet de travailler par batchs (ie: par petits lots de données) dans un environnement distribué : c'est à dire que les data et metadata seront segmentées en sous ensembles plus facilement manipulables, puis répartis sur plusieurs machines où elles subiront toutes les mêmes traitements en parallèle (filtres, jointure, groupement, etc). Spark est un outil flexible dans lequel nous pouvons définir une suite d'instructions ayant pour finalité la fusion des data et metadata en une seule et même source de données. La figure \ref{fig:spark} décrit la manière dont s'enchainent ces diverses transformations. Les metadata sont traitées en premier et peuvent suivant la configuration du système être filtrées par date de téléchargement ou nom de domaine (ie: par site Web). Puis, en nous rappelant que les metadata possèdent chacune un pointeur vers le bloc de data dont elles sont l'extension (champ \textit{content} en DAFF, Section \ref{sec:3_constituer}), nous remplaçons l'identifiant des metadata pour l'identifiant de la data correspondante. Cette manipulation nous permet ensuite de grouper les metadata par identifiants communs, c'est à dire, toutes les metadata d'une seule et même chaine de persistance (Section \ref{sec:4_temporalite}). C'est à cette étape que nous identifions notamment les dates de dernière modification. De là, nous opérons une jointure entre les metadata et data afin de rassembler enfin les méta données du crawler et le contenu même des pages archivées. Pour terminer, nous préparons nos données à être envoyées dans le moteur de recherche, sans oublier de les enrichir avec des informations tirées de l'Atlas e-Diasporas (type de sites, langue, etc).  

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/spark}
  \caption{Transformation des data et metadata dans Spark}
  \label{fig:spark}
\end{figure*}

\noindent Deux configurations différentes ont été testées pour Spark, l'une distribuée entre plusieurs machines d'un même cluster (ie: groupe de machines), l'autre distribuée sur l'ensemble des cœurs d'une seule machine puissante. Si la première configuration s'est révélée la plus rapide (traitement de l'ensemble du corpus marocain en 3 jours), il a fallut néanmoins s'en détourner. En effet, Spark étant particulièrement dur à piloter sur un réseau souvent instable, il était régulier de voir le traitement des données s'arrêter après avoir perdu connexion avec une ou plusieurs machines. De fait, nous avons préféré nous contenter de la seconde configuration, plus lente (une dizaine de jours) mais garantissant la totalité du traitement. 

\subsection{Adapter un moteur de recherche}

\noindent Au cours de la Section \ref{sec:3_constituer}, nous avons présenté l'ensemble des méthodes aujourd'hui utilisées pour fouiller dans les archives. La plupart d'entre elles, s'appuient sur l'utilisation de moteurs de recherches qui offrent la possibilité de requêter des documents\footnote{Les données manipulées par des moteurs de recherche sont de manière générale appelées \textit{documents}} en plein texte. De notre côté, nous avons choisi d'adapter une solution open-source existante Solr/Lucene à la nature particulière de nos archives Web. Solr\footnote{Voir \citep{grainger_solr_2014} et \url{http://lucene.apache.org/solr/}} est un serveur de \textit{search}, c'est à dire qu'il permet de faire le lien entre une requête utilisateur (un mot clé, une dimension, etc) et un ensemble de documents préalablement indexés. Solr est construit au dessus de la librairie d'indexation Lucene\footnote{Voir \citep{hatcher_lucene_2004} et \url{http://lucene.apache.org/index.html}} dont le principe de base est de stocké un texte dans un \textbf{index inversé}.

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/index}
  \vspace*{0.2cm}  
  \caption{Principe de base d'un index inversé}
  \label{fig:index}
\end{marginfigure} 


Le fonctionnement basique d'un index inversé est illustré par la figure \ref{fig:index}. Un premier tableau de données (a) renferme le contenu de deux documents $doc 1$ et $doc 2$ dont les identifiants (\textit{doc id}) sont respectivement $0$ et $1$. Un second tableau de données (b) contient ce que l'on appelle un dictionnaire de termes (\textit{term dict}). Dans ce dictionnaire, sont répartis l'ensemble des mots uniques (numérotés de $1$ à $5$) contenus dans $doc 1$ et $doc 2$. Ces mots sont identifiés par des \textit{term id} et sont, de plus, associés à une \textit{posting list} qui fait correspondre à chaque mot la liste des documents où il est présent. 

Ainsi, pour réaliser une recherche plein texte, il suffira de parcourir l'ensemble du dictionnaire jusqu'à trouver les mots correspondants à la requête de l'utilisateur et, par extension, les documents associés. Ces deux tableaux de données (a) et (b) forment ce que l'on appelle un \textit{segment}, soit le bloc de base de tout index inversé. On nomme \textbf{indexation} l'action de stocker un texte dans un index inversé. Différentes stratégies peuvent être mises en place pour accélérer la recherche dans un index, en optimiser la taille, etc.\footnote{Pour de plus amples détails sur l'indexation via Lucene/Solr, voir mon cours sur le fonctionnement interne des moteurs de recherche (Lobbé, Q. 2016, Voyage au cœur d'un index Lucene, \url{http://qlobbe.net/ressources/search.pdf})}. 

Un moteur de recherche se doit d'ordonner ses résultats avant de les retourner à l'utilisateur. On nomme cette étape le \textit{ranking}. Pour cela, il fait appel à une \textbf{fonction de similarité} qui trie les documents résultants en leur attribuant à chacun un score. Parmi les nombreux critères de ranking, les systèmes de search favoriseront souvent les documents où les mots clés recherchés sont les plus fréquents. Cette mesure sera pondérée par l'ajout d'une prime aux termes rares, c'est à dire : peu présents dans l'ensemble des indexes. C'est tout le sens du fameux \textit{tf-idf}\footnote{\textit{Term frequency-inverse document frequency}, fonction de similarité qui évalue l'importance d'un terme dans un document au regard d'un corpus donné (\url{https://fr.wikipedia.org/wiki/TF-IDF})} dont nous réutilisons ici une version légèrement modifiée : la \textit{defaultSimilarity} de Lucene\footnote{En plus du simple tf-df, cette fonction de similarité prend en compte la taille du document et la taille des champs vérifiant la requête (\url{http://lucene.apache.org/core/4_0_0/core/org/apache/lucene/search/similarities/TFIDFSimilarity.html})}. 

Nous n'avons pas eu l'occasion de tester une fonction de similarité propre aux archives Web, la nôtre est finalement très générique. C'est pourtant une question  intéressante, puisque comme le suggère G. Weikum \citep{weikum_longitudinal_2011}, les moteurs d'exploration d'archives pourraient prendre en compte l'aspect temporel des documents collectés. En se basant sur les dérivées premières et secondaires du rapport entre deux dates de téléchargement, il serait ainsi possible de traduire une forme de vitesse ou d'accélération de certains termes dans les archives Web.  

\subsection{Le schéma d'indexation}

\noindent Tout document destiné à l'indexation doit d'abord passer au crible du \textbf{schéma} qui est considéré comme la pierre angulaire de tout moteur de recherche. 

Le schéma est un fichier décrivant dans le détail la façon dont tout document sera indexé, il forme l'ossature de l'indexation. En effet, un document n'est pas indexé d'un seul tenant. Pour maximiser les chances de le voir matcher une requête, il peut être nécessaire de le découper en plusieurs champs (\textit{fields}), ayant chacun des attributs particuliers. Par exemple, un article issu d'un site de news pourra être segmenté suivant son titre, sa date, l'auteur et finalement le cœur du texte. Ce dernier sera indexé de manière classique en vue d'une recherche plein texte, l'auteur en revanche pourra être destiné à une recherche par \textit{facet}, c'est à dire par dimension  (ie: Quels sont les textes de tel ou tel auteur ?). Si tel est le cas, son indexation se fera via des \textit{docValues}\footnote{\url{https://lucene.apache.org/solr/guide/6_6/docvalues.html}}. Dans notre schéma, la plupart des informations issues du fichier de méta données DAFF sont destinées à une recherche par facet, telles que :\\


\begin{fullwidth}
\small
\begin{verbatim}
<field name="id"                type="string"  indexed="true"    multiValued="false" required="true" />

<!-- archive fields -->
<field name="archive_active"    type="boolean" indexed="true"    multiValued="false"/>
<field name="archive_corpus"    type="string"  indexed="true"    docValues="true" multiValued="false"/>
<field name="archive_ip"        type="string"  indexed="true"    docValues="true" multiValued="false"/>
<field name="archive_length"    type="double"  indexed="true"    docValues="true" multiValued="false"/>
<field name="archive_level"     type="int"     indexed="true"    docValues="true" multiValued="false"/>
<field name="archive_referer"   type="string"  indexed="true"    docValues="true" multiValued="false"/>
<field name="archive_mime"      type="string"  indexed="true"    docValues="true" multiValued="false"/>
<field name="is_page"           type="boolean" indexed="true"    multiValued="false" default="false"/>    

<!-- client fields -->
<field name="client_country"    type="string"  indexed="true"    docValues="true" multiValued="false"/>
<field name="client_ip"         type="string"  indexed="true"    docValues="true" multiValued="false"/>
<field name="client_lang"       type="string"  indexed="true"    docValues="true" multiValued="true" />

<!-- crawl fields -->
<field name="crawl_id"          type="string"  indexed="true"    docValues="true" multiValued="true" />
<field name="crawl_id_f"        type="string"  indexed="true"    docValues="true" multiValued="false"/>
<field name="crawl_id_l"        type="string"  indexed="true"    docValues="true" multiValued="false"/>
<field name="crawl_date"        type="date"    indexed="true"    docValues="true" multiValued="true" />
<field name="crawl_date_f"      type="date"    indexed="true"    docValues="true" multiValued="fasle"/>
<field name="crawl_date_l"      type="date"    indexed="true"    docValues="true" multiValued="true" />
\end{verbatim} 
\end{fullwidth}

\newpage
\begin{figure*}
\begin{fullwidth}
\small
\begin{verbatim}
<!-- download fields -->
<field name="download_date"     type="date"    indexed="true"    docValues="true" multiValued="true" />
<field name="download_date_f"   type="date"    indexed="true"    docValues="true" multiValued="false"/>
<field name="download_date_l"   type="date"    indexed="true"    docValues="true" multiValued="false"/> 

<!-- page fields -->
<field name="page_site"         type="string"  indexed="true"    docValues="true" multiValued="false"/>
<field name="page_url"          type="string"  indexed="true"    docValues="true" multiValued="false"/>
<field name="page_url_id"       type="string"  indexed="true"    docValues="true" multiValued="false"/>      

<!-- extracted page fields -->
<field name="page_link"         type="string"  indexed="true"    docValues="true"  multiValued="true"/>     
<field name="page_meta_title"   type="string"  indexed="true"    docValues="false" multiValued="false"/>
<field name="page_meta_desc"    type="text"    indexed="true"    docValues="false" multiValued="false"/>
<field name="page_meta_img"     type="string"  indexed="true"    docValues="false" multiValued="false"/>
<field name="page_meta_date"    type="date"    indexed="true"    docValues="true"  multiValued="false"/>    
<field name="page_meta_author"  type="string"  indexed="true"    docValues="true"  multiValued="false"/>    
<field name="page_title"        type="text"    indexed="true"    docValues="false" multiValued="false"/>

<!-- searchable page fields -->
<field name="page_text"         type="text"    indexed="true"  stored="false" multiValued="true"/>
<field name="page_text_shingle" type="shingle" indexed="true"  stored="false" multiValued="true"/> 
\end{verbatim} 
\end{fullwidth}
\caption{Schéma d'indexation de notre moteur d'exploration d'archives Web}
\label{fig:schema_1}
\end{figure*}

\noindent Pour chaque champ, nous définissons un nom et un type : une date, un nombre (int), du texte (string et text), ... Tous les champs sont indexés (\textit{indexed="true"}). Ceux destinés à une recherche par dimension sont associés à une docValue. Certain champs, comme les dates de téléchargement, sont multivalués (une page peut avoir été crawlée plusieurs fois à l'identique). Le champ de recherche plein text par défaut est le champ page\_text qui couvre l'ensemble du texte d'une page archivée\footnote{Ce champ subit un traitement particulier (un découpage en bi-gram : page\_text\_shingle) dont nous reparlerons en section \ref{sec:retour_au_moteur}}. Dans Spark, lors de la transformation des DAFF en document Solr, nous extrayons les liens de citation hypertextes (page\_link) et les informations contenues dans l'en tête des pages\footnote{Tout ce qui est contenu dans les balises HTML \textit{<meta>} (\url{https://www.w3schools.com/Tags/tag_meta.asp})} (page\_meta\_img, page\_meta\_date, etc). Les diverses dates associées à une page sont également présentes : allant de la date de lancement du crawl (crawl\_date\_f), à la date de téléchargement (download\_date) et en passant par la date de dernière modification (download\_date\_f).

Mais les documents à indexer ne sont pas les seuls à devoir passer par le schéma. En effet, un moteur de recherche est un système à double entrée (Figure \ref{fig:architecture}), conjuguant des documents et des requêtes utilisateurs grâce à une fonction de similarité. Pour ce faire, documents et requêtes doivent parler la \textit{même langue}, c'est à dire qu'une requête utilisateur devra être traitée de la même manière que les documents à indexer, subir les mêmes transformations et interroger les bons champs. Cette suite de transformations est appelé \textit{analyzer}\footnote{Pour plus d'informations sur les analyzers et tokenizers dans Solr: \url{https://wiki.apache.org/solr/LanguageAnalysis}}. Sur ce point, documents et requêtes suivent les traitements suivants : tout d'abord, les majuscules deviennent minuscules (Figure \ref{fig:tokenizer}, (a)), puis le texte est découpé en termes distincts (Figure \ref{fig:tokenizer}, (b)) et les \textit{stopwords}\footnote{Mots qui ne sont généralement pas intéressants pour l'analyse : et, il, elle, ... (\url{https://fr.wikipedia.org/wiki/Mot_vide})} sont écartés (Figure \ref{fig:tokenizer}, (c)), s'en suit une phase appelée \textit{stemming} dans laquelle on ne garde finalement que la racine des termes restants (Figure \ref{fig:tokenizer}, (d)) avant indexation (Figure \ref{fig:tokenizer}, (e)).

\begin{figure}%
  \includegraphics[width=\linewidth]{graphics/tokenizer}
  \caption{Cycle de transformation d'un texte dans notre moteur de recherche}
  \label{fig:tokenizer}
\end{figure}

\noindent Dans le cas particulier des archives Web, l'exploration de pages collectées peut être focalisée autour d'un instant précis. Si l'utilisateur en fait la demande, notre moteur lui proposera différentes stratégies de recherche pour retrouver les pages les plus proches d'une date donnée: soit en amont (Figure \ref{fig:date-picker}, (a)) , soit en aval (Figure \ref{fig:date-picker}, (c)) ou soit autour de cette dernière (Figure \ref{fig:date-picker}, (b)). Internet Archive par exemple utilise la première option dans la WayBack Machine.

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/date-picker}
  \caption{Stratégies de choix d'un ensemble de pages par rapport à une date précise}
  \label{fig:date-picker}
\end{figure*}

\noindent Notre moteur de recherche est finalement déployé sur les serveurs. Les indexes sont distribués entre plusieurs machines pour améliorer les temps de réponse et d'indexation du système. On appelle \textit{sharding} l'action de scinder un indexe en plusieurs sous indexes avant de les distribuer. Nous suivons une configuration classique \textit{master-slave}\footnote{\url{https://lucene.apache.org/solr/guide/6_6/solrcloud.html}} où l'instance maître de notre moteur de recherche centralise les requêtes utilisateur avant de les dispatcher entre ses diverses instances esclaves qui, elles seules, sont habilitées à retourner des résultats. 

\subsection{Interface de visualisation}

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/gui}
  \caption{Capture d'écran de notre interface de visualisation}
  \label{fig:gui}
\end{figure*}

\noindent Une système de visualisation et d'interrogation des archives est dé\-veloppé\footnote{Nommé \textit{Peastee} en référence au narrateur de la nouvelle de H.P. Lovecraft \textit{The Shadow Out of Time} (1935), ce système est téléchargeable ici \url{https://github.com/lobbeque/peastee}} au dessus de notre moteur. Il s'agit en fait d'un \textit{service Web} permettant à tout utilisateur d'écrire une requête et de se voir présenter les résultats sous diverses formes. Un service Web est composé de deux briques logicielles distinctes : un serveur en \textit{node.js}\footnote{\url{https://nodejs.org/en/about/}} se charge tout d'abord de faire la liaison avec le moteur de recherche, une interface Web permet ensuite de visualiser les documents archivées. Les éléments de visualisation sont développés en \textit{d3.js} et l'architecture de l'interface en tant que telle se base sur \textit{angularjs}\footnote{D3 est une librairie Javascript de visualisation de données \url{https://d3js.org/}, Angular est un framework Javascript pour les applications Web \url{https://angularjs.org/}}. Notre interface suit un modèle \textit{en liste} très classique : les résultats sont présentés les uns à la suite des autres et des facets, à la marge, permettent de les filtrer ou de les trier après coup. Divers histogrammes offrent à voir une répartition dans le temps des pages archivées ayant matché la requête de l'utilisateur. La figure \ref{fig:gui} présente une capture d'écran de cette interface, une démonstration en vidéo permet de se faire une idée plus précise de son fonctionnement\footnote{\url{https://youtu.be/snW4O-usyTM}}

Notre service Web accueille les nombreux prototypes que nous avons pu expérimenter tout au long de ces trois années de travail. Nous ne reviendrons pas ici en détail sur leurs développements respectifs, mais bien qu'incomplets ou inabouties, ces prototypes restent des jalons qui nous ont permis de cheminer vers les résultats présentés au chapitre~\ref{chap:6}. On retiendra une visualisation \textit{en oursin} de l'arborescence d'un site archivé mois après mois (Figure \ref{fig:prototypes}, (a)) ou encore une distribution temporelle des liens hypertextes sortant d'une page (Figure \ref{fig:prototypes}, (b), différenciés par catégories : réseaux sociaux, sites migrants e-Diasporas et reste du Web). 

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/prototypes}
  \caption{Prototypes de visualisation d'archives Web}
  \label{fig:prototypes}
\end{figure*}

\section{Les archives ne sont pas des traces directes du Web}
\label{sec:4_legacy}

\noindent Notre moteur d'exploration maintenant présenté, nous voilà enfin en mesure d'interroger les archives de l'Atlas e-Diasporas. Depuis le chapitre \ref{chap:2}, le site \textit{yabiladi.com} et plus particulièrement son forum de discussion attire notre attention. De part la place qu'il occupe dans le corpus marocain depuis le début des années 2000, le site a su jouer un rôle clé pour l'ensemble de la diaspora en ligne. 

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/yabiladi-map}
  \vspace*{0.2cm}  
  \caption{\textit{yabiladi.com} (rouge) dans l'e-Diaspora marocaine}
  \label{fig:yabiladi-map}
\end{marginfigure} 

Notre première requête consiste donc à voir la répartition de \textit{yabiladi.com} dans les archives, saisir et comprendre la dynamique des publications postées sur le forum afin d'identifier des moments clés de l'histoire du site. Mais les résultats que nous retourne notre moteur d'exploration semblent très curieux, la figure \ref{fig:yabiladi-download} présente ainsi la répartition du nombre de pages collectées par jour pour \textit{yabiladi.com}\footnote{Pour être plus précis nous ne conservons que les pages de la section forum du site}, de Mars 2010 à Septembre 2014. Il semble que le site ait littéralement cessé de produire du contenu de Janvier 2013 à début 2014. Or, si l'on passe maintenant par la Wayback Machine\footnote{Voir \url{https://web.archive.org/web/20130801000000*/http://yabiladi.com/}}, on se rend rapidement compte que chez, Internet Archive, des ressources Web ont bel et bien été capturées à ces même dates. Où se situe donc notre erreur ?

\begin{figure}%
  \includegraphics[width=\linewidth]{graphics/yabiladi-download}
  \caption{Distribution du nombre de pages archivées par jours pour \textit{yabiladi.com}}
  \label{fig:yabiladi-download}
\end{figure}

\noindent Nous interprétons nos résultats de la mauvaise manière. Depuis le début de ce chapitre, nous cherchons à mettre en évidence les nombreux biais d'analyse possibles pour qui souhaite mener à bien une exploration d'ar\-chives Web. Nous avons notamment évoqué les cécités de crawl (Section \ref{sec:4_temporalite}), mais sans nous attendre à trouver dans nos propres corpus une telle défaillance de la collecte. Après enquête auprès des équipes de l'INA, il s'avère que le crawler de l'institution a stoppé sa collecte de \textit{yabiladi.com} pendant toute une année, avant qu'archivistes et chercheurs ne s'en rendent compte et relancent le robot. Ce que la figure \ref{fig:yabiladi-download} donne à voir est un \textit{artéfact de crawl}. C'est à dire un effet mécanique du crawler qui aura influencé la forme même du collectage, au delà du simple décalage ou de l'imprécision inévitable pour ce type de campagne. 

Les archives Web ne sont pas les traces directes du Web, elle sont les traces directes des crawlers. Les outils de collecte façonnent l'image de ce qu'ils sont supposés préserver. Ils en modifient la forme, potentiellement le fond, et par là même, la nature des interprétations que nous ferons du corpus si l'on n'y prend pas garde. Ce que l'on voit dans les archives reste avant le geste de l'archiviste et des ses dispositifs de capture. 

\begin{center}
	*
\end{center}

\noindent En 1838, L. Daguerre réalise un daguerréotype\footnote{Procédé photographique basé sur l'exposition à la lumière d'une surface d'argent pure (\url{https://fr.wikipedia.org/wiki/Daguerréotype})}, le "\textit{Boulevard du Temple}", qui est aujourd'hui reconnu comme l'une des première photographie figurant un être humain : deux hommes seuls dans une rue vide (Figure \ref{fig:boulevard}). En réalité, le boulevard était ce jour là bondé. Demandant un temps d'exposition particulièrement long, les seuls sujets (en plus des bâtiments et des arbres) que le dispositif a su capturer sont ceux qui étaient restés pratiquement immobiles : un cireur de chaussures et son client assis devant lui. Cet exemple illustre parfaitement ce que nous rencontrons dans les archives Web. Associées à des dates de téléchargement qui les arrachent à leur temporalité, les archives Web sont des objets discrétisés et figés. Sans lien direct avec la réalité dont elles sont pourtant censées être le reflet. 

Afin d'améliorer la pertinence historique de nos analyses à venir et pour s'affranchir des crawlers et de leurs artéfacts, nous proposerons, dans la suite de ce manuscrit, de descendre au delà du niveau des pages Web capturées et de s'appuyer sur une nouvelle unité d'exploration des archives : le \textit{fragment Web}. 

\begin{figure*}
  \centering
  \includegraphics[width=\linewidth]{graphics/boulevard}
  \caption{"Boulevard du Temple", Louis Daguerre, 1838}
  \label{fig:boulevard}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Chapitre 5 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Fragmenter les Archives Web}
\label{chap:5}

\par\noindent Les artefacts de crawl sont indissociables des archives Web telles que nous les connaissons (Section \ref{sec:4_legacy}). Ils sont liés organiquement à la structure même des ressources collectées (Section \ref{sec:3_constituer}), issus de l'as\-sociation d'une page Web et d'une date de téléchargement. Ces artefacts et leurs effets induisent nombres de biais pour qui souhaite explorer le Web passé : collectages non régulier, sur-représenta\-tion de certaines parties d'un site, incohérences entre les contenus préservés, etc. 

Nos travaux portant sur l'exploration de corpus d'archives Web déjà existants ou constitués de longue date, nous ne proposerons pas ici d'alternative aux formats WARC et DAFF. Nous chercherons plutôt à définir, partant d'une collecte terminée, une stratégie d'analyse capable de s'affranchir de l'héritage pesant des crawlers ou, tout au moins, d'en atténuer les effets. Par ailleurs, nous souhaitons mener une exploration large (en terme de pages à visiter) et profonde (en terme de durée à balayer) de nos corpus d'archives Web. Mais ce faisant, nous voulons aussi garder la possibilité de nous appuyer, au besoin, sur une analyse plus fine de certains éléments. Cela implique le développement d'une méthodologie hybride capable de débrayer du quantitatif vers le qualitatif. Cette approche s'articulera autour d'une nouvelle entité qui pourra faire cohabiter traitements algorithmiques à grande échelle et campagnes de validation humaine.

Sur ce point, nous proposerons dans ce chapitre de changer d'unité d'exploration en introduisant les \textbf{fragments Web}. Nous pensons, en effet, qu'il peut être bénéfique de mener une analyse au dessous du niveau des pages Web archivées. Pour valider cette intuition, le fragment Web offrira aux explorateurs une plus grande souplesse et de nouveaux outils pour interroger les archives. Il se voudra également object d'étude à part entière. À travers les fragments Web, nous questionnerons directement le geste des auteurs et des lecteurs des sites collectés, en suivant les indices de leur passage sur la toile. Revenir à l'humain dissimulé sous les archives. Pour ce faire, nous porterons notre réflexion sur la question de la datation des archives Web en associant à chaque fragment une date d'édition. Ainsi nous nous approcherons, au mieux, du Web tel qu'il a été de son vivant. Enfin, nous reviendrons en miroir sur les modalités techniques et théoriques d'un moteur d'exploration basé, cette fois ci, sur le fragment Web comme unité principale d'indexation. Un cas simple de détection d'événements dans les archives Web nous permettra d'en faire la démonstration.

\section{Au dessous des pages Web}
\label{sec:5_dessous}

\noindent Comme le résume G. Weikum \citep{weikum_longitudinal_2011}, les archives Web sont de véritables mines d'or pour qui souhaite étudier l'histoire du Web passé\footnote{"\textit{These archives host a wealth of information, providing a gold mine for sociological, political, business, and media analysts.}" \citep{weikum_longitudinal_2011}}. Mais tout trésor est difficile d'accès et nous avons déjà évoqué, au regard de l'état de l'art (Section \ref{sec:3_20ans}), à quel point les corpus archivés restaient pour nous des territoires inexplorés, repliés et fortifiés. 

L'archive est pourtant une matière qui ne doit pas rester fermée \citep{ketelaar_de_2006}. Toujours prête à être questionnée. C'est au travers des lectures, discussions et interprétations successives des archives que s'écrit l'histoire. Pour plonger au cœur des archives Web, essayons d'ouvrir une brèche dans nos corpus afin d'y extraire une nouvelle entité. Ce fragment Web, comme nous le nommons, est issue du fractionnement des pages Web collectées. Sa construction s'appuie sur plusieurs éléments, plusieurs inspirations. Tout d'abord, il s'agira pour nous d'adopter une attitude plus souple vis à vis des archives en cherchant à les décomposer pour mieux les explorer. Ensuite, nous inscrirons les fragments dans la droite lignée des strates du Web au sens où les décrit N. Brügger. Nous nous attarderons alors sur la question de la datation des ressources collectées en introduisant les dates d'édition à notre grammaire. Nous nous servirons de ces dates, pour finalement descendre vers une plus grande précision historique et ramener les archives vers la temporalité du Web passé.

\subsection{Découper, déplacer, monter}

\noindent Funes, vit dans l'indexation d'un présent perpétuel (Section \ref{sec:4_derrida}). Condamné à ne plus jamais rien oublier, il lui devient impossible de penser, de raisonner et de s'inventer :

\begin{fullwidth}
"\textit{[Funes], ne l’oublions pas, était presque incapable d’idées générales, platoniques. Son propre visage dans la glace, ses propres mains, le surprenaient chaque fois. (...) Penser c'est oublier des différences, c'est généraliser, abstraire. Dans le monde surchargé de Funes il n'y avait que des détails, presque immédiats.}" --- \citep[p.~117-118]{borges_fictions_1974}\\
\end{fullwidth} 

\noindent Pour mémoriser il faut oublier. Ré-arranger et faire du montage. Nos souvenirs sont des sélections qui, mises bout à bout, collées, accélérées ou ralenties forment le fil de nos histoires et de nos vies. Nous décrivions en section \ref{sec:3_constituer}, comment les conditions d'accès aux archives Web rendaient difficile leur exploration par les chercheurs\footnote{Notons  néanmoins l'existante du projet \url{https://archivesunleashed.org/} et des outils de l'Omilab \url{https://github.com/omilab/internet-archive-link-extractor}}. Chevillées aux niveaux des seules pages Web les outils d'analyse existants (la Wayback Machine tout autant que notre propre moteur d'exploration, Section \ref{sec:4_moteur})) nous nous permettent pas de manipuler les résultats de nos requêtes. Les archives sont consultables, certes, mais restent enfermées dans des \textit{interfaces-vitrines} plutôt que de nous être restituées sur des tables de montage. 

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/marker}
  \caption{C. Marker, 1977, Le Fond de l'Air est Rouge, (\url{https://youtu.be/dO1E4GYjF1s})}
  \label{fig:marker}
\end{marginfigure}

En achevant \textit{Le Fond de l'Air est Rouge} en 1977, le cinéaste C. Marker revient amère sur l'avènement des mouvements contestataires et révolutionnaires dans années 1960, événements dont il a été le témoin direct. Il remonte et assemble 15 années de ses propres archives filmiques qu'il aborde sous un angle inédit : "\textit{on ne sait jamais ce que l'on film, on ne sait jamais ce qu'il y a derrière une image}" (Ibid, Partie II, 14mn 22s) nous dit il en voix off. Détachées de lui et faisant désormais partie de l'histoire, ses archives peuvent enfin être confrontées et ré-interrogées. En cela la posture de l'historien face à un document archivé se rapproche de celle du monteur de cinéma face à une matière filmée. Leurs outils sont semblables. Lorsqu'il invente l'histoire, l'historien découpe, isole et rapproche des sources archivées potentiellement très éloignées. 

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/godard}
  \caption{J.L. Godard, 1993, Je~Vous Salue, Sarajevo, (\url{https://youtu.be/WKbfu8rRrho})}
  \label{fig:godard}
\end{marginfigure}

Dans son court métrage \textit{Je Vous Salue, Sarajevo}, réalisé en 1993 pendant la Guerre de Bosnie-Herzégovine, J.L. Godard déconstruit une photographie du reporteur de guerre R. Haviv. Il fragmente cette image pour faire se correspondre des inserts éclatés à la manière d'un collage-poème ou d'un cinétract\footnote{Mini-films non signés à caractère militant, réalisés en mai et juin 1968 (\url{https://fr.wikipedia.org/wiki/Cin\%C3\%A9tract})}. Par le collage, les fondus et les découpes Godard rompt la continuité de l'archive qu'il utilise comme source première. Il peut ainsi rendre compte, image après image, de la cruauté qui frappe les rues Sarajevo. Le film finit par dévoiler entière, l'image dans toute son horreur., \textbf{décomposer} pour mieux \textbf{recomposer}.\\

\noindent Il y a dans les travaux de Godard et de Marker une souplesse d'action vis à des archives que nous pourrions appliquer à nos propres corpus. Chercher à avoir en main des éléments fragmentés de pages Web éloignées, que nous pourrions associer, à souhait, afin de traiter plus largement d'un moment particulier de l'histoire du Web. Comment se donner la possibilité de rapprocher automatiquement deux contenus archivés hors du carcan de leurs pages Web respectives ? Peut on ralentir ou accélérer le cours de nos archives ? 

\subsection{Les strates du Web}

\noindent Le glissement d'un niveau d'analyse à un autre, vers un en-dessous de la page archivée, est formulé pour la première fois par l'historien du Web N. Brügger lorsque, cherchant à définir le site Web comme objet potentiel de recherches historiques \citep{brugger_website_2009}, ce dernier en vient à introduire la notion de \textbf{strates analytiques du Web}\footnote{En anglais : \textit{analytical Web strata}.}.

Brügger suggère de construire un système d'analyse dynamique pour réajuster, au besoin, le périmètre d'une recherche portant sur le Web. L'observateur doit ainsi pouvoir passer d'un ensemble de sites, à une page unique, voire descendre jusqu'aux éléments constitutifs de cette dernière (un texte, une image, etc)\footnote{"\textit{One can distinguish the following five analytical strata: the web as a whole; the web sphere; the individual website; the individual webpage; and an individual textual web element on a webpage, such as an image}", \citep[p.19]{brugger_website_2009}}. 
Cette approche, notons le, n'est pas confinée au Web archivé, elle peut très bien s'adapter au Web vivant. Brügger définit ainsi 5 niveaux d'analyses, allant du  plus englobant au plus élémentaire, comme l'illustre la figure \ref{fig:web_strata}.

\begin{figure*}
  \centering
  \includegraphics[width=\linewidth]{graphics/strata}
  \caption{Les 5 strates analytiques du Web, d'après \citep{brugger_website_2009}}
  \label{fig:web_strata}
\end{figure*} 

\noindent Le premier niveau englobe l'entièreté des sites du Web vivant (Figure \ref{fig:web_strata}, (1)). Il inclut également les éléments de back-end (base de données, code côté serveur, etc) et plus généralement l'ensemble de l'infrastructure physique du Web (serveurs, câbles réseax, supports numériques, etc). Une sphère Web désigne un ensemble de sites Web sélectionnés par un chercheur (Figure \ref{fig:web_strata}, (2)). C'est une construction ad hoc motivée par une question de recherche donnée, une thématique précise\footnote{La notion de sphère Web est inspirée des travaux de K. Foot sur le volet numérique des campagnes électorales états-uniennes du début des années 2000 \citep{foot_web_2006}}. Les acteurs Web regroupés au sein de ces sélections n'ont pas forcément conscience d'appartenir à un tel groupe. Par exemple, les réseaux de sites e-Diasporas (Section \ref{sec:2_atlas}) peuvent être considérés comme des sphères Web. Sites et pages Web (Figure \ref{fig:web_strata}, (3-4)) sont ensuite définis de manière égale à ce que nous proposions en section~\ref{sec:4_temporalite}. L'élément Web, quant à lui, est considéré comme l'élément textuel minimal d'une page Web\footnote{"\textit{The Web element is the minimal textual element on a webpage}", \citep[p.20]{brugger_website_2009}} (Figure \ref{fig:web_strata}, (5)). Ce peut être un ensemble de caractères écrits sur une page, des images fixes ou mobiles, ainsi que des sons. Brügger en revanche écarte de cette liste les menus, barres d'informations et autres éléments de navigations. \\

\noindent Nous voulons penser le futur fragment Web comme un \textbf{sous ensemble cohérent} d'une page Web. Il s'inscrira dans la continuité des strates du Web, en se situant quelque part entre l'élément Web et la page Web. Un fragment pourra, en fonction des cas, être un élément Web seul, un groupe de plusieurs éléments, voire la page Web dans son entièreté\footnote{Nous reviendrons dans le détail sur la question de l'étendue du fragment Web dans la section~\ref{sec:5_scraping}}. 

Dès à présent, pour tout site Web composé de \textit{n} pages Web $\{p_1$,...,$p_n\}$, nous assumons que chacune de ses pages $p_j$ consiste en \textit{m} fragments Web numérotés $\{f_{j1},...,f_{jm}\}$ (Figure \ref{fig:fragment}).

\begin{figure}%
  \includegraphics[width=\linewidth]{graphics/fragment}
  \caption{Une page $p_1$ et ses fragments Web $f_{11}, f_{12}, f_{13}$}
  \label{fig:fragment}
\end{figure}

\subsection{Dater une page archivée}

\noindent La datation des pages archivées peut être réévaluée à l'aune du fragment Web. Depuis la fin du précédent chapitre une question demeure : Comment tendre vers une plus grande précision historique ? Comment s'affranchir des seules dates de téléchargement ? Comment \textit{bien} dater une page Web et son contenu ?

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/yabiladi-wayback}
  \caption{Répartition des archives de \textit{yabiladi.com} dans la WayBack Machine (\url{https://web.archive.org/web/*/www.yabiladi.com})}
  \label{fig:yabiladi-wayback}
\end{marginfigure}

Les archives Web sont les traces directes des crawlers (Section \ref{sec:4_derrida}). En DAFF ou en WARC, une page archivée sera toujours adressée par sa seule et unique date de téléchargement. Dans la plupart des moteurs d'exploration (par exemple la WayBack Machine, Figure \ref{fig:yabiladi-wayback}), cette date est l'unique dimension temporelle interrogeable. Il est néanmoins possible d'établir une échelle de datation plus complète en introduisant la notion de date de dernière modification (Section \ref{sec:4_temporalite} et Table \ref{tab:datation_1}). Échelle, dont nous pensons maintenant pouvoir à nouveau améliorer la précision, en associant aux futurs fragments Web une \textbf{date d'édition}. 

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/yabi-date-com}
  \caption{Date d'éditions (rouge) d'un post de forum sur \textit{yabiladi.com}}
  \label{fig:yabi-date-com}
\end{marginfigure}  

Une page Web évolue (Section \ref{sec:3_constituer}) dès que son contenu est édité par un tiers : humain ou robot. Par \textit{édition}, nous entendons ici la création, la modification ou la suppression d'un élément d'une page. Comme les actes de modification et de suppression demandent, pour être datés (même approximativement), de comparer deux versions archivées d'une même page \citep{rocco_page_2003, nunes_using_2007}, leur détection semble de prime abord compliquée à intégrer à notre moteur d'exploration. La création d'un message ou d'un commentaire peut en revanche être plus facilement datée. Des indices sont souvent dispersés à même la page (Figure \ref{fig:yabi-date-com}), reste alors à les interpréter et à les formater avant indexation \citep{de_jong_temporal_2005,kanhabua_using_2009}. Si l'en-tête HTTP d'une page Web a été archivé, celui-ci peut nous renseigner sur une date de dernière modification qui ne dépende pas directement du crawler  \citep{amitay_trend_2004}. À défaut, la création d'un contenu donné sera rapportée à sa première apparition sur l'ensemble des versions archivées d'une même page \citep{jatowt_detecting_2007}, cette comparaison peut être affinée si des URIs ont été par ailleurs collectées\footnote{Le système Memento propose de voir une page archivée comme la concaténation de toutes les URIs qu'elle agrège. Cette vue est appelée TimeMaps  \citep{van_de_sompel_http_2013} et peut être exploitée pour comparer les dates de certaines URIs d'images par exemple.} \citep{aturban_difficulties_2017}. Notons enfin qu'il existe des stratégies de datation adaptées à la nature interdépendante de certains contenus archivés, comme un réseau de citation d'articles de blogs par exemple \citep{toyoda_whats_2006,spitz_predicting_2018}. Quoi qu'il en soit, l'identification et l'extraction d'une telle date d'édition reste possible et nous nous y emploierons en section \ref{sec:5_scraping}. \\

\noindent Mais dès à présent, faisons par avance l'hypothèse d'être en capacité de doter chaque fragment Web d'une date d'édition. Ainsi, à tout fragment $\{f_{j1},...,f_{jm}\}$ d'une page $p_j$ nous associons maintenant une date d'édition $\phi(f_{j1}),...,\phi(f_{jm})$. De plus, nous nommons \textbf{date de création} de tout la page $p_j$ la plus ancienne date d'édition de l'ensemble de ses fragments telle que $\min\limits_{k} \phi(f_{jk})$. La figure \ref{fig:edition_creation} décrit l'imbrication de ces nouvelles datations. 

\begin{figure*}
  \centering
  \includegraphics[width=\linewidth]{graphics/edition_creation}
  \caption{Dates d'édition des fragments Web $\{f_{11}, f_{12}\}$ et date de création de la page $p_1$}
  \label{fig:edition_creation}
\end{figure*} 

\noindent Ainsi, nous pouvons mettre à jour de notre échelle de datation en y ajoutant dates de création et d'édition, telles que :

\begin{table}
\hspace{2em}%
  \label{tab:datation_2}
  \begin{tabular}{lll}
    \toprule
    Niveau & Nature de la date &\\
    \midrule
    page&lancement du crawl & \tikzmark{start}\\    
    page&téléchargement &\\
    page&dernière modification &\\
    page&création & \\
    fragment&édition & \tikzmark{end}\\     
  \bottomrule
\end{tabular}
  \bigskip
  \caption{Échelle (actualisée) de datation d'une page Web archivée}
\end{table} 

\begin{tikzpicture}[overlay,remember picture]
\draw[->] let \p1=(start), \p2=(end) in ($(\x1,\y1)+(0.8,0.2)$) -- node[label=right:précision historique] {} ($(\x1,\y2)+(0.8,0)$);
\end{tikzpicture}

\noindent En pratique, tout fragment Web devra être associé à une date d'édition. Dans le cas contraire, sa datation sera rapportée à la date de création de la page Web à laquelle il appartient. Et si bien dater une page archivée participe de son émancipation vis à vis du crawler, cela donne, par la même occasion, corps aux acteurs qui l'ont fait vivre. 

Un article de blog ne s'écrit pas de lui même, il est le fruit du geste d'un auteur (unique ou collectif, humain ou robot) qui l'a mis en ligne. Derrière les dates d'édition des fragments Web, peuvent transparaitre les gestes de divers auteurs : blogueurs, commentateurs ou contributeurs qui deviennent dès lors objets ou dimensions possibles d'une exploration d'archives Web\footnote{Pour le philosophe V. Flusser les gestes sont des séries de mouvements significatifs dont le but est déchiffrable, ils "\textit{montrent la façon dont nous sommes au monde}", \citep[p.319]{flusser_les_2014}}. Serait-il alors possible, comme le suggère l'historien J. Morsel, d'écrire une histoire \textit{symptomale}\footnote{Alors que la trace, telle que nous la décrivions jusqu'ici (Section \ref{sec:4_derrida}), suggère l'absence de l'agent qui l'a produite (elle s'en est détachée), le symptome, selon Morsel, suppose la présence latente de l'agent, coprésent à ce dont il est le signe \citep{morsel_traces?_2016}} \citep{morsel_traces?_2016} à partir de nos corpus d'archives Web ? Cela reviendrait à considérer que certains fragments Web se trouvent chargés de la présence l'attente d'un auteur, dissimulée sous la surface des pages archivées et prête à être questionnée. Cette nouvelle perspective d'exploration nous mènera à considérer, depuis les archives Web, le devenir de communautés d'utilisateurs ou de collectifs d'auteurs tel que nous l'illustrerons dans le Chapitre \ref{chap:6}. Avec le fragment Web, une nouvelle dimension d'analyse des archives s'offre donc à nous : l'exploration par acteur (auteur, contributeur, commentateur, etc), plutôt que la simple exploration par page ou site.

\subsection{Désagréger pour changer de temporalité}

\noindent Le Web est un flot grandissant d'information tout autant qu'un territoire en perpétuelle expansion. Par l'action des crawlers, les archives Web sont arrachées à la temporalité continue du Web vivant pour rejoindre celle figée et discrétisée des corpus collectés (Section \ref{sec:4_derrida}). Les archives, malheureusement, ne peuvent revenir au temps du Web vivant, mais grâce au fragment Web, nous pouvons les faire basculer dans la \textbf{temporalité du Web tel qu'il a été}. Temporalité que nous allons essayer de caractériser ci dessous.  

Commençons par une expérience. Au cours de la section \ref{sec:4_legacy}, nous avions visualisé, dans le temps, la répartition des pages archivées de la section forum du site \textit{yabiladi.com}, et ce, par rapport à leurs seules dates de téléchargement. Maintenant, essayons plutôt de nous focaliser sur les dates d'édition des fragments Web de chacune de ces pages et tentons une comparaison. 

Faute de ne pas avoir encore définit clairement la nature d'un fragment Web, nous nous contenterons, à ce niveau du manuscrit, de l'approximation suivante : chaque \textit{post} (ie: message individuel) publié sur le forum de \textit{yabiladi.com} sera considéré comme fragment de la page dont il dépend. Un post est écrit par un unique auteur (identifié comme tel) et associé à une date d'édition (Figure \ref{fig:yabi-date-com}). 

D'un point de vue pratique, nous procédons à une extraction focalisée\footnote{Cette extraction de données n'est donc pas générique, voir la section~\ref{sec:5_scraping} pour une approche plus générale} dans nos archives, afin de ne conserver que les dates d'édition des posts collectés. Dans Spark, nous modifions le moteur en conséquence, les dates d'édition étant identifiées dans le code des pages Web par un nœud HTML unique : 
\begin{quote}
\small
\begin{verbatim}
<div class="com-date">17 Novembre 2009</div>
\end{verbatim}
\end{quote}
Nous construisons ensuite un index dédié dans Solr. Ne reste plus alors qu'à visualiser les deux distributions côte à côte : d'une part la répartition des pages par date de téléchargement (Figure \ref{fig:edition_vs_download}, bleu) et d'autre part la répartition des fragments correspondants par date d'édition (Figure \ref{fig:edition_vs_download}, rouge) :

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/edition_vs_download}
  \caption{Distribution, pour \textit{yabiladi.com}, du nombre de pages et de fragments archivés par jours et suivant leurs dates de téléchargement (bleu) et de d'édition (rouge) respectives}
  \label{fig:edition_vs_download}
\end{figure*}

\noindent Tout d'abord, la répartition par date d'édition (rouge) semble \textit{gommer} l'artéfact de crawl précédemment observé sur l'année 2013 (bleu). La distribution des fragments est linéaire et ne souffre d'aucune cécité remarquable. Détachée de l'influence du crawler, elle n'en subit plus les effets. 

Par ailleurs, nos archives Web semblent chargées d'une mémoire plus étendue que celle initialement prévue. Ainsi, partant d'une collecte débutée en Mars 2010, nous voilà maintenant capable de considérer et d'analyser des fragments Web édités 7 années plus tôt, jusqu'en 2003 pour les plus anciens. Les pages archivées contiennent, en elles même, les traces sédimentaires de publications antérieures.

Par l'extraction et l'étude des fragments Web, nous nous donnons les moyens d'un saut dans le passé considérable. Ces fragments sont potentiellement porteurs d'une mémoire préexistante à chaque collecte et dont nous pouvons dater avec précision l'apparition. En désagrégeant les archives Web, en les fragmentant, nous changeons une nouvelle fois de temporalité pour entrer dans le temps du Web tel qu'il a été. Là ou le temps des archives était figé et fait de séries de captures discrètes d'une même page, le temps du Web tel qu'il a été est un temps fragmenté. C'est à dire un temps éclaté, où chaque fragment se voit définit relativement par rapport à lui même.

Nos expérimentations pratiques ne portent que sur les seules dates d'édition, mais dans le temps du Web tel qu'il a été, chaque fragment Web suit sa propre temporalité, détachée de celle des autres. Une ligne allant de son apparition sur le Web (date d'édition) jusqu'à sa possible disparition de la toile. Isolées les unes des autres, c'est à l'explorateur d'archives que revient le rôle de naviguer entre ses lignes de temps éclatées. L'explorateur sélectionne, découpe et assemble des fragments pour construire ce que l'anthropologue T. Ingold nomme un \textbf{trajet}, support de l'exploration à venir : \\ 

\begin{fullwidth}
"\textit{Dans le cas du trajet, en revanche, on s'engage dans une voie qu'on a déjà explorée avec d’autres, ou qui a été explorée par d'autres, en reconstruisant l'itinéraire au fur et à mesure de sa progression.}" --- \citep[p.26]{ingold_breve_2013}\\
\end{fullwidth} 

\noindent Un trajet est fait de détours, de contours et de bifurcations. À mesure qu'il se conçoit, le trajet se développe et s'inscrit dans le temps. Suivant le cours de son analyse, c'est par le montage que le chercheur chemine d'un fragment Web à l'autre, dans le sens et l'ordre qu'il juge pertinent. En conjuguant les multiples lignes de temps il s'affranchit ainsi des formes classiques et linéaires d'accès aux archives Web. Ouvrant la voie à de nouveaux degrés de liberté, les fragments pourront être associés sur la base d'un lien hypertexte partagé, d'une présence sur la même page à un instant donné, d'une filiation commune, etc. Ces trajets entre fragments deviennent sous la plume de J. Bashet des \textbf{lignes processuelles} \citep[p.227]{baschet_defaire_2018}. L'historien cherche, ce faisant, à rompre avec une vision linéaire de l'histoire dont il faudrait faire éclater la continuité : \\

\begin{fullwidth}
"\textit{En effet, il ne s'agit en aucun cas de penser l'Histoire tout entière comme un seul processus unifié, mais de saisir, dans l'histoire, un entrelacement complexe de multiples processus.}" --- \citep[p.227]{baschet_defaire_2018}\\
\end{fullwidth} 

\noindent En suivant le devenir historique de multiples lignes processuelles, l'écri\-ture de l'histoire revient à raisonner autour de \textbf{moments} singuliers où convergent et se croisent temporalités et processus hétérogènes :\\

\begin{fullwidth}
"\textit{Et on proposera plutôt d'explorer diverses manière de penser l'événement - le surgissement, le nouveau, la rupture mais aussi l'imprévu, l'imprévisible, l'improbable - à partir d'une pensée des processus. Ainsi, outre qu'elle peut naître ou disparaître, une ligne processuelle connaît par elle-même  des variations de rythme et des moments singuliers de concentration ou d'expansion des forces à l'œuvre : l'événement tient alors à une étape particulière de maturation ou correspond, peut-être à un seuil d'ébullition ou de cristallisation.}" --- \citep[p.227-228]{baschet_defaire_2018}\\
\end{fullwidth} 

\noindent Avec le passage de la page au fragment, nous basculons d'une unité d'exploration à l'autre. Le fragment Web nous invite à un changement d'échelle temporelle et spatiale dans le rapport que nous entretenons aux archives Web. Situé entre la page et l'élément Web, le fragment peut contenir en lui la trace du Web tel qu'il a été : une mémoire jusqu'ici retenue dans les fichiers archivés. Le chercheur associe alors un à un les fragments qu'il juge pertinents et conduit, chemin faisant, son exploration pour saisir l'histoire du Web et ses cristallisations autour de moments singuliers. 

Dans notre méthodologie, la place du chercheur est donc centrale. C'est lui qui, par ses choix de montage (basés sur sa propre expertise ou sur des indices qu'il aura recueilli en amont) définit les fragments Web à explorer et la manière de les parcourir. Il peut, dans cette tâche, se faire aider de scripts informatiques pour automatiser certains traitements. Le fragment Web doit ainsi être interprétable par une machine : un programme pourra l'analyser, le manipuler, le stocker, etc. Mais le fragment doit aussi rester compréhensible, en lui même, afin d'être étudié par un chercheur (sociologue, historien, ...). Nous discuterons, en section \ref{sec:5_scraping}, de l'implication ou non du chercheur dans le choix même de la forme des fragments Web. Nous donnerons, enfin, dans le chapitre \ref{chap:6}, deux exemples d'explorations désagrégées de nos corpus et basées sur le fragment Web.

\section{Le fragment Web : définition}

\noindent La définition suivante est intentionnellement générique. Nous souhai\-tons par là, que d'autres chercheurs puissent se saisir après nous du fragment Web. Par ailleurs, la nature des fragments dépendant beaucoup du contexte de l'analyse et de la sensibilité propre à chaque chercheur, soit qu'il voudra une fragmentation plus ou moins englobante, soit qu'il se satisfera d'éléments abstrait, nous ne donnerons pas ici de définition technique précise du fragment. Nous proposerons, dans la section \ref{sec:5_scraping}, notre propre système d'extraction des fragments Web depuis une page archivée, d'autres approches et stratégies peuvent naturellement exister.\\

\begin{itshape}
\noindent Considérant la page web comme unité de consultation de base du World Wide Web, bâtit sur des modalités d'écriture propre au support numérique et constatant que du point de vue de la perception humaine \citep{bernard_criteria_2003, michailidou_visual_2008} une page web est le résultat de l'agencement logique d'éléments sémantiques distincts, alors nous nommons \textbf{fragment Web} un sous ensemble sémantique et syntaxique d'une page Web donnée.

\begin{enumerate}[leftmargin=*]  
\item Il y a une relation d'échelle entre une page Web et ses fragments Web. Ceux-ci peuvent couvrir l'entièreté de la page ou n'être qu'un élément unitaire de cette dernière
\item Un fragment Web est un assemblage cohérent d'éléments textuels, visuels, sonores ou logiciels extraits d'une page Web. Le fragment Web doit ainsi être compréhensible par lui même.
\item Au sein d'une même page Web, deux fragments Web ne peuvent pas se superposer, même partiellement
\item Certains éléments d'un fragment Web peuvent faire l'objet d'une catégorisation lors de l'extraction. Un fragment Web peut ainsi être associé à un titre, à un auteur, à une date d'édition, etc
\item Le fragment Web capture l'ensemble des dispositifs d'écriture ( nœuds HTML, CMS widgets, éditeurs de texte ... ) et de partage ( liens hypertextes, liens de syndications, liens de publications … ) utilisés pour publier son contenu sur le Web
\end{enumerate}
\end{itshape}

\section{Scraping et méthodologie d'extraction}
\label{sec:5_scraping}

\subsection{Extraire de l'information issue d'une page Web}

\noindent On appelle \textbf{scraping} l'ensemble des techniques et méthodes employées pour extraire de l'information depuis une page Web. Un \textbf{scraper} est, de fait, un robot chargé de scraper une page ou un ensemble de pages données. En pratique, un scraper ne travaille jamais au hasard, il est focalisé, c'est à dire qu'il est paramétré pour ne conserver que certaines parties ou éléments distincts d'une page : des noms, des adresses, des images, des mots clés, etc. La recherche de nos fragments Web passera obligatoirement par une étape d'extraction, il faudra alors scraper l'ensemble des pages archivées.

De prime abord, les scrapers ne considèrent pas les pages Web telles que nous les voyons depuis nos écrans, interprétées par les navigateurs Web. Lors d'une extraction, les scrapers parcourent, d'abord et avant tout des portions de code issues des fichiers HTML et CSS qui forment l'ossature de ces pages. 

Le \textbf{HTML}\footnote{\textit{HyperText Markup Language}} est un langage de programmation décrivant, à la fois, la structure et le contenu d'une page Web. Le HTML est la grammaire de l'hypertexte. Écrites en HTML, les pages sont alors \textit{rendues} par le navigateur Web qui transcrit visuellement les instructions codées.  

\begin{figure}%
  \includegraphics[width=\linewidth]{graphics/html}
  \caption{Ajout successif d'élément HTML, CSS et JavaScript à une page Web et transcription sur l'écran d'un internaute}
  \label{fig:html}
\end{figure}

\noindent Le HTML est un langage à \textbf{balise}, c'est à dire que chaque élément d'une page Web est délimité par une balise ouvrante à une extrémité (ex: \textit{<p>}) et fermante à l'autre bout (ex: \textit{</p>}). La nature de ces balises, leur syntaxe et leur agencement permettent d'enrichir le contenu textuel de l'élément qu'elles définissent (Figure \ref{fig:html}, (a)). Une balise HTML est identifiée par un \textit{tag} qui donne une indication sur le type de l'élément caractérisé (du texte \textit{<p>}, un lien \textit{<link>}, ...). Elle peut être complétée (entre autres) par un \textit{id} (ie: identifiant unique) et une ou plusieurs \textit{class} (ie: attribut qualifiant un ou plusieurs éléments). Classes et id peuvent servir à associer un comportement spécifique à un ou plusieurs éléments cibles.

Une page Web est un document structuré (Figure \ref{fig:html}, (b)). Les éléments HTML s'agencent entre eux suivant la forme particulière d'un arbre : l'\textbf{arbre DOM}\footnote{\textit{Document Object Model Tree}, en anglais. En informatique un arbre est une structure de données où chaque élément (nœud) est codé hiérarchiquement par rapport aux autres (\url{https://fr.wikipedia.org/wiki/Arbre_binaire})}. Par convention, on appelle \textbf{nœud} HTML tout élément d'un arbre DOM. Il existe ainsi un seul et unique nœud racine, plusieurs nœuds parents, enfants, etc. Un scraper peut accéder à un nœud HTML donné soit en l'adressant directement via son id ou sa classe, soit en parcourant l'arbre DOM.  

Le \textbf{CSS}\footnote{\textit{Cascading Style Sheets}, en anglais} est un langage pensé pour décrire l'aspect visuel (rendu et animation) d'un nœud HTML à l'écran. Le CSS transcrit ainsi des règles de style directement depuis le HTML de la page Web ou dans un fichier dédié. Une correspondance est alors faite entre l'id (et/ou la classe) d'un nœud et la règle à lui appliquer (Figure \ref{fig:html}, (c)). On jouera ainsi sur la couleur, la police d'écriture, la marge, les bordures,~... de chaque éléments.  

Les éléments dynamiques d'une page Web sont générés, majoritairement, par du code \textbf{JavaScript} (Figure \ref{fig:html}, (d)). Le JavaScript peut s'écrire dans un ou plusieurs fichiers séparés ou être directement ajouté au HTML, au sein de balises dédiées. Il est alors possible de manipuler des nœuds HTML et de leur attribuer à chacun comportement. En réaction au geste d'un internaute, par exemple : un clic, un défilement, etc. D'autres langages ont, par le passé, également su gérer ces aspects dynamiques. Le \textbf{PHP}\footnote{\textit{Hypertext Preprocessor}, en anglais} a, ainsi, abondamment été employé tout au long des années 2000. Mais il semble, aujourd'hui, en perte de vitesse\footnote{Voir le dernier rapport de StackOverFlow sur les préférences des développeurs en 2018 (\url{https://insights.stackoverflow.com/survey/2018/})}. Au final, HTML, CSS et JavaScript forment le triptyque le plus courant face auquel doivent se débattre les scrapers.\\

\noindent En effet, un scraper doit savoir s'adapter à la complexité des pages qu'il parcourt. Par exemple, une page contenant du JavaScript devra être préalablement interprétée par le scraper (et non juste parcourue), afin de prendre en compte les éléments dynamiques qui ne se chargeraient qu'à l'affichage écran\footnote{Les librairies Beautifulsoup (python, \url{https://www.crummy.com/software/BeautifulSoup/bs4/doc/}) et After-load (nodeJs, \url{https://www.npmjs.com/package/after-load}) peuvent ici nous aider}. Une campagne de scraping se prépare donc en amont, il s'agira alors de définir une stratégie adaptée au contexte de notre extraction. Le scraper peut, ainsi, s'attarder sur les éléments visuels d'une page Web \citep{cai_vips:_2003}, ou se contenter de la seule information sémantique présente dans le HTML \citep{jatowt_detecting_2007}. Là où la première solution sera en quête d'une vision humaine des pages Web, la seconde privilégiera la recherche d'un temps de réponse réponse acceptable. 

La vitesse des traitements est, de fait, une composante essentielle de tout scraping, notamment dans ses applications industrielles. Faut-il parcourir toute la page ? Attendre qu'elle se charge intégralement~? Ou retourner des résultats partiels ? Ce faisant, les travaux portants sur l'extraction d'information depuis une page Web sont souvent pensés pour s'intégrer, d'abord et avant tout, à de vastes campagnes d'a\-nalyses \citep{weninger_text_2008, adar_web_2009, oita_forest:_2015}. La validation humaine n'est pas la finalité de ces recherches. De notre côté, nous préférons nous rapprocher de méthodes plus qualitatives, conçues sciemment pour être utilisées, en premier lieu, par des êtres humains. 

L'extension Readability\footnote{\url{https://github.com/mozilla/readability}} du navigateur Web Firefox propose, par exemple, aux internautes d'expérimenter une forme de lecture \textit{zen} sur la toile. Le système cherche ainsi à identifier le contenu principal de chaque page (le corps d'un article), à l'extraire et à le présenter dépouillé des publicités, menus et autres suggestions qui pourraient nuire à la lecture. Bien que Readability se limite seulement à certains types de sites  (les sites de news notamment), une partie de son fonctionnement pourra être adaptée à notre propre moteur. 

\subsection{Implémentation technique}

\noindent Notre tâche consiste ici à fragmenter une page Web donnée. C'est à dire, à extraire de nos fichiers archivés, des sous ensembles cohérents de nœuds HTML. Pour ce faire nous prendrons chaque page une par une, nous la nettoierons, puis, nous définirons une mesure censée traduire la cohérence entre des nœuds HTML deux à deux, avant de les grouper en un ensemble de fragments Web distincts. Soit une page Web $p_1$ composée de $m$ nœuds HTML $\{n_1,...,n_m\}$, organisés en un arbre DOM $t$ et associés à des règles CSS.

La première étape consiste à nettoyer $p_1$ de tous les éléments qui ne nous semblent pas pertinents : publicités, menus de navigation, scripts~... Pour ce faire, nous utilisons certaines heuristiques issues de la méthode \textit{boilerplate}\footnote{Boilerplate fait en partie appel à une tâche d'apprentissage automatique, cherchant à définir un modèle de nettoyage pour chaque site. Or, nous ne connaissons pas, à priori, les évolutions structurelles et stylistiques subies par un site au cours de son histoire. La définition d'un set d'apprentissage nous semble compromise } \citep{kohlschutter_boilerplate_2010} auxquelles nous ajoutons nos propres observations ad hoc. Il s'agit essentiellement de supprimer certaines balises et certaines classes  

C'est notre premier compromis entre manuel et générique  

wesh \\

readability va avec fathom

Là on parle de rivelaine et de la fonction distance ...
L'algo 
les différents filtres
on dit qu'il y a plusieures implémentations

\subsection{Exemples et discussions}

là on donne qq exemples 
et on dit que en tant qu'ingé on avait cherché un truc qui fonctionnait tout le temps
mais c'est pas possible et un historien a besoin de vouloir resizer à volonter 
Là on parle de l'automatique vs le fait à la main avec le truc firefox
Du coup dans la suite, le treshold de la fonction distance sera toujours testées avec le truc firefox

\section{Penser une exploration désagrégée}

\subsection{Atténuer les "crawl blindness"}

\subsection{Cohérence relative entre archives}

\subsection{Dédupliquer les corpus}

\section{Intégration à un moteur d'exploration}
\label{sec:retour_au_moteur}

\subsection{D'un schéma à un autre}

\subsection{Retour à la détection d'événements}

\subsection{S'éloigner des moteurs d'exploration}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Chapitre 6 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Explorations de Collectifs Migrants Éteints}
\label{chap:6}

Où l'on parle d'exploration de blogs, de forum et de moments

%%%%%%%%
\section{À la recherche de l'étonnement : l'analyse exploratoire de données}

\subsection{De Tuckey à Fry}

Où l'on explique l'EDA de où ça vient 

\subsection{Abduction, déduction, induction}

Où l'on introduit la philosophie générale de l'EDA et on peut faire un lien avec Ginsburg

\subsection{Méthodologie technique d'exploration}

Où l'on explique comment techniquement nous allons procéder en suivant plutôt Fry

%%%%%%%%
\section{Les traces d'une mutation numérique}

\subsection{D'une communauté vibrante de blogs ...}

Là on raconte l'état des blogs en 2008

En revanche, une blogosphère dont les acteurs sont parfois à l'origine même de sa construction et de sa promotion\footnote{De 2007 à 2011 la blogosphère marocaine a organisé ses propres \textit{blog awards} pour récompenser, promouvoir et connecter ses acteurs. Voir \url{https://fr.wikipedia.org/wiki/Maroc_Web_Awards}} n'est pas strictement équivalente à une sphère Web. \citep{keren_blogosphere:_2006}

\subsection{... à un collectif éteint}

Là on raconte l'état des blogs en 2018

\subsection{Définir l'espace d'exploration}

Là on explique la forme des fragments que l'on va chercher à retrouver

\subsection{Migration d'un territoire Web à un autre}

Là comprend que les blogs se sont déplacé vers Fb et Twitter

\subsection{Conserver son identité numérique}

Là on parle de la communauté des blogs

\subsection{Le Printemps Arabe vu comme un moment-clé}

Là on introduit le Printemps arabe marocain 

%%%%%%%%
\section{Un soulèvement en ligne éphémère}

\subsection{Yabiladi.com : porte d'entrée sur la diaspora}

Là on explique ce qu'est Yabiladi

\subsection{La manifestation du 20 Février 2011}

Là on rappelle ce qu'est cet événement

\subsection{Définir l'espace d'exploration}

Là on explique la forme des fragments que l'on va chercher à étudier

\subsection{Voir un site évoluer}

Là on explique comment on va visualiser ces fragments

\subsection{Agréger les contributeurs}

Là on s'intéresse au graph des contributeurs

\subsection{De l'embrasement à l'évasion}

Là on regarde les clusters de Threads

\section{Les Moments Pivot du Web}

\subsection{Les limites de l'archivage du Web}

Les archives ne capturent pas le Web comme un environnement

\subsection{Les moments pivots du Web}

Un moment pivot c'est quoi ? Les geste et compagnie ainsi que la micro-histoire

\subsection{Temporalités d'analyse}

Là on se dit que l'exploration désagrégée c'est quand meme pas mal et que l'on peut étudier les archives autour de moments singuliers

\subsection{Repenser nos archives vis à vis des moments pivots}

Là on commence à parler de la suite, du web que l'on souhaite, de la neutralité et des défis à venir de l'archivage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Chapitre 7 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Au Delà Des Archives Web}
\label{chap:7}

\section{Remettre l'humain au cœur des archives}

\section{Fouiller les archives du Web profond}

voir el bouquin de mazanès p63

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Conclusion %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Conclusion}


%%
% The back matter contains appendices, bibliographies, indices, glossaries, etc.







\backmatter

\bibliography{biblio}
\bibliographystyle{apalike}


\printindex

\end{document}

