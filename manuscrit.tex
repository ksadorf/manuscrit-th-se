\documentclass[symmetric,justified,marginals=raggedouter]{tufte-book}

%\hypersetup{colorlinks}% uncomment this line if you prefer colored hyperlinks (e.g., for onscreen viewing)

%%
% If they're installed, use Bergamo and Chantilly from www.fontsite.com.
% They're clones of Bembo and Gill Sans, respectively.
%\IfFileExists{bergamo.sty}{\usepackage[osf]{bergamo}}{}% Bembo
%\IfFileExists{chantill.sty}{\usepackage{chantill}}{}% Gill Sans

%\usepackage{microtype}

%%
% For nicely typeset tabular material
\usepackage{booktabs}

%%
% For graphics / images
\usepackage{graphicx}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
\graphicspath{{graphics/}}

% The fancyvrb package lets us customize the formatting of verbatim
% environments.  We use a slightly smaller font.
\usepackage{fancyvrb}
\fvset{fontsize=\normalsize}

%%
% Prints argument within hanging parentheses (i.e., parentheses that take
% up no horizontal space).  Useful in tabular environments.
\newcommand{\hangp}[1]{\makebox[0pt][r]{(}#1\makebox[0pt][l]{)}}

%%
% Prints an asterisk that takes up no horizontal space.
% Useful in tabular environments.
\newcommand{\hangstar}{\makebox[0pt][l]{*}}

%%
% Prints a trailing space in a smart way.
\usepackage{xspace}

%%
% Some shortcuts for Tufte's book titles.  The lowercase commands will
% produce the initials of the book title in italics.  The all-caps commands
% will print out the full title of the book in italics.
\newcommand{\vdqi}{\textit{VDQI}\xspace}
\newcommand{\ei}{\textit{EI}\xspace}
\newcommand{\ve}{\textit{VE}\xspace}
\newcommand{\be}{\textit{BE}\xspace}
\newcommand{\VDQI}{\textit{The Visual Display of Quantitative Information}\xspace}
\newcommand{\EI}{\textit{Envisioning Information}\xspace}
\newcommand{\VE}{\textit{Visual Explanations}\xspace}
\newcommand{\BE}{\textit{Beautiful Evidence}\xspace}

\newcommand{\TL}{Tufte-\LaTeX\xspace}

% Prints the month name (e.g., January) and the year (e.g., 2008)
\newcommand{\monthyear}{%
  \ifcase\month\or January\or February\or March\or April\or May\or June\or
  July\or August\or September\or October\or November\or
  December\fi\space\number\year
}


% Prints an epigraph and speaker in sans serif, all-caps type.
\newcommand{\openepigraph}[2]{%
  %\sffamily\fontsize{14}{16}\selectfont
  \begin{fullwidth}
  \sffamily\large
  \begin{doublespace}
  \noindent\allcaps{#1}\\% epigraph
  \noindent\allcaps{#2}% author
  \end{doublespace}
  \end{fullwidth}
}

% Inserts a blank page
\newcommand{\blankpage}{\newpage\hbox{}\thispagestyle{empty}\newpage}

\usepackage{units}

% Typesets the font size, leading, and measure in the form of 10/12x26 pc.
\newcommand{\measure}[3]{#1/#2$\times$\unit[#3]{pc}}

% Macros for typesetting the documentation
\newcommand{\hlred}[1]{\textcolor{Maroon}{#1}}% prints in red
\newcommand{\hangleft}[1]{\makebox[0pt][r]{#1}}
\newcommand{\hairsp}{\hspace{1pt}}% hair space
\newcommand{\hquad}{\hskip0.5em\relax}% half quad space
\newcommand{\TODO}{\textcolor{red}{\bf TODO!}\xspace}
\newcommand{\ie}{\textit{i.\hairsp{}e.}\xspace}
\newcommand{\eg}{\textit{e.\hairsp{}g.}\xspace}
\newcommand{\na}{\quad--}% used in tables for N/A cells
\providecommand{\XeLaTeX}{X\lower.5ex\hbox{\kern-0.15em\reflectbox{E}}\kern-0.1em\LaTeX}
\newcommand{\tXeLaTeX}{\XeLaTeX\index{XeLaTeX@\protect\XeLaTeX}}
% \index{\texttt{\textbackslash xyz}@\hangleft{\texttt{\textbackslash}}\texttt{xyz}}
\newcommand{\tuftebs}{\symbol{'134}}% a backslash in tt type in OT1/T1
\newcommand{\doccmdnoindex}[2][]{\texttt{\tuftebs#2}}% command name -- adds backslash automatically (and doesn't add cmd to the index)
\newcommand{\doccmddef}[2][]{%
  \hlred{\texttt{\tuftebs#2}}\label{cmd:#2}%
  \ifthenelse{\isempty{#1}}%
    {% add the command to the index
      \index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2}}% command name
    }%
    {% add the command and package to the index
      \index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2} (\texttt{#1} package)}% command name
      \index{#1 package@\texttt{#1} package}\index{packages!#1@\texttt{#1}}% package name
    }%
}% command name -- adds backslash automatically
\newcommand{\doccmd}[2][]{%
  \texttt{\tuftebs#2}%
  \ifthenelse{\isempty{#1}}%
    {% add the command to the index
      \index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2}}% command name
    }%
    {% add the command and package to the index
      \index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2} (\texttt{#1} package)}% command name
      \index{#1 package@\texttt{#1} package}\index{packages!#1@\texttt{#1}}% package name
    }%
}% command name -- adds backslash automatically
\newcommand{\docopt}[1]{\ensuremath{\langle}\textrm{\textit{#1}}\ensuremath{\rangle}}% optional command argument
\newcommand{\docarg}[1]{\textrm{\textit{#1}}}% (required) command argument
\newenvironment{docspec}{\begin{quotation}\ttfamily\parskip0pt\parindent0pt\ignorespaces}{\end{quotation}}% command specification environment
\newcommand{\docenv}[1]{\texttt{#1}\index{#1 environment@\texttt{#1} environment}\index{environments!#1@\texttt{#1}}}% environment name
\newcommand{\docenvdef}[1]{\hlred{\texttt{#1}}\label{env:#1}\index{#1 environment@\texttt{#1} environment}\index{environments!#1@\texttt{#1}}}% environment name
\newcommand{\docpkg}[1]{\texttt{#1}\index{#1 package@\texttt{#1} package}\index{packages!#1@\texttt{#1}}}% package name
\newcommand{\doccls}[1]{\texttt{#1}}% document class name
\newcommand{\docclsopt}[1]{\texttt{#1}\index{#1 class option@\texttt{#1} class option}\index{class options!#1@\texttt{#1}}}% document class option name
\newcommand{\docclsoptdef}[1]{\hlred{\texttt{#1}}\label{clsopt:#1}\index{#1 class option@\texttt{#1} class option}\index{class options!#1@\texttt{#1}}}% document class option name defined
\newcommand{\docmsg}[2]{\bigskip\begin{fullwidth}\noindent\ttfamily#1\end{fullwidth}\medskip\par\noindent#2}
\newcommand{\docfilehook}[2]{\texttt{#1}\index{file hooks!#2}\index{#1@\texttt{#1}}}
\newcommand{\doccounter}[1]{\texttt{#1}\index{#1 counter@\texttt{#1} counter}}

% Generates the index
\usepackage{makeidx}
\makeindex


\usepackage{indentfirst}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Customization %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\setcounter{tocdepth}{1}
\setcounter{secnumdepth}{1}

\renewcommand\contentsname{\normalfont \huge Table des matières}

\titlecontents{chapter}%
    [0em]% distance from left margin
    {\vspace{1\baselineskip}\begin{fullwidth}Chapitre }% above (global formatting of entry)
    {\contentslabel{0em} \hspace{1em} \huge $\vert$ \Large}% before w/ label (label = ``Chapter 1'')
    {\hspace{1em}}% before w/o label
    {\hfill\qquad\thecontentspage}% filler and page (leaders and page num)
    [\end{fullwidth}]% after
\titlecontents{section}% FIXME
    [0em] % distance from left margin
    {\vspace{0\baselineskip}\begin{fullwidth} \rmfamily\itshape} % above (global formatting of entry)
    {\hspace*{6em}\contentslabel{2em}} % before w/label (label = ``2.6'')
    {\hspace*{7em}} % before w/o label
    {\normalfont\hfill\qquad\thecontentspage} % filler + page (leaders and page num)
    [\end{fullwidth}] % after

\usepackage{enumitem}
\setlist{leftmargin=20mm}

\usepackage{tikz}
\usetikzlibrary{calc}

\newcommand\tikzmark[1]{%
  \tikz[overlay,remember picture] \coordinate (#1);}
  
 \renewcommand\labelitemi{--}

\usepackage{multirow}

\makeatletter
    \newcommand{\vast}{\bBigg@{3}}
    \newcommand{\Vast}{\bBigg@{3.5}}
    \newcommand{\vastt}{\bBigg@{4}}
    \newcommand{\Vastt}{\bBigg@{11}}
    %%
    %% Size from smallest to largest:
    %%\[ ( \big( \Big( \bigg( \Bigg( \vast( \Vast( \vastt( \Vastt(\]
\makeatother

\usepackage{dpfloat}

\usepackage[]{algorithm2e}

\begin{document}

% Front matter
\frontmatter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Titre %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\author{Quentin Lobbé}
\title{\nohyphenation{Archives et Fragments Web}}
\cleardoublepage
{  
  \begin{fullwidth}%
  \thispagestyle{empty} 
  \setlength{\parskip}{\baselineskip}
  \begingroup
  \vspace*{10em}
  \par\noindent\Large{Quentin Lobbé}
  \vspace*{-1em}
  \par\noindent\Huge\textbf{Archives et Fragments Web}
  \par\noindent\nohyphenation\Large{Pour une exploration désagrégée des traces numériques des migrations, préservées au sein de corpus d'archives Web}
  \endgroup
  \vfill  
  \par\noindent\nohyphenation Université Paris-Saclay, École doctorale des sciences et technologies de l'information et de la communication.  Thèse pour l'obtention du doctorat de Télécom ParisTech et de l'Université Paris-Saclay.    
  \end{fullwidth}%
}

\blankpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Info Thèse %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\newpage
\begin{fullwidth}
~\vfill
\thispagestyle{empty}
\setlength{\parskip}{\baselineskip}

\par\noindent Thèse présentée par \textbf{\thanklessauthor}\\
LTCI, Télécom ParisTech, Université Paris Saclay \& Inria. Paris, France.\\
quentin.lobbe@telecom-paristech.fr

\par\noindent Sous la direction de :\\
\textbf{Pierre Senellart}, professeur à l'École Normale Supérieure\\
\textbf{Dana Diminescu}, professeure à Télécom ParisTech

\par\noindent Soutenue publiquement à Paris le 9 novembre 2018, devant un jury composé de :\\
\textbf{Bruno Bachimont} (Rapporteur), enseignant-chercheur à l'Université Technologique de Compiègne\\
\textbf{Marc Spaniol} (Rapporteur), professeur à l'Université de Caen Basse-Normandie\\
\textbf{Anat Ben-David}, professeure à l'Open University of Israel\\
\textbf{Valérie Schafer}, professeure à l'Université du Luxembourg\\
\textbf{Bruno Defude}, directeur adjoint de la recherche et des formations doctorales à Télécom SudParis


\end{fullwidth}
  
\thispagestyle{empty}%
\clearpage%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Remerciements %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

~\vfill
\noindent
\par\noindent Il me demanda de chercher la première page.\\
\noindent Je posais ma main gauche sur la couverture et ouvris le volume de mon pouce serré contre l'index. Je m'efforçais en vain : il restait toujours des feuilles entre la couverture et mon pouce. Elles semblaient sourdre du livre.\\
- Maintenant cherchez la dernière.\\
\noindent Mes tentatives échouèrent de même; à peine pus-je balbutier d'une voix qui n'était plus ma voix :\\
- Cela n'est pas possible.\\
\noindent Toujours à voix basse le vendeur me dit : \\
- Cela n'est pas possible et pourtant cela \textit{est}. Le nombre de pages de ce livre est exactement infini. Aucune n'est la première, aucune n'est la dernière.
\\~\\
\noindent\textit{Jorge Luis Borges - Le livre de sable} 
\vfill
\indent
\newpage
\begingroup
\vspace*{8em}
\huge $\vert$ \huge Remerciements
\vspace*{4em}
\par\normalsize Là il faudra remercier du monde ...
\endgroup
\vfill

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Tables %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\tableofcontents

\listoffigures

\listoftables

\mainmatter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Chapitre 1 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction générale}
\label{chap:1}

\noindent Ici l'introduction de la thèse.

\section{Mise en garde}
\label{sec:1_mise_en_garde}

\subsection{Penser le passé depuis le présent}

\noindent Ici on fait un rapide détour par l'historiographie et les difficultés à parler du passé depuis le présent.

\subsection{Conservation différentielle et nature des archives Web}

\noindent Ici on parle de la raréfaction de la matière Web à mesure que l'on remonte le temps et également à mesure que le web fournit du contenu.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Chapitre 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cleardoublepage
\begin{minipage}[t,leftmargin=5em]{1.5\linewidth}%
\begin{adjustwidth}{-0.5cm}{}
\chapter{Les Représentations en Ligne des Diasporas} 
\label{chap:2}
\end{adjustwidth}
\end{minipage}
\hfill

\section{Aux origines du Web}
\label{sec:2_web}

\noindent Ici on parle des origines du Web, de sa genèse et des premières communautés en ligne.

\section{Le migrant connecté}
\label{sec:2_migrant_connecte}

\noindent Ici on parle du migrant connecté et de la manière dont les TIC ont fait évoluer l'étude des migrations

\section{Un espace de communication et d'organisation}
\label{sec:2_espace}

\noindent Ici on revient sur les premiers temps des communautés diasporiques sur le Web. Et on présente également la manière dont tout cela se passe aujourd'hui (smartphone, applications, ...)

\section{L'Atlas e-Diasporas}
\label{sec:2_atlas}

\noindent Ici on présente l'Atlas e-Diasporas (en tant qu'Atlas, pas en tant qu'archives)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Chapitre 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Archiver le Web}
\label{chap:3}

\noindent Face à la disparition totale ou partielle des sites Web recensés par l'atlas e-Diasporas (Section \ref{sec:2_atlas}), il a rapidement été décidé de mettre à l'abri cet héritage numérique. De le préserver. Dès Mars 2010, se met en place, sous la responsabilité technique des équipes de l'Institut Nationale de l'Audiovisuelle (INA), une vaste campagne d'archivage des quelques 9000 sites migrants alors cartographiés. Ces archives Web, aujourd'hui constituées\footnote{L'archivage s'est officiellement terminé en Septembre 2014}, peuvent désormais être l'objet de recherches et d'explorations. Grâce à ce travaille d'archivage, nous pouvons aujourd'hui questionner les traces du Web passé. 

Mais de part la nature même du médium, le Web appelle la mise en place d'un archivage particulier, basé sur des techniques de collectage dédiées. Comment archiver ce qui est, tout autant, un flot continue d'informations qu'un territoire en perpétuelle expansion ? Quels compromis ont du faire les archivistes pour garantir une collecte représentative ? Jusqu'à quel point les archives du Web passé, sont-elle fidèles au Web vivant ? 

Dans ce chapitre, nous prendrons le point de vue des archivistes. Nous évoquerons la genèse de l'archivage du Web qui, au tournant des années 2000, a connu un essor mondial, mobilisant nombre d'acteurs et d'institutions. Ces diverses initiatives capturent et stockent, jour après jour, plusieurs centaines de milliers de pages Web. Internet Archive, par exemple, a depuis son lancement en 1996, réalisée près de 650,000,000,000 captures d'objets Web (pages, images, vidéos, etc), cela pour un total de 40 PetaBytes d'archives. Mais la réussite écrasante d'Internet Archive ne cache-elle pas un mouvement en perte de vitesse ? Qu'en est-il dans le reste du monde ? Où en sont les travaux de recherche portant directement sur les archives Web ?

Pour comprendre la manière avec laquelle ces archives sont constituées, nous introduirons ensuite, d'un point de vue technique, les principales méthodes de sélection, collecte et stockage des corpus à préserver. Nous présenterons, enfin, les contours des archives e-Diaspo\-ras à proprement parler. Leurs particularités et leurs caractéristiques. Leur durée et leur étendue.\\

\noindent Bien que cette thèse se concentre sur l'exploration d'archives Web déjà existantes, il nous semble important d'évoquer la façon dont ces dernières sont constituées en amont afin de mieux saisir les biais analytiques (Chapitre \ref{chap:4}) qui motiveront la présentation de notre principale contribution (Chapitre \ref{chap:5}). Ce faisant, les éléments que nous nous apprêtons à présenter s'appuient principalement sur la lecture de l'ouvrage de J.~Masanes : "\textit{Web Archiving}" \citep{masanes_web_2006} qui reste, encore aujourd'hui, une référence. Nous compléterons et mettrons à jour, au besoin, ces informations.

\section{Vingt ans d'archivage du Web}
\label{sec:3_20ans}

\noindent En Octobre 2016, se tenait à la Bibliothèque Nationale de France (BNF) une grande conférence anniversaire réunissant, pour les 20 ans de l'archivage du Web\footnote{\url{http://www.bnf.fr/fr/professionnels/anx_journees_pro_2016/a.jp_161122_23_archivage_web.html}}, les acteurs français de la pratique. Alors qu'était évoqués les conditions du partage du dépôt légal du Web national entre la BNF et l'INA, il a été rappelé qu'à l'origine chacune des deux institutions souhaitait se voir attribuer la pleine gestion de ce dépôt. L'INA mettait en avant ses compétences techniques acquises en archivant les flux audiovisuels nouvellement introduits dans le paysage culturel. La BNF, pour sa part, s'appuyait sur son expérience pluricentenaire de préservation du patrimoine\footnote{\url{http://multimedia.bnf.fr/video/prof/161123_10_dl_web.mp4}}. 

Cette querelle initiale et son dénouement (la cotutelle du dépôt légale) sont à l'image de l'histoire même de l'archivage du Web : la conjugaison d'une tradition longue de sauvegarde des savoirs et d'un ensemble de techniques de collecte nouvellement pensées pour cet objet complexe qu'est le Web, le tout porté par une poignée de pionniers.

\subsection{Préserver la mémoire collective}

\noindent L'archivage du Web s'inscrit dans la tradition longue des techniques d'élaboration et de conservation de la mémoire collective. Tradition qui remonte aux origines même de l'humanité où technique et mémoire se trouvaient étroitement liées. 

A. Leroi-Gourhan fait émerger, de l'étude de séries d'objets (silex taillés, percuteurs, harpons, etc) et de figures préhistoriques (gravures et peintures des grottes ornées), une ligne de rencontre entre technique et mémoire \citep{leroi-gourhan_geste_1964}. Le préhistorien décrit la technique comme un système évolutif, soumis aux lois générales de la technologie et apparaissant comme transversal à des cultures parfois diverses et éloignées\footnote{
Leroi-Gourhan associe les formes animales des grottes ornées à des signes, réalisant des couplages basés sur l'observation (comptages et statistiques) de dizaines de cavités. Il cherche à établir une échelle évolutive des styles pariétaux, transversale aux premiers âges de l'Europe de l'Ouest \citep{leroi-gourham_art_1984}}. La technique est chargée, en elle même, de l'histoire passée des continuités, ruptures et transformations technologiques dont elle est l'aboutissement à un instant t.   

Avec Leroi-Gourhan, la technique devient mémoire. Elle peut en être chargée et/ou être conçue à dessein de la conserver. Involontairement, le silex taillé porte en lui la trace de l'homme qui l'a élaboré. Lorsque le tailleur finit par mourir, son geste continue à s'extérioriser à travers l'outil qui demeure. Précieux indice pour celui qui vient à sa suite ou pour l'archéologue qui, des millénaires après, saura grâce à cet objet assembler les traces fragmentées d'une pratique passée. Mais l'homme aurait aussi très bien pu choisir, en conscience, d'inscrire son expérience individuelle sur des supports de mémoire dédiés. L'écriture est ainsi l'une des premières techniques de la mémoire, utilisée par l'humanité depuis le néolithique. L'écriture est en cela une \textit{mnémotechnologie} \citep{stiegler_leroi-gourhan:_1998}. 

Poursuivant son évolution, l'humanité développe plus avant les techniques de transmission des savoirs pour sélectionner et agréger ses expériences individuelles en une mémoire collective. Des espaces et des structures voient le jour, appuyés par divers pouvoirs politiques ou religieux, avec le double objectif de préserver et d'administrer l'héritage collectif. J. Derrida décrit ainsi le geste d'archiver comme un "\textit{geste de pouvoir}" \citep[p.60]{derrida_trace_2014}. Choisir ce que l'on garde ou non dans les archives ne peut être que le fruit d'une hégémonie, d'une hiérarchie et "\textit{d'un certain nombre d'opérations de pouvoir}" rendues légitimes par une institution. L'État est ainsi caractérisé par \textit{sa capacité d'accumuler, contrôler et exploiter la mémoire collective} \citep{stiegler_etat_1991}, capacité dont on retrouve divers incarnations au court de l'histoire :

\begin{itemize}[leftmargin=*]  
\item Au IVe millénaire av. J.C, les tablettes d'argiles étaient accumulées par les mésopotamiens pour constituer les premières bibliothèques
\item Entre 535 et 555, Cassiodore pense le Monastère de Vivarium comme un lieu de transmission où, pour la première fois, seraient associés culture savante et christianisme
\item François Ier crée le dépôt légal\footnote{"\textit{Nous avons délibéré de faire retirer, mettre et assembler en notre librairie toutes les livres dignes d'être vues qui ont été ou qui seront faites, compilées, amplifiées, corrigées et amendées de notre tems}", extrait de l'ordonnance royale \citep{dougnac_depot_1960}} en France, par l'ordonnance royale du 28 décembre 1537, à des fins de préservation culturelle mais également de contrôle politique
\end{itemize}

\noindent Les siècle passent et les archives s'adaptent à la transformation des supports de mémoire et à l'émergence de formes nouvelles d'enre\-gistrement. Avec l'arrivée des technologies analogiques\footnote{Cinématographie, photographie, radiodiffusion, etc}, il faut désormais capter et archiver des flux d'images et de sons, ce qui conduira en France à la création de l'Institut National de l'Audiovisuel (INA) en 1974. L'apparition du numérique\footnote{Bases de données, logiciels, interfaces, etc} marque la dernière étape de ce cheminement en ouvrant la voie à un renouveau des formes de lecture et d'étude des archives. L'accès à distance de documents numérisés facilite leur consultation, mais il devient également possible de les qualifier, de les annoter ou de les mettre en relation, et ce, de manière large voire exhaustive \citep{borgman_digital_2000} :

\begin{itemize}[leftmargin=*]  
\item En 1971, le projet Gutenberg commence à collecter des copies numéri\-ques (recopiées et tapées \textit{à la main}) d'ouvrages du domaine public
\item Le Thesaurus Linguae Graecae cherche, depuis 1972, à numériser la plupart des textes littéraires rédigés en grecs ancien et toujours subsistants
\end{itemize}

\noindent Mais si le numérique permet aujourd'hui de revisiter des ressources anciennement archivées, il est aussi créateur d'objets nativement numé\-riques tout autant porteurs d'un héritage à préserver. Le web en est la parfaite illustration.

\subsection{Un héritage numérique}

\noindent Nous appelons \textbf{initiative d'archivage} tout projet d'archivage mené ou piloté par une seule, un collectif, une institution, etc. L'archivage du Web débute à la fin des années 90, et plus précisément en 1996 lorsque se développent les premières initiatives de préservation du Web, soit 4 années à peine après la publication de la première page sur la toile~(Section \ref{sec:2_web}). La National Library of Australia est ainsi à l'initiative du projet Pandora\footnote{\url{http://pandora.nla.gov.au/}} qui vise à archiver les publications en ligne australiennes sur la base d'une collecte sélective et continue de sites Web australiens. La Swedish Royal Library, quant à elle, lance le projet Kulturarw3\footnote{\url{https://web.archive.org/web/20040206225053/https://www.kb.se/kw3}} qui s'essaye à une collecte "\textit{intégrale}" et espacée dans le temps des sites du Web suédois \citep{arvidson_kulturarw3_2000}.

Mais c'est avec la création d'Internet Archive par B. Kahle la même année \citep{kahle_preserving_1997}, que s'écrit véritablement la première page de l'histoire des archives du Web. Ingénieur et activiste, Kahle s'inspire de la Bibliothèque d'Alexandrie pour motiver la création d'une organisation à but non lucratif afin de rendre accessible au plus grand nombre le passé du Web\footnote{\url{https://archive.org/}}. Utilisant un crawler développé pour le compte de son autre société Alexa Internet, Kahle revendiquait, dans les premières années de la collecte, être capable d'archiver au moins une fois tous les deux mois chacun des sites de l'ensemble du Web \citep{mohr_introduction_2004}. La revente d'Alexa au groupe Amazon en 1999 va lui permettre de pérenniser financièrement Internet Archive, qui depuis ce temps n'a eu de cesse d'archiver le Web.

Ces pionniers de l'archivage sont rapidement suivis par la Finlande en 1997, le Danemark en 1998 et d'autres pays nordiques rassemblés autour du projet NWA\footnote{Nordic Web Archive} \citep{hallgrinsson_nordic_2003}. En 2003, la publication par l'UNESCO de la \textit{Charte sur la conservation du patrimoine numérique} \citep{unesco_charter_2003} marque un nouveau tournant pour l'archivage du Web en reconnaissant la valeur universelle d'une telle démarche et l'urgence face à la disparition potentiel de tout, ou d'une partie, de l'héritage numérique mondial : \\

\begin{fullwidth}
"\textit{Le  patrimoine  numérique  mondial  risque  d'être  perdu  pour  la  postérité.  Les  facteurs  qui  peuvent  contribuer  à  sa  perte  sont  l'obsolescence  rapide  du  matériel  et  des  logiciels  qui  servent  à  le  créer,  les  incertitudes  concernant  les  financements,  la  responsabilité  et  les  méthodes  de  la  maintenance  et  de  la  conservation et l'absence de législation favorable à sa préservation. L'évolution des attitudes n'a pas suivi celle des technologies. L'évolution numérique a été trop rapide et trop coûteuse pour que les pouvoirs publics et les institutions élaborent en temps voulu et en connaissance de cause des stratégies de conservation. La menace qui plane sur le potentiel économique, social, intellectuel et culturel du patrimoine, pierre angulaire de l'avenir, n'a pas été pleinement saisie.}" --- Charte sur la conservation du patrimoine numérique, Article 3, \citep{unesco_charter_2003}\\
\end{fullwidth}

\noindent Pour de nombreuses bibliothèques nationales, la charte de l'UNESCO fait l'effet d'un accélérateur (Figure \ref{fig:date-initiative}). Les institutions sont encouragées dès 2003 à archiver leur Web national \citep{gomes_survey_2011}. Mais notons ici que la notion de Web national reste discutable \citep{abiteboul_first_2002}, il s'agira souvent de crawler le Web en fonction d'une extension de nom de domaine donnée (.fr, .jp, .uk, etc), extension qui ne couvre pas exhaustivement l'ensemble des sites associés à un domaine national précis, elle est plutôt à considérer comme une borne inférieure de celui-ci \citep{koehler_analysis_1999}. Le cas des corpus de l'Atlas e-Diasporas en est un très bon contre-exemple, nombre de sites migrants possédant une extension générique (.com, .net) ou correspondant au pays d'accueil plutôt qu'au pays d'origine \citep{leclerc_cyberespace_2012}.  

\begin{figure}%
  \includegraphics[width=\linewidth]{graphics/date-initiative}
  \caption{Évolution cumulée du nombre d'initiatives d'archivage du Web par année de création (source de données \citep{gomes_survey_2011} et Wikipédia \url{https://en.wikipedia.org/wiki/List_of_Web_archiving_initiatives})}
  \label{fig:date-initiative}
\end{figure} 

\noindent Ces nouveaux acteurs de l'archivage du Web peuvent être classés suivant la terminologie introduite par J. Masanes \citep[p.76]{masanes_web_2006}, entre initiatives publiques ou privées, poursuivant un but lucratif ou non. L'accès aux corpus archivés peut être entièrement public ou restreint et limité, en ligne ou physique (machine de consultation dans une bibliothèque). Par exemple, Internet Archive est une initiative à but non lucratif, avec un accès public à l'ensemble de ses corpus, en ligne depuis 2001 et physique depuis 2002\footnote{La Wayback Machine est officiellement lancée en 2001. Avant cette date les corpus d'Internet Archives n'étaient pas accessibles au public. En 2002, une copie intégrale des archives est consultable à la Bibliotheca Alexandrina, en Égypte}. 

Une autre manière de catégoriser ces initiatives est de regarder la nature des corpus archivés. Nous avons déjà évoqué les corpus territoriaux censés capturer les contours d'un Web national. Cette notion peut être également transposée à plus fine échelle : celle d'une région ou d'une ville \citep{boudrez_archiving_2002}. Un corpus d'archive peut être conçu pour cibler une thématique donnée, souvent  centrée sur des événements politiques \citep{voerman_archiving_2002,schneider_building_2003} : élections, référendums, etc. Certaines initiatives s'affranchissent même de barrières géographiques devenues contraignantes en préservant des sites de domaines nationaux étrangers \citep{gomes_introducing_2009}.

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/map-initiative}
  \caption{Carte des initiatives d'archivage du Web par pays et années de création (source de données \citep{gomes_survey_2011} et Wikipédia \url{https://en.wikipedia.org/wiki/List_of_Web_archiving_initiatives})}
  \label{fig:map-initiative}
\end{figure*} 

\noindent Enfin, il est possible de voir les corpus d'archives Web par rapport à l'utilisation que l'on en fait. Certains sont ouvertement tournés vers la consultation publique (Internet Archives, à nouveau), d'autres sont constitués à des fins universitaires (on pense au corpus japonais WARP\footnote{\url{http://warp.da.ndl.go.jp/search/}} de la National Diet Library). La British Library, de son côté, fait de ses archives Web une utilisation détournée, voire cachée, en chargeant la version passée d'une page Web de son site si cette dernière n'est momentanément ou définitivement plus accessible à un visiteur\footnote{\url{https://www.bl.uk/collection-guides/uk-web-archive}}. L'un des plus gros corpus d'archives Web reste en revanche celui détenu par Google qui permet d'accéder depuis le cache\footnote{\url{https://fr.wikipedia.org/wiki/Mémoire_cache}} de son moteur de recherche à une version précédemment crawlée d'une page. Notons, pour terminer ce tour d'horizon, que même si elles sont nombreuses de part le monde, les initiatives d'archivage du Web sont historiquement et géographiquement le fait d'états occidentaux (Figure \ref{fig:map-initiative}). Les continents sud Américain et Africain (hormis la la Bibliotheca Alexandrina) sont absents de ce paysage, rendant encore plus précieux les corpus transnationaux archivés par l'Internet Archives et l'Atlas e-Diasporas.  

En vingt années d'existence, les archives du Web ont agrégé autour d'elles une communauté de chercheurs et d'ingénieurs participant à sa promotion. L'International Internet Preservation Consortium (IIPC) est fondé en 2003 dans l'idée de proposer des rapports et des suivis réguliers de l'archivage\footnote{\url{http://internetmemory.org/images/uploads/Web\_Archiving\_Survey.pdf}} et des workshops sont organisés (l'IWAW, International Web Archiving Workshops). Mais le fait est de constater que la dynamique visible à la fin des années 2000 est en train de se tasser. En 2017, une seule et unique initiative a vu le jour (en Belgique autour du projet Promise\footnote{\url{https://promise.hypotheses.org/}}). Les vastes projets de recherche et d'exploitation des archives Web que sont ARCOMEM \citep{risse_arcomem_2014}, LAWA \citep{spaniol_tracking_2012} et LIWA \citep{denev_sharc:_2009} n'ont pas trouvé de successeurs et, en 2018, l'Internet Memory Foundation a annoncé stopper ses activités d'archivage. Enfin, rappelons que la position centrale d'Internet Archive dans cet écosystème ne la met pas à l'abri d'une possible disparition. En 2016, B. Kahle prévoyait (notamment en réaction à l'élection de D. Trump à la tête des États Unis) de déplacer une nouvelle copie intégrale d'Internet Archive au Canada\footnote{\url{http://blog.archive.org/2016/11/29/help-us-keep-the-archive-free-accessible} \url{-and-private/}}. Et n'oublions pas que, jusqu'à présent, la survie d'Internet Archive est et reste étroitement liée à son fondateur, la question de sa succession et de la pérennité du corpus après sa mort devra être rapidement abordée.  

Mais alors que les institutions semblent s'en détourner, le futur des archives Web viendra peut être de ceux que M. Graham, directeur de la WayBack Machine, nomme les "\textit{rogue archivists}"\footnote{\url{https://youtu.be/33_fnPwaEM0}}. S'inscrivant dans les pas d'A. Schwartz\footnote{En 2011, Schwartz hacke la base de données de l'éditeur JSTOR afin de "\textit{libérer}" plusieurs millions d'articles scientifiques payants, dont une part importante appartenait au domaine public (voir \textit{The Internet Own Boy} réalisé par B. Knappenberger en 2014 \url{https://archive.org/details/TheInternetsOwnBoyEsp}). Schwartz est à l'origine de contributions considérables, à la fois sur des aspects techniques du Web (développeur du format de flux RSS), mais également sur la question plus politique de l'accès universel aux connaissances.}, les rogues archivits sont des activistes et libristes militants s'appropriant politiquement la question des archives. Soit qu'ils considèrent le Web et son contenu comme un commun de l'Humanité \citep{coriat_retour_2015}, soit qu'ils voient dans les archives un moyen de faire perdurer la mémoire de minorités opprimées \citep{de_kosnik_rogue_2016}. Ils administrent de manière autonome certains des 7000 crawlers qui alimentent quotidiennement Internet Archive. Encore mineure, au regard des volumes globaux d'archives Web, il est néanmoins possible de déceler la trace de leurs contributions dans les travaux d'A. Ben David \citep{ben-david_internet_2018} qui révèle que les archives du Web Nord Coréen (présentes dans le corpus d'Internet Archive) n'ont pas été collectées par des crawlers institutionnels mais par des crawlers indépendants, non assujettis à la géopolitique des proxys.       

\subsection{Le cas des archives Françaises}

\noindent En France, l'archivage du Web est l'aboutissement singulier de dix années d'expérimentations techniques et de construction d'un cadre législatif inédit. Aujourd'hui, deux institutions se partagent le périmètre du dépôt légal du Web : la Bibliothèque Nationale de France (BNF) et l'Institut National de l'Audiovisuel (INA). 

Crée par François 1er, le \textbf{dépôt légal} est l'obligation pour tout éditeur (imprimeur, producteur, importateur, etc) de déposer chaque document dont il a la charge (en France) à la BNF ou auprès de l'organisme le plus adapté à la nature particulière de ce document. Tout ce qui se publie et s'édite en France est donc directement collecté par la BNF. L'INA, quant à elle, administre les archives radio et télé. Elle fut initialement créée pour en faire une exploitation commerciale et destinée aux professionnels de l'audiovisuel. L'État français est l'un des premiers états au monde a avoir posé la question des conditions de la mémoire culturelle et patrimoniale du Web. Le Web devait rentrer dans le périmètre du dépôt légal et c'est ainsi que furent posée les bases d'un futur \textbf{dépôt légal du Web}.

Les tractations commencent officiellement en 2001. En s'appuyant sur la directive européenne 2001/29/EC\footnote{\url{https://en.wikipedia.org/wiki/Copyright\_Directive}}, dite \textit{Information Society Directive}, l'Assemblée Nationale ouvre au débat la discussion du \textit{Projet de loi sur la société de l'information}\footnote{\url{http://www.assemblee-nationale.fr/11/projets/pl3143.asp}}. Cette loi vise à adapter le droit français aux NTIC en matière de libertés de communication, de commerce en ligne, mais également de droit d'auteur. De ces débats découle, en 2006, l'adoption de la loi DADVSI\footnote{\url{https://www.legifrance.gouv.fr/affichTexte.do?cidTexte=JORFTEXT000000266350}} relative \textit{au droit d'auteur et aux droits voisins dans la société de l'information} qui définit le cadre légale des archives Web à venir :\\

\begin{fullwidth}

"\textit{Les logiciels et les bases de données sont soumis à l’obligation de dépôt légal dès lors qu’ils sont mis à disposition d’un public par la diffusion d’un support matériel, quelle que soit la nature de ce support. Sont également soumis au dépôt légal les signes, signaux, écrits, images, sons ou messages de toute nature faisant l’objet d’une communication au public par voie électronique.}" --- Loi DADVSI, Article 21\\

\end{fullwidth}

\noindent La BNF et l'INA souhaitant toutes deux se voir confier le plein contrôle du dépôt légal de Web par l'État, les équipes de J. Masanès (BNF) et de T. Drugeon (INA) se lancent l'une comme l'autre dans la course à l'archivage dès tournant des années 2000, c'est à dire bien en amont de tout arbitrage politique. Comme nous le verrons dans la section suivante (Section \ref{sec:3_constituer}), la masse de travail à mettre en place pour débuter une collecte est considérable, l'histoire de l'archive du Web en France est donc tout autant l'aboutissement d'une volonté politique que le fruit d'années de recherches et développements. D'un point de vue purement technique, les deux institutions suivent des directions divergentes : la BNF et J.Masanès s'associent à la définition du format d'archivage WARC, l'INA et T. Drugeon créent le format DAFF et développent un crawler indépendant\footnote{Nous discuterons dans la section \ref{sec:3_constituer} des aspects techniques des divers formats d'archivage}. L'INA lance sa collecte de sites Web de manière expérimentale en 2009 alors que l'État s'oriente vers un partage du dépôt légale : une solution à deux têtes. Le cadre de cette partition est défini par le décret du 19 Décembre 2011\footnote{\url{https://www.legifrance.gouv.fr/affichTexte.do?cidTexte=JORFTEXT000025002022&categorieLien=id}} qui établit que :

\begin{itemize}[leftmargin=*]  
\item La BNF archivera l'ensemble du domaine national français et d'outre-mer au moins une fois par an. Par là, sont identifiés tous les sites Web en .fr ainsi qu'une liste blanche de sites en .com, .org, .net, etc "\textit{édités par des personnes physiques ou morales domiciliées en France}".
\item L'INA archivera un sous ensemble thématique du Web français centré sur les sites dit \textit{médias} (sites des services des médias audiovisuels, Web TV et Web radios, programmes radio et télé, professionnel de l'audiovisuel, etc). La fréquence de collecte sera variable et adaptée à la nature même des mises à jour de ces sites (chaque jour, semaine, mois, etc)
\end{itemize}

\noindent Si le périmètre de la collecte de la BNF reste classique au regard de ses consœurs mondiales, le corpus archivé par l'INA, lui, est tout à fait singulier. La collecte se concentre sur un jeu de 14.000 sites Web médias (sélectionnés manuellement). Seulement $30\%$ d'entre eux ont une extension .fr contre $50\%$ en .com \citep{drugeon_technical_2005}. L'INA intègre à son corpus des sources vidéos (de youtube et dailymotion dès 2010), des flux RSS, des Tweets (depuis 2014), etc. Les deux institutions offrent également la possibilité à des chercheurs de constituer des corpus tiers et portés sur une thématique précise : l'ANR Web90 est montée en partenariat avec la BNF\footnote{\url{https://web90.hypotheses.org/}} autour des premières années du Web français \citep{schafer_web_2016}, l'INA réalise une collecte dédiée aux attentats de Paris fin 2015\footnote{\url{https://asap.hypotheses.org/173\#more-173}}. L'INA est enfin la seule des deux institutions à avoir encore aujourd'hui une équipe technique dédiée à la recherche et à l'exploitation de ses archives Web.

Les corpus de la BNF et de l'INA se veulent donc complémentaires. Ils appartiennent à la catégorie des initiatives publiques mais n'offrant qu'un accès physique aux contenus archivés : il n'y a pas de portail en ligne de consultation des archives. Le chercheur doit se déplacer dans l'un des 31 centres locaux de l'INA ou, s'il est a Paris, il reste possible d'accéder aux deux corpus depuis la BNF.

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/ticket-bnf}
  \vspace*{0.2cm}  
  \caption{Demande d'accréditation pour accéder aux zones de consultations des archives Web à la BNF (site François Mitterand)}
  \label{fig:ticket-bnf}
\end{marginfigure} 

\begin{center}
	$\sim$
\end{center}

\noindent Je reprend ici, le \textit{je} pour parler d'une expérience personnelle. Courant 2017, je me suis rendu à la BNF (site François Mitterrand) afin d'y consulter les archives de l'INA et de la BNF et tester les modalités d'accès aux corpus. Ce récit ne vaut pas généralité, mais doit être pris comme un témoignage d'une exploration d'une heure trente dans la bibliothèque avant de trouver les archives. Je pensais tout d'abord (en me fiant aux indications du site Web) qu'il était possible de consulter les archives depuis le réseaux Wifi du lieu. 

Après divers échecs, les bibliothécaires m'ont progressivement fait passer d'interlocuteur en interlocuteur jusqu'à finalement me faire accéder (moyennant une demande d'accréditation, Figure \ref{fig:ticket-bnf}) à l'une des salles du Rez-de-Jardin de la BNF (Figure \ref{fig:map-bnf}). Là les archives de l'INA ne sont consultables que depuis une poignée de postes labellisés \textit{Inathèque}. Les archives de la BNF, elles, sont accessibles depuis l'ensemble des  machines de la zone. Une fois connecté, un moteur de recherche classique nous permet de faire des recherches par URL (pour la BNF) et plein texte (pour l'INA), il n'est en revanche pas possible de sauvegarder ses recherches ou de les exporter d'une quelconque manière. Je me suis donc servi de mon téléphone pour photographier les pages Web qui m'intéressaient. Mais, je ne cherche pas ici à pointer du doigt ou accuser. 

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/bnf}
  \caption{Localisation (en bleu) des postes de consultation des archives Web à la BNF (Rez-de-Jardin, site François Mitterand)}
  \label{fig:map-bnf}
\end{marginfigure} 

Au contraire j'ai été étonnamment surpris de voir que la consultation des archives Web à Paris relève d'une véritable expédition et je remercie les bibliothécaires d'avoir finalement su me guider. Mais s'il ne faut pas faire grief de leur méconnaissance, force est de constater que nous ne devons pas être très nombreux à consulter les archives Web là bas et que celles ci ne sont pas particulièrement mise en avant après du public et du personnel.

\begin{center}
	$\sim$
\end{center}
  
\noindent Les conditions d'accès restreintes aux corpus de l'INA et de la BNF ne jouent pas en la faveur d'une démocratisation de leur exploitation. Auprès du grand public d'une part, mais également vis à vis de potentiels chercheurs voulant questionner les archives. S'il reste tout à fait possible de venir étudier une liste prédéfinie d'URL et de sites (en lecture seule), les modalités d'accès n'encouragent pas à l'exploration des corpus ni à la possibilité de mener des recherches larges et/ou automatisées (il n'y a pas d'API\footnote{Point d'entrée à une source de données depuis l'extérieure de son environnement de stockage} par exemple). 

\section{Sélectionner, collecter et fouiller des corpus}
\label{sec:3_constituer}

\noindent D'un point de vue purement technique, il a fallut penser de toute pièce de nouveaux processus d'archivage. Alors que la préservation du Web a rapidement été considérée comme une nécessité, des équipes de pionniers ont rassemblé des connaissances éparses et les ont conjuguées pour créer les outils de sélection, collecte et fouille des futurs corpus d'archives Web. 

\subsection{Un objet éphémère et multiforme}  
 
\noindent À l'époque où le Web ne contenait qu'une poignée de pages et de sites, la question de son auto-préservation\footnote{traduit ici de l'anglais "\textit{self preserving}" \citep{spinellis_decay_2003}} fut posée. Le Web s'archivait-il déjà de lui même ? Pourquoi faire intervenir un archiviste ?

Lorsqu'un nouveau contenu est publié, il est possible de reléguer en bas de page les élément plus anciens, à la manière d'une pile. Les \textit{content management system} (CMS\footnote{Système permettant de gérer automatiquement et de manière dynamique le contenu d'un site Web, tels due : Wordpress, Drupal, Joomla!, etc}), apparus avec l'essor des blogs, avaient ainsi pour vocation de stabiliser ce processus de création et suppression de contenus en ligne, en reléguant les publications passées dans une section dédiée. Rien n'empêche également un site Web d'être copié dans son intégralité puis redéployé sur de nouveaux serveurs pour le préserver. Le numérique facilite et rend possible la reproduction à l'infini de ses objets. Ainsi, une page Web aurait théoriquement pu ne jamais disparaitre de la toile. Mais il fut montré, dès le début des années 2000, que malgré ces possibilités, le Web restait un milieu hautement instable. La disparition de contenu est un phénomène inhérent au Web. 

La durée de vie d'un site Web, peut être calculée par rapport à la mesure de sa \textit{half life}, soit la durée qu'il faut pour que la moitié de son contenu (ici ramené au nombre de pages\footnote{La \textit{half life} peut être appliquée à d'autres objets numériques comme des bases de données ou des documents scannés. Cette mesure est elle même dérivée des techniques de datation des atomes.}) disparaisse du Web \citep{koehler_longitudinal_2004}. Dès 1999, la \textit{half life} moyenne d'un site Web est ramenée à 50 jours \citep{cho_evolution_1999}, estimation qui doit être relativisée par rapport au contexte de publication du site et à la nature de ses pages \citep{mcdonnell_cataloging_1999,fetterly_large-scale_2003}. De même, 80\% des pages Web collectées entre 2003 et 2004, par les archives du Web japonais ont été effacée en moins d'un an du Web vivant \citep{toyoda_whats_2006}. Pour préserver le Web il faut donc intervenir et archiver avant qu'une page ne soit détruite. Ainsi, avant toute collecte, l'archiviste doit prendre en compte les changements susceptibles d'intervenir sur une page ciblée, afin de minimiser la perte d'information.

Les changements subis par une page Web au cours de son existence sont multiples \citep{douglis_at&t_1998, adar_web_2009}, allant de la modification de son contenu jusqu'à une évolution de la structure des liens qui la relie au reste du site. La fréquence de changement d'une page peut être estimée et prédite en s'appuyant sur des versions précédemment archivées \citep{chawathe_meaningful_1997,khoury_efficient_2007}. Il est possible d'affiner l'estimation de cette fréquence en catégorisant les changements par types (structurels, sémantiques ou cosmétiques) \citep{yadav_change_2007}. Plutôt que de considérer chaque page indépendamment les unes des autres, les changements peuvent être détectés à l'échelle d'un site ou d'un réseau. On s'appuyera alors sur la présence de liens hypertextes \citep{liu_webcq-detecting_2000} ou sur des relations hierarchiques plus marquées \citep{lim_automated_2001}. Dans la suite de ce manuscrit, nous nous limiterons à considérer comme changements les seuls actes de création, de modification ou de suppression d'une partie ou de l'ensemble d'une page Web.

Il faut finalement attendre qu'il soit archivé, pour pouvoir considérer le Web comme un support d'informations \textit{self preserving}. Ce n'est qu'une fois les corpus d'archives rendus accessibles depuis le Web lui même \citep{brugger_website_2009}, que l'on peut considérer qu'il garde en lui la trace (mesurée et mesurable) de ses états passés.\\

\noindent Par ailleurs et à la différence d'autres types de documents à archiver, une page Web possède ce que J. Masanès \citep[p.47]{masanes_web_2006} nomme une \textbf{double cardinalité}. 

\begin{figure}
  \includegraphics[width=\linewidth]{graphics/web-ressource}
  \caption{La double cardinalité d'une ressource Web, d'après \citep{masanes_web_2006}}
  \label{fig:web-ressource}
\end{figure} 

\noindent La cardinalité est le nombre d'instances en circulation d'un artéfact donné : un musée conservera des pièces uniques et originales, une librairie mettra à disposition de ces visiteurs des copies. La cardinalité donne toute sa valeur à un objet archivé et influence les techniques de préservation. Jusqu'à l'invention de l'imprimerie, pour archiver un livre il fallait le copier à la main. L'original était conservé dans un lieu donné et les copies envoyées vers d'autres bibliothèques \citep{canfora_vanished_1990}. L'original se perdant parfois, la copie (rectifiée ou annotée) devenait à défaut œuvre de référence. Mais la notion d'original disparait avec l'imprimerie. Le livre dans sa forme est stabilisée, les bibliothèques possédant toute la même version d'un ouvrage devenu reproductible à l'identique \citep{febvre_apparition_2013}. Ainsi  Au contraire du livre, sites et pages Web ont la singularité de présenter une double cardinalité : 

\begin{enumerate}[leftmargin=*]  
\item les fichiers sources, hébergés sur un serveur donné 
\item l'infinité d'accès possibles à cette source
\end{enumerate}

\noindent D'une machine à l'autre ou d'un écran à l'autre, une page Web sera toujours vue différemment \citep{bon_apres_2014}. Soit que la taille de l'écran (ordinateur, smartphone, tablette, etc) aura modifié son aspect, soit que la qualité de la connexion à Internet n'aura pas permis de tout charger, ou encore que l'historique de navigation aura influencé l'affichage de la page à nos yeux. 

C'est pour cela que J. Masanès propose de parler de \textbf{ressource Web} \citep[p.48]{masanes_web_2006} pour nommer tout objet Web susceptible d'être archivé. Une ressource Web est un document unique dont la source peut être identifiée précisément mais interprétée d'une infinité de ma\-nière possibles. Depuis son navigateur, derrière son écran. Pour l'archiviste se pose alors la question de quoi archiver ? L'originale ou toutes les interprétations d'une même page ? 

Arrivé à ce point, archiver le Web revient donc à prendre en compte l'ensemble des états successifs d'une ressource Web afin de ne rien rater. Une fois la fréquence d'archivage décidée, la collecte peut être opérée du point de vue de la source ou du point de vue de l'internaute naviguant derrière son écran. Où l'archiviste choisira-il de se positionner, lui, et ses outils de collecte~?

\subsection{Sélection}

\noindent Toute collecte sur le Web débute par le choix d'un point d'entrée clairement identifié. L'archiviste ne peut se permettre de dériver au hasard du Web pour trouver les sites qui l'intéressent. La \textbf{sélection} désigne donc l'ensemble des techniques mises en place pour définir ce ou ces points d'entrée. S'agit-il d'un site Web précis ? D'une liste de pages Web ? D'un masque ou d'un pattern d'URL à satisfaire ? À quelle profondeur débuter l'archivage ? Doit-on commencer par collecter la page principale d'un site, la \textit{front page}\footnote{Page principale d'un site Web, on peut également parler de page d'accueil}, ou un sous ensemble de pages contenant un mot clé donné ?     

Le principal critère de sélection définissant le périmètre d'un corpus à archiver reste l'extension d'URL. Les .fr, .uk et autres .ma définissent le cadre grossier d'un domaine national sur le Web (Section \ref{sec:3_20ans}). La sélection s'opère alors en validant un masque d'URL ou une heuristique prédéfinie\footnote{Dans un script cherchant à sélectionner les sites du domaine français, on ne conservera que les noms de domaine dont l'extension valide l'expression régulière suivante : $*.fr\$$}. Il est également possible de dessiner les contours d'une archive en partant d'une liste initiale de sites Web, appelés sites sources et liés à une thématique précise. Il faut pour cela faire appelle, en amont de toute collecte, à des experts (sociologues, historiens, etc). L'INA, par exemple, a procédé par expertise pour identifier les 14.000 sites média de son périmètre d'archivage. 

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/degree}
  \caption{Graphe dont les nœuds sont labellisés par degré. En théorie des graphes, le degré $deg(v)$ d'un nœud $v$ correspond au nombre de liens incidents (entrant ou sortant) à ce nœud.}
  \label{fig:degree}
\end{marginfigure} 

Lors d'une collecte plus large, les archivistes peuvent s'appuyer sur des indices traduisant la valeur d'un site ou d'une page visitée. Le degré d'un site Web (Figure \ref{fig:degree}) permet ainsi d'estimer son autorité au sein d'un environnement hypertexte \citep{abiteboul_first_2002}. Une autre stratégie consiste à identifier les sites sources par rapport aux habitudes de navigation des internautes. Quels sites sont fréquemment visités ? Quelles pages en particulier ? Il s'agit alors d'exploiter les requêtes adressées à un moteur de recherche en ligne \citep{pandey_user-centric_2005}. Ou encore, de se baser sur les \textit{access patterns} \citep{alnoamany_access_2013} afin de privilégier, au sein d'un même site, l'archivage de séquences de pages fréquemment visitées. Le système de sélection d'Internet Archives, en particulier, utilise cette dernière méthode \citep{kimpton_year-by-year:_2006}. Dans la même veine, un historique de navigation personnel pourra faire office de liste de primo-candidats à archiver \citep{dumais_stuff_2016}. Cette fonctionnalité a récemment été ajoutée aux archives du Web danois\footnote{Une version bêta du système danois est disponible ici : \url{https://github.com/netarchivesuite/solrwayback}}. Mais précisons, qu'aucune stratégie de sélection ne prévaut sur une autre. Elles sont d'ailleurs souvent combinées pour former une chaine complexe en amont de tout collectage (Section \ref{sec:3_edias}). 

Par ailleurs, il est possible d'opérer une sélection par \textit{crowd sourcing} en faisant appel à des archivistes tiers. C'est toute l'idée du service payant \textit{Archive-it}\footnote{\url{http://www.archive-it.org}}, lancé en 2006 par Internet Archive, permettant à tout un chacun de se constituer des corpus d'archives Web. Quelques 230 millions d'URLs ont ainsi été recueillies entre 2006 et 2007 avant d'être reversées dans le fond d'archives principales de la Wayback Machine. Dans l'idée de démocratiser encore d'avantage leur exploitation, les archives portugaises offrent la possibilité à chaque internautes de suggérer une liste de pages ou de sites Web (pas nécessairement appartenant au domaine portugais .pt) à ajouter aux  collectages\footnote{\url{http://sobre.arquivo.pt/en/collaborate/suggest/}}. L'utilisateur devient ainsi acteur de la préservation du Web.

Terminons en soulignant que le choix d'archiver une page plutôt que l'ensemble d'un site (et inversement) n'est pas trivial. À quelle échelle doit on archiver ? L'arbitrage est souvent décidé au cas par cas et peut faire l'objet de compromis. Même si l'on demande à Internet Archive de sauvegarder une page précise, le système remontera toujours à la front page du site afin d'en archiver la racine  \citep{kimpton_year-by-year:_2006}. Ce genre de mécanisme permet d'amender et d'enrichir les points d'entrées après chaque collecte. La découverte de nouveaux sites appelant à réévaluer sans cesse la liste d'origine.    

\subsection{Collecte}

\noindent Par \textbf{collecte} nous désignons l'ensemble des techniques visant à transformer une page du Web vivant en une page archivée. Comme nous l'indiquions précédemment le Web peut, sous certains aspects, être considéré comme \textit{self preserving}. Une fois archivés, l'ensemble des éléments collectés restent accessible depuis le Web. Le Web contient en lui même les traces de son passé. Lorsque l'on scanne une pellicule, image par image pour archiver un film, on fait subir à ce support de mémoire une transformation. De l'analogique au numérique. Dans le cas d'une ressource Web, la transformation induite par la collecte est minime. Il s'agit grossièrement de venir prélever les fichiers d'origines d'une page, sans les altérer et de les dater avant de les réintroduire dans les archives Web.     

Or, le protocole HTTP qui régit les règles de communication sur le Web, entre client (l'internaute) et serveur (la page), n'autorise qu'un accès unitaire aux ressources Web. Il n'est possible d'accéder au Web qu'une page à la fois. La page Web (identifiée par une URL unique) est en cela l'unité de consultation de base du Web. La collecte doit donc s'effectuer page après page et non par lot, telle que :   

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/collecte}
  \caption{Archivage du Web vivant page après page, de $p_1$ à $p_3$, entre les instants $t_1$ et $t_3$}
  \label{fig:collecte}
\end{figure*} 

\noindent Il existe ainsi trois grandes familles de techniques d'archivage du Web, qui témoignent du déplacement progressif des outils de collecte du serveur vers le client. 

La première, nommée \textit{serveur-side-archiving}, consiste à collecter les ressources directement depuis le serveur hébergeant un site ou une page Web. Cette technique est plutôt employée par les producteurs de données eux même, s'ils souhaitent archiver l'ensemble des ressources d'une de leur plateforme Web par exemple. Mais les moyens à mettre en place sont considérables car, à la différence d'une simple copie, le \textit{serveur-side-archiving} induit la réplication du site (les fichiers HTML, CSS, etc d'origine) et de l'entièreté de son environnement de dévelop\-pement et d'hébergement. On utilisera plutôt cette méthode pour archiver le Web dit profond\footnote{À opposer à la portion visible du Web (ce que l'on voit derrière son écran), le Web profond désigne tout élément qui n'est pas accessible directement depuis un crawler : les formulaires, bases de données, etc \citep{lawrence_accessibility_2000}. Une discussion sur l'exploration d'archives du Web profond sera menée au chapitre \ref{chap:7}}.

La seconde approche, dite \textit{transaction-archiving}, se situe à la frontière entre serveur et client. Il s'agit ici de positionner l'outil de collecte au niveau du système d'entrées/sorties (IO) du serveur hébergeant le site Web ciblé \citep{fitch_web_2003}. Ce que l'on archivera sera le couple [requête, réponse] du client au serveur, soit la demande d'un internaute cherchant à visiter une URL donnée et la page Web telle que retournée par le serveur. Cette forme d'archive dessine une vision non exhaustive d'un site Web mais néanmoins fidèle à la réalité du flot d'internautes qui le parcourent. En capturant la trace des pages effectivement visitées et la manière toujours unique dont celles-ci sont affichées à l'écran des utilisateurs, cette technique est la seule qui intègre directement l'humain et ses gestes dans les archives Web\footnote{Ce type d'archive fera l'objet d'une exploration dédiée au chapitre \ref{chap:7}, où nous interrogerons les logs de navigation Web de la Bibliothèque du Centre Pompidou}.  

La dernière famille, connue sous l'appellation de \textit{client-side-archiving}, est aussi la plus rependue. Ayant acté qu'une ressource Web pouvait être visualisée d'une infinité de manière possible par le client (l'internaute), l'archiviste choisit ici de placer son outil de collecte en lieu et place de l'utilisateur. L'outil devient client et cherche à reproduire les interactions d'un internaute pour accéder au contenu ciblé~: la page Web à archiver. Tout l'enjeu est donc de définir et de contrôler l'exhaustivité de ces interactions pour construire une copie fidèle d'une page ou d'un site. 

Comme la collecte doit être menée page par page, programmée à l'avance et conduite à échelle large, les archivistes du Web se sont inspirés des \textit{crawlers} développés pour les moteurs de recherche \citep{pant_crawling_2004} à la fin des années 1990. Un \textbf{crawler} est un robot programmé pour parcourir un site ou un ensemble de sites, une page à la fois, en capturant au passage l'ensemble de ses fichiers d'origine. Un crawler, pour bien fonctionner, doit respecter des règles de politesse~: éviter les dénis de services (DNS, Serveurs HTTP), les blacklistages officiels (robots.txt, sitemap.xml, etc.) et officieux (\textit{cloaking}, pièges à robot)\footnote{Ce manuscrit n'étant pas spécifiquement centré sur la question des crawlers, il est possible d'en apprendre d'avantage en se référant aux cours de C. Maussang (\url{https://frama.link/FrFrZ5EC}) ou en se tournant vers des ouvrages dédiés \citep{chakravarthy_webvigil:_2002,mitchell_web_2015}}. Dans le cadre spécifique des archives du Web, un crawler doit en plus intégrer les contraintes temporelles évoquées précédemment. Il a pour mission de capter l'ensemble des changements intervenants sur une page ou un site cible. Enfin, nous appelons \textbf{crawl} une campagne d'archivage menée par un crawler et (par abus de langage) le résultat même de cette campagne. 

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/crawl}
  \caption{Différentes stratégies adoptées par un crawler $c$ pour collecter les pages $\{p_1,...p_n\}$ d'un même site}
  \label{fig:crawl}
\end{figure*}

\noindent L'aspect d'un corpus d'archives Web est directement le fait du crawler qui a mené la collecte. Un crawl peut ainsi être conduit de plusieurs manières. Une première possibilité revient à entreprendre une collecte en profondeur d'abord (\textit{depth-first}, Figure \ref{fig:crawl} (a)). Le crawler capturera en priorité les pages filles de la page sur laquelle il se trouve. Un autre approche consiste à travailler en largeur d'abord (\textit{breadth-first}, Figure \ref{fig:crawl} (b)). Le crawler privilégiera cette fois les pages sœurs. Mais ces techniques sont lentes et il faudra prévoir un temps considérable pour parcourir l'entièreté d'un site, or l'archiviste cherchera au contraire à minimiser le temps de capture. Aussi, on peut envisager l'instauration d'une limite en profondeur pour ne pas archiver des pages trop éloignées de la racine du site (Figure \ref{fig:crawl} (c)).

En pratique, l'archiviste optera plutôt pour un compromis entre largeur et profondeur. Avec la démocratisation des moteurs de re\-cherches en ligne, l'internaute n'est plus obligé de passer par la front page d'un site pour en consulter le contenu. Les profils de navigation se diversifient rapidement \citep{holscher_web_2000}. Pour identifier les pages pertinentes, les crawler doivent donc intégrer à leurs programmations divers indicateurs topologiques ou sémantiques. Le pageRank \citep{page_pagerank_1999} ou le degré entrant d'un site (Figure \ref{fig:degree-in}) traduisent tous deux l'importance d'une page crawlée \citep{cho_efficient_1998}. Ces mesures peuvent être enrichie au regard de l'historique du crawl ou d'une possible hiérarchie entre pages \citep{baeza-yates_crawling_2005}. 

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/degree-in}
  \caption{Graphe dont les nœuds sont labellisés par degré entrant. En théorie des graphes, le degré $deg^-(v)$ d'un nœud $v$ correspond au nombre de liens incidents entrant à ce nœud.}
  \label{fig:degree-in}
\end{marginfigure} 

Nous le verrons, lorsque dans la chapitre \ref{chap:4} nous changerons de point de vue, passant de l'archiviste à l'explorateur d'archives, la cohérence est une notion fondamentale. Un crawl doit garantir une forme de cohérence topographique et temporelle vis à vis du corpus qu'il cherche à constituer. Sur ce point, il faudra poser la question de l'ordonnancement des sites les uns par rapport aux autres. Certains sites, larges ou volatiles, feront l'objet d'une collecte rapide (\textit{short-term scheduling}) qui mobilisera toutes les ressources du crawler.  Pour les autres, le crawl s'inscrira dans le temps long (\textit{long-term scheduling}) et pourra prendre plusieurs jours \citep{castillo_scheduling_2004}. Ne crawler que lorsque qu'un site est le moins susceptible de subir des changement peut aussi garantir une cohésion temporel au corpus \citep{saad_coherence-oriented_2011}. S'appuyant sur toutes ces réflexions, les équipes d'Internet Archive présentent en 2004 un crawler open-source, l'Heritrix \citep{mohr_introduction_2004} capable de s'adapter à divers type de collecte : large (\textit{broad crawling}), en continue (\textit{continuous crawling}) ou focalisée (\textit{focused crawling}). Heritrix reste encore aujourd'hui le crawler le plus répandu pour l'archivage du Web. 

Face à l'évolution du Web, les crawlers s'adaptent et archivent de nouveaux objets : allant des sites Flash\footnote{\url{https://fr.wikipedia.org/wiki/Adobe_Flash}} aux vidéos Youtube ou Dailymotion \citep{pop_archiving_2010}. Mais alors que les contenus publiés incorporent de plus en plus d'éléments dynamiques, se syndiquer à un flux RSS devient une stratégie à part entière pour collecter de l'information en continue \citep{oita_archiving_2010}. Des librairies sont développées pour interpréter les portions de code utilisant du Javascript\footnote{\url{https://github.com/ariya/phantomjs}} et les crawlers commencent à se spécialiser pour archiver certains réseaux sociaux. Les interfaces de programmation applicative (API) s'imposent comme des sources de données auxquelles il convient de se connecter. Si certaines APIs ouvertes permettent de crawler l'entièreté d'une plateforme\footnote{Voir la crawl de Github réalisé en 2010 par F. Cunny et Linkfluence (\url{http://www.visualcomplexity.com/vc/project.cfm?id=785})}, d'autres plus limitées obligent les archivistes à faire preuve d'inventivité. Ainsi, Internet Archive possède, depuis Mars 2016, un compte Facebook \textit{charlie.archivist} dont la timeline est régulièrement archivée\footnote{\url{https://web.archive.org/web/20170914234842/https://www.facebook.com/charlie.archivist}}. 

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/new_crawl_ina}
  \caption{Fonctionnement général du système de collecte de l'INA}
  \label{fig:crawl-ina}
\end{figure*}

\noindent Terminons ce descriptif des techniques de collecte, par une présentation plus poussée du crawler de l'INA. Développé par les équipes de T. Drugeon \citep{drugeon_technical_2005}, ce crawler fut en charge (de 2010 à 2014) de l'archivage des sites Web de l'Atlas e-Diasporas. Pour commencer (Figure \ref{fig:crawl-ina}), un ordonnanceur général (\textit{scheduler}) gère la liste des sites sources auxquels une fréquence de collecte a été associée. En se basant sur cette fréquence, l'ordonnanceur choisit les sites à archiver en priorité et leur dédie à chacun un crawler (\textit{site crawler}). Plusieurs centaines de crawlers peuvent être ainsi lancés en parallèle. Ces crawlers procèdent à une récolte en largeur d'abord, mais sans sortir du périmètre du site qui leur est alloué. Une fois les pages visitées, le contenu est indexé et stocké sur fichiers. À ce niveau, si des liens hypertextes sortants sont détectés dans une page, les sites pointés par ces derniers sont conservés afin de potentiellement venir enrichir la liste des sites sources. La fréquence de collecte est mise à jour entre deux crawls successifs. Soit en analysant les informations venues de l'agrégateur de flux RSS, soit en comparant l'évolution d'une page archivée d'une version à l'autre. Cette architecture, fait du crawler de l'INA un outil extrêmement réactif, adapté à la nature même des sites médias et, de fait, très efficace lorsqu'il s'agit de constituer rapidement des corpus portant sur un événement singulier\footnote{Voir la collecte réalisée pour les attentats de Paris en 2015 (\url{https://asap.hypotheses.org/173})}.

\subsection{Stockage}

\noindent Le \textbf{stockage} représente l'ensemble des techniques d'enregistrement d'une ressource Web crawlée. Ainsi, parallèlement à son crawler, l'INA développe son propre format de fichier destiné au stockage des archives Web : le Digital Archive File Format (DAFF). L'INA prend ainsi le contre pied du reste de la communauté qui, elle, continue de s'en tenir au format Web ARChive (WARC) pour sauvegarder la grande majorité des corpus existants.

C'est en 1996, s'inspirant du format de compression et d'archivage ARC (popularisé à la fin des années 80), qu'Internet Archive définie le ARC\_IA\footnote{\url{https://www.loc.gov/preservation/digital/formats/fdd/fdd000235.shtml}}. L'idée étant de combiner plusieurs ressources collectées en un seul et même fichier avant de les compresser pour en réduire la taille sur disque.

Mais l'ARC\_IA évolue rapidement, suivant les avancées des techniques de crawl, et ce, jusqu'à atteindre sa version actuelle : le WARC. Devenu le format standard d'archivage Web en 2009\footnote{\url{https://www.iso.org/standard/44717.html}}, un fichier d'ar\-chives WARC peut être vu comme la concaténation de plusieurs enregistrements (ou blocs), chaque enregistrement correspondant à une ressource Web crawlée\footnote{Une page, une image, ... Chez Internet Archive, toute objet associé à une URL unique sera archivé comme ressource Web}. Les informations contenues dans un bloc WARC sont de deux natures : des \textit{meta données} et des \textit{données}. Les méta données (stockées dans le \textit{header} du bloc) couvrent toutes les informations relatives au crawl : date de collecte, taille de la ressource, URL de la ressource, ID du bloc, ... Ces méta données sont directement suivies des données à proprement parler : soit l'enregistrement brut des fichiers .HTML, .CSS, etc collectés. Ainsi, chaque fois qu'une page Web est archivée (qu'elle ait évolué ou non depuis le précédent crawl) un bloc est ajouté au fichier WARC courant (Figure \ref{fig:daff-warc} (a)). 

Directement liés au WARC, il est possible d'extraire d'un bloc deux sous-formats spécialement dédiés à l'exploitation des corpus : les WAT et WET. Un fichier WAT (Web Archive Transformation) ne contient que des méta données. Contrairement au WET (Web Extracted Text) et à ses dérivés (LGA ou WANE\footnote{\url{https://webarchive.jira.com/wiki/spaces/ARS/pages/90997507/Datasets+Available}}) qui, eux, ne stockent que des éléments de texte issus de la partie données d'un bloc WARC. Ces fichiers WAT et WET répondent à l'une des principales critiques lancées à l'encontre du format WARC, pourtant hégémonique : le WARC introduit de la redondance dans les stocks d'archives Web.

En effet, entre deux crawls successifs, un bloc WARC sera invariablement crée (que la ressource Web collectée ait évolué ou non). Une page Web stable dans le temps, verra ainsi son contenu archivé autant de fois qu'elle aura été crawlée, conduisant à une consommation d'espace de stockage considérable. C'est donc en partant de l'intuition selon laquelle méta données et données devraient être stockées séparément (pour ne pas surcharger les corpus) que l'INA a développé le format DAFF.  

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/daff-warc}
  \caption{Différences entre les formats WARC (a) et DAFF (b)}
  \label{fig:daff-warc}
\end{figure*}

\noindent Une archive DAFF est en réalité l'association de deux fichiers complémentaires : un fichier de méta données et un fichier de données. Comme pour le WARC, chaque fichier est une suite de blocs correspondant à une ressource Web crawlée. Le fichier de méta données contient divers champs relatifs à la collecte (voir Table \ref{tab:daff}). Le fichier de données, quant à lui, ne renferme que deux champs : un identifiant unique et le contenu (HTML, CSS, etc) de la page Web archivée. Avec le DAFF, données et méta données sont stockées séparément. Ainsi, d'un crawl à l'autre, si la page Web visitée n'a pas évolué, alors son contenu ne sera pas re-téléchargé : seul un nouveau bloc de méta données sera ajouté pour témoigner du passage du crawler. Chaque bloc de données est donc associé à un ou plusieurs blocs de méta données (le champ \textit{content} des méta données correspondant au champ \textit{id} des données). Ce mécanisme permet, d'une part de ne pas dupliquer inutilement le contenu d'une page Web archivée et, d'autre part, de pouvoir pratiquer rapidement divers calculs statistiques sur le seul fichier de méta données. Celui étant pas nature plus léger qu'un WARC complet, donc moins long à traiter.

Notons que pour tester si une page a évolué depuis sa précédente visite, le crawler de l'INA compare la valeur des champs \textit{id} des blocs de données concernés. En effet l'\textit{id} est une clé SHA-256 résultant du hachage\footnote{En cryptographie, le hachage consiste à une donnée de taille arbitraire, une image (ou clé) de taille fixe et unique} du contenu même de la page Web archivée. On dira donc d'une page Web qu'elle s'est transformée si et seulement si les deux clés successives sont différentes. La nature de ce changement ne sera en revanche pas connue, celui-ci pouvant aller de la refonte entière de la page à la simple suppression d'une virgule.        

\begin{table*}
  \label{tab:daff}
  \begin{tabular}{lrl}
    \toprule
    Méta données& Champ & Description\\
    \midrule  
    \multirow{5}{*}{\emph{obligatoire} \vastt\{ }&id&identifiant unique du bloc\\
    &url&url associée à la ressource\\
    &date&date de téléchargement (Timesone GMT, ISO 8601)\\
    &content&identifiant unique du bloc de données associé\\     
    &status&statut de retour du crawler (ok, request\_error, server\_error, etc)\\
    \midrule     
    \multirow{12}{*}{\emph{facultatif   } \Vastt\{ }&crawl\_session&identifiant unique de la campagne de crawl\\
    &charset&encodage de la ressource\\
    &type&MIME Type (identifiant du format de donnée de la ressource)\\
    &corpus&nom du corpus d'archives associé\\
    &ip&adresse ip associée au crawl\\
    &level&profondeur du crawl\\
    &page&la ressource est elle une page Web (0|1)\\
    &client\_country&nationalité associée à la page\\
    &length&taille du bloc de données associé\\
	&active&la ressource était elle active au moment du crawl\\
	&client\_lang&langue associée à la ressource\\
	&referer\_url&url précédemment visitée par le crawler\\
    \midrule
    Données& Champ & Description\\
    \midrule 
    &id&clé SHA-256 unique\\
    &content&contenu (HTML, CSS, etc) de la ressource\\    	
    \bottomrule
\end{tabular}
  \bigskip
  \caption{Ensemble des champs disponibles dans les fichiers de méta données et de données DAFF}
\end{table*} 

\noindent Outre le stockage sous formats WARC et DAFF, J. Masanès \citep[p.64]{masanes_web_2006} rappelle qu'il existe des méthodes alternatives de sauvegarde des archives. Associées aux stratégies de collecte situées côté serveur (\textit{serveur-side-archiving}) on trouvera les formes dites de \textit{local file system served archives} qui consistent à transformer un site Web archivé en une copie locale de l'ensemble de ses ressources. Ainsi les URIs absolues, permettant (sur le Web) de naviguer d'une page à l'autre, seront transformées en URIs relatives à l'intérieur du fac-similé. Très couteuse, cette méthode nécessite de transformer en profondeur la nature des pages archivées et devient vite ingérable à mesure qu'augmente le nombre de collectes.

Enfin, il reste toujours possible de copier un site, page après page, sous format PDF ou image (capture d'écran ou vidéo). Bien que facile à mettre en place (techniquement parlant) cette stratégie ne passera pas non plus à l'échelle\footnote{En 2013, K. Goldsmith imprime littéralement plusieurs centaines de milliers de pages Web en soutient à A. Schwartz, remplissant l'équivalent d'une pièce de 1,100 $\mathrm{m}^2$} et aura pour conséquence d'arracher les sites et pages Web archivés à leur environnement hypertexte d'origine.  

Pour terminer, les corpus d'archives Web répartis dans le monde se comptent par centaines. Alors que le Web vivant continue son expansion, le volume du Web archivé ne cesse de croitre. En 2017, la BNF avait archivé 18,000 millions de pages Web (soit environ 370TB) tandis que l'INA plafonnait à 43,000 millions de pages pour un total avoisinant les 420TB. Et depuis sa création, l'Internet Archive a collecté à elle seule pas moins de 650,000 millions de ressources Web soit 40,000TB de données. Ainsi, face à ces corpus qui s'amassent et à la nécessité de les exploiter, les archivistes du Web ont du développer des outils dédiés à leur exploration.

\subsection{Fouille}

\noindent Par \textbf{fouille}, nous désignons les stratégies d'interrogation et de requêtage des archives Web. Ainsi, pour permettre aux chercheurs d'ana\-lyser le résultat des collectes, les archivistes déploient des dispositifs techniques \textit{au dessus} des corpus existants. 

Comme nous l'évoquions en section \ref{sec:3_20ans}, la fouille est tributaire des modalités d'accès aux données qui, pour $50\%$ des initiatives \citep{costa_survey_2013}, passent par la mise en place d'un portail en ligne. Mais cela ne signifie pas pour autant que les archives sont entièrement accessibles. Su ce point, $38\%$ des initiatives restreignent la consultation de leurs corpus : soit que l'analyse doit se faire localement (INA, BNF, etc), soit que les archives ne sont pas intégralement mises à disposition du public (The Library of Congress, Australia's Web Archive, etc). Contrairement à Internet Archive et aux Portuguese Web Archives qui proposent un plein accès, en ligne, à leurs collectages.  

Les dispositifs de fouille\footnote{Souvent désignés par \textit{search strategies} ou simplement \textit{search} dans la littérature}, déployés par dessus les archives, reprennent l'architecture générale de la plupart des système de moteurs de recherche \citep{grainger_solr_2014,hatcher_lucene_2004}. Les archives sont ainsi indexées puis mises à disposition d'un serveur de \textit{search} qui les rend interrogeables\footnote{Nous développerons le processus d'indexation des archives Web plus en détail dans la section \ref{sec:4_moteur}}. L'indexation définie l'étape de transformation d'un document texte en une liste de mots ou d'ensembles de mots, cette étape est nécessaire à toute construction d'un moteur de recherche. On appelle index la structure de données obtenue après indexation. 

Côté utilisateur, la recherche se traduit par une interface Web, dans laquelle il est possible de rentrer une requête (un texte, un ensemble de mots clé, des filtres, etc), puis de consulter les résultats correspondants sous la forme d'une liste ou d'un histogramme (Figure \ref{fig:ia-search}). Tout l'enjeu pour ces moteurs d'exploration d'archives est de proposer la meilleur technique de recherche possible pour fouiller efficacement un corpus d'archives Web. Ou comment, partant de la requête d'un chercheur, proposer avec justesse un ensemble de page archivées qui satisfasse ses interrogations ?

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/IA-search}
  \caption{Interface de search de la WayBack Machine (\url{https://web.archive.org/web/*/yabiladi})}
  \label{fig:ia-search}
\end{marginfigure} 

Dans ce domaine, la recherche dite plein texte (\textit{full-text}) est ce vers quoi tendent toutes les initiatives d'archive du Web \citep{costa_characterizing_2011,costa_evaluating_2012}. En recherche d'information, le full-text revient à faire correspondre les mots d'un document ou d'un ensemble de documents avec les critères fournis par un utilisateur (des mots clés, une phrase, etc). Popularisée sur le Web par AltaVista, c'est la recherche telle que nous l'expérimentons quotidiennement en interrogeant Google. Mais encore aujourd'hui, son application aux moteurs d'exploration d'archives Web reste limitée. Si l'INA, les portugais d'Arquivo.pt ou encore The Web Archive of Catalonia\footnote{\url{https://www.padicat.cat/en}} proposent d'étendre cette fonctionnalité à l'ensemble de leurs corpus \citep{stack_full_2006}, à la BNF, au contraire, le full-text n'est applicable que sur les seules URLs des pages archivées. C'est à dire qu'à une requête utilisateur donnée, le moteur de la BNF ne pourra faire correspondre qu'une recherche stricte par URL, sans regard pour le contenu même des pages. En 2016, Internet Archive ajoute à la Wayback Machine un système full-text basé sur le titre des pages Web collectées\footnote{Si et seulement si le titre est présent dans la balise \textit{head} du HTML de la page (\url{http://blog.archive.org/2016/10/24/beta-wayback-machine-now-with-site-search/}, \url{https://www.w3schools.com/html/html_head.asp})}, avant cette date seule une recherche stricte par URL était proposée. Selon M. Costa \citep{costa_survey_2013}, $67\%$ des initiatives d'archivage du Web proposaient en 2013 une recherche full-text complète, limitée ou dégradée. 

La taille importante des corpus ou la difficulté des archivistes à définir quelles doivent être les éléments d'une page Web à indexer, peuvent expliquer que le full-text soit si compliqué à mettre en place. Mais des alternatives existent : on peut ainsi envisager une recherche par catégories \citep{holzmann_tempas:_2016}, par entités nommées \citep{spaniol_tracking_2012} ou en se basant sur des tendances issues des réseaux sociaux \citep{risse_arcomem_2014}. Une solution originale consiste à ne pas construire de moteur d'exploration mais à s'adosser à des systèmes existants. Les créateurs du projet Memento\footnote{\url{http://mementoweb.org/about/}}, par exemple, intègrent les corpus d'Internet Archive directement à l'intérieur d'un navigateur Web. Ce faisant, la fonction de recherche à proprement parlé est assuré par le navigateur. 

Créée en 2001, la Wayback Machine est le plus emblématique des systèmes de fouille d'archives Web \citep{tofel_waybackfor_2007}. L'ensemble des re\-ssources archivées y sont indexées page par page (bloc WARC par bloc WARC). Les indexes sont ensuite répartis sur les quelques 2500 serveurs de stockage du data center principal d'Internet Archive. Un ordonnanceur central envoie les requêtes utilisateurs à l'ensemble des serveurs avant d'agréger les résultats. Il existe un système de cache destiné à améliorer les temps de réponse de la Wayback Machine, ainsi l'attente variera en fonction de la popularité de chaque requête. Le moteur des Portuguese Web Archives propose, lui, d'indexer les archives d'abord par date de téléchargement puis page par page \citep{costa_survey_2013}. Si l'utilisateur associe à sa requête une années ou un intervalle de temps précis, le résultat de ses recherches lui sera plus rapidement retourné. Les moteurs d'exploration peuvent aussi être entièrement décentralisés. A. Anand décrit Everlast \citep{anand_everlast:_2009} comme un système de fouille \textit{peer-to-peer} où chaque élément du réseau est à la fois serveur et client, plus scalable donc. 

Bénéficiant d'une distribution open source, la Wayback Machine est aujourd'hui réutilisée ou sert de base\footnote{Dans son intégralité ou élément par élément : Héritrix pour la partie crawl, NutchWAX pour la partie recherche full-text} à l'architecture de $62\%$ des initiatives d'archivage du Web \citep{costa_survey_2013}. Mais force est de constater que les systèmes existants n'encouragent pas particulièrement à l'exploration des archives, à la découverte des corpus, ou plus basiquement à leur exploitation à grande échelle (en terme de quantité de pages ou de temporalité). Il est de plus difficile de s'évader du cadre stricte imposé par des interfaces invariablement semblables, proposant une expérience des archives Web toujours identique identique.

La Wayback Machine est redoutablement efficace lorsqu'il s'agit de rechercher une version précise d'une page déjà connue. Notons aussi qu'elle propose une API\footnote{\url{https://archive.org/help/wayback_api.php}} pour accélérer les traitements. Mais sans la possibilité de filtrer à priori le contenu des pages, tout le dispositif de fouille sera à développer du côté de l'explorateur qui bien souvent n'a pas les compétences ou les moyens techniques pour y arriver. Et si l'INA offre de meilleures fonctionnalités de recherche (plein text complet, n-gram, etc ...), le fait que l'on ne puisse accéder aux corpus que depuis des lieux dédiés reste un frein majeur à toute analyse. Ainsi, une asymétrie se dessine rapidement lorsque l'on parcourt la littérature basée sur les archives Web. Beaucoup de travaux portent sur la constitution en amont de corpus d'archives (sélection, collecte, etc), très peu en revanche se lancent dans l'exploitation ou le questionnement de corpus existants. Ces derniers, bien que précieux à juste titre, se \textit{limitent}\footnote{Pas forcément en conscience, mais nous pensons que les outils d'exploration jouent un rôle quand il s'agit de définir de la portée de ces travaux} soit à l'analyse de versions passées de sites identifiés à priori \citep{schafer_web_2016,gebeil_les_2016}, soit à l'extraction d'éléments singuliers d'un contenu archivé : des images \citep{ben-david_internet_2018} ou des liens hypertextes \citep{weltevrede_where_2012}. 

\section{Les archives Web de l'Atlas e-Diasporas}
\label{sec:3_edias}

\noindent Parallèlement au travail de cartographie présenté en section \ref{sec:2_atlas}, les chercheurs pilotant la construction l'Atlas e-Diasporas prennent la décision d'archiver l'ensemble des sites Web déjà répertoriés. Tout autant pour les préserver des assauts du temps \citep{khouzaimi_e-diasporas_2015} que pour permettre la tenue de recherches futures : se donner la possibilité d'un retour arrière, analyser les évolutions et transformations subies par ces réseaux. Déjà associée à la collecte des sites, l'INA se voit confier la charge de l'archivage. Toutes les e-Diasporas seront concernées par cette campagne de sauvegarde, mais  nous nous attarderons ici sur la seule description de la section marocaine de l'Atlas. 

L'archivage du corpus marocain débute en Mars 2010 et se termine en Septembre 2014 après une collecte patiente et continue. Le collectage couvre l'ensemble des 156 sites de l'e-Diasporas marocaine. La fréquence de collecte associée à chaque site est définie en amont par les chercheurs. Celle ci est sera au final soit hebdomadaire (pour $56\%$ des sites), soit mensuelle (pour les $44\%$ restants). La majorité des sites archivés à la semaine sont les plus fréquements mis à jours : des blogs, des portails communautaires ou des médias. Les archives sont stockées suivant le format DAFF, vu comme l'union d'un fichier de méta données (\textit{metadata-r-00006.daff} : 13GB) et d'un fichier de données (\textit{data-r-00006.daff} : 151GB). Ces fichiers représentent un total de 17,043,833 ressources collectées, parmi lesquelles nous comptons 16,897,787 pages Web ($99\%$), 145,301 images, 700 vidéos et 44 enregistrements audio. Dans le chapitre \ref{chap:6} nous explorerons les sites \textit{yabiladi.com} et \textit{larbi.org} dont une présentation détaillée est donnée par la table \ref{tab:detail-archive}. Cette table introduit un premier élément de comparaison entre les archives e-Diasporas et leurs équivalents chez Internet Archive.

\begin{table}
  \label{tab:detail-archive}
  \begin{tabular}{lrr}
    \toprule
    &larbi.org&yabiladi.com\\
    \midrule
    Nombre d'archives (e-Diasporas)  & 78,311 & 2,683,928\\
    Nombre d'archives (Internet Archive) & 24,537 & 887,981\\
    \midrule
    Début de l'archivage  (e-Diasporas) & Mars 2010 & Mars 2010\\
    Début de l'archivage  (Internet Archive) & Oct. 2002 & Fev. 2001\\
    \midrule
    Fin de l'archivage  (e-Diasporas) & Sept. 2014 & Sept. 2014\\
    Fin de l'archivage  (Internet Archive) & Sept 2018 & Sept 2018\\    
  \bottomrule
\end{tabular}
  \bigskip
  \caption{Décompte des archives Web des sites \textit{yabiladi.com} et \textit{larbi.org}}
\end{table} 


\noindent Si la fréquence d'archivage (telle que mise en place par l'INA) semble plus élevée du côté d'e-Diasporas, la durée de collecte est importante chez Internet Archive. L'idée ici n'est pas de prouver qu'un corpus est mieux qu'un autre, mais de saisir les particularités de chacun. Un corpus d'archives Web n'est jamais parfait, bien au contraire, et nous émettons ici l'hypothèse que c'est par une approche mixte, en conjuguant diverses sources de données que nous maximiserons la précision scientifique des explorations à venir. Ainsi, sur la période 2010-2014 et dans les cas précis de \textit{yabiladi.com} et \textit{larbi.org}, la capture réalisée pour e-Diasporas semble plus fidèle. Il faudra en revanche l'associer à Internet Archive lorsque nous chercherons à remonter au delà de 2010. 

Mais essayons maintenant d'étendre cette comparaison à l'ensemble des sites du corpus marocain. Voyons comment ces sites ont été archivés par différentes initiatives. Comme beaucoup de ces observations devrons être faites à la main (notamment à la BNF), nous nous limitons tout d'abord aux seules front pages (pages racines) de chaque site Web de l'e-Diasporas marocaine. Pour chacune de ces pages, nous consultons successivement les archives e-Diasporas (produites par l'INA), les archives de la BNF et les archives d'Internet Archive, puis nous notons leurs dates de premier et dernier crawl afin de se donner une idée de l'étendue des collectes. Les résultats sont présentés et agrégés par les figures \ref{fig:date-crawl-ina} (pour l'INA), \ref{fig:date-crawl-bnf} (pour la BNF) et \ref{fig:date-crawl-ia} (pour Internet Archive). Le nom de domaine des sites est inscrit en ordonnée, le temps en abscisse. Chaque ligne est divisée en années puis en mois (1 mois = un tiret). Si un tiret est colorié c'est qu'il se trouve entre les dates de premier et de dernier crawl du site correspondant.

\iffalse

\begin{figure*}[hbtp]%
  \includegraphics[width=\linewidth]{graphics/date-crawl-ina}
  \caption{Préservation des sites de l'e-Diaspora marocaine par l'INA}
  \label{fig:date-crawl-ina}
\end{figure*}

\begin{figure*}[hbtp]%
  \includegraphics[width=\linewidth]{graphics/date-crawl-bnf}
  \caption{Préservation des sites de l'e-Diaspora marocaine par la BNF}
  \label{fig:date-crawl-bnf}
\end{figure*}

\begin{figure*}[hbtp]%
  \includegraphics[width=\linewidth]{graphics/date-crawl-ia}
  \caption{Préservation des sites de l'e-Diaspora marocaine par Internet Archive}
  \label{fig:date-crawl-ia}
\end{figure*}

\fi

L'intuition précédente est confirmée par ces trois figures : les corpus vu depuis Internet Archive et (dans une moindre mesure) depuis la BNF couvrent naturellement une plus grande étendue temporelle que la collecte de l'INA, limitée aux seules années 2010-2014. En revanche, leurs collectages sont incomplets, les sites marocains ne sont pas tous archivés. C'est assez naturel au regard du périmètre d'archivage de la BNF notamment, qui ne doit théoriquement couvrir que les sites du domaine français, ici la BNF aura archivé par effet de bord des sites marocains en .com ou .org ce qui nous amène à relativiser la notion de domaine Web national telle que présenté plus tôt (Section \ref{sec:3_20ans}). Ce qui frappe, enfin, est la cohérence générale de notre corpus tel qu'il est présenté par l'INA. L'ensemble des sites sont archivés, collectés au moins une fois entre 2010 et 2014 et forment un ensemble thématiquement homogène. 

Mais ces figures ne doivent pas non plus nous induire en erreur, nous ne voyons pas le détail des collectes : ni ce qu'il s'est passé entre les dates de premier crawl et de dernier crawl, ni le comportement du crawler vis à vis de pages éloignées de la racine des sites. Or, nous le découvrirons dans le chapitre suivant, archiver est avant tout une question de choix et de sélections, ce qui posera nombre de problèmes à l'explorateur d'archives Web. 

\begin{center}
	\textbf{***}
\end{center}

\noindent Les archives Web ont été construites pour inscrire la mémoire du Web sur un support durable et préserver notre héritage numérique. Offrir ainsi la possibilité aux chercheurs de demain d'interroger, de questionner et de critiquer le Web qui nous est contemporain. Mais plus on archive et plus la taille de ce Web passé grandit, laissant parfois les chercheurs seuls face à des corpus trop larges et trop vastes pour être explorer sans méthode et stratégie clairement définies. Les archives Web doivent rester une matière vivante. Prenons garde à ce qu'elles ne deviennent pas des capsules temporelles\footnote{\url{https://en.wikipedia.org/wiki/Westinghouse\_Time\_Capsules}} que l'on enterre dans l'espoir, qu'un jour, peut être, quelqu'un se décide à les rouvrir.

Il existe, selon nous, un espace et un intérêt scientifique indéniable pour des recherches portants sur de larges corpus d'archives Web. Mais cet espace appelle la création de méthodes d'analyse capables de passer, au besoin, d'une étude quantitative à grande échelle vers des travaux et des validations qualitatives, au cas par cas. Sur ce point, ne faisons l'hypothèse que le quantitatif ne pourra se concevoir sans le qualitatif (Chapitre \ref{chap:6}) et que l'automatisation des traitements ne pourra se faire sans une part de travail manuel (Chapitre \ref{chap:5}).

Au cours de ce chapitre, nous nous sommes attaché à décrire la genèse de l'archivage du Web comme technique de préservation d'un nouvel héritage numérique. L'idée étant de comprendre la nature des corpus que nous manipulerons dans la suite de manuscrit. Au tournant des années 2000, de nombreuses initiatives privées et publiques se sont emparées du sujet, déployant en un temps record (à l'échelle du Web) des moyens humains et techniques. Néanmoins cette dynamique semble aujourd'hui s'essouffler et force est de constater que, même si les corpus grandissent toujours plus, peu de chercheurs se sont déjà aventurés dans les archives. Le Web passé reste un terrain en partie inexploré. 

Pour ce saisir du Web, il aura fallu, aux pionniers de l'archivage, inventer et déployer de nouvelles méthodes de collecte et de stockage. Ces choix techniques façonnent et régissent les archives Web telles que nous les découvrons aujourd'hui. Détacher du Web vivant, l'archive Web se consultent dans des lieux sanctuarisés, souvent à la main et à travers des outils qui, malgré eux, réduisent les archives Web à de simples documents. La sensation du Web comme environnement n'est pas restituée dans les archives. L'exploration y est forcément ciblée, réduite à une URL ou un mot clé. 

Cependant, comme pour le Web vivant, l'unité d'exploration des archives reste la page Web. WARC et DAFF sont deux formats construits au dessus des pages qu'ils capturent et dotent, par là même, d'une nouvelle temporalité. Une fois sur fichier chaque version d'une même page se voit associée à une date de téléchargement. Cette date devient dès lors le seul marqueur temporel par lequel nous pouvons explorer les archives Web. Dans le chapitre suivant, nous exposerons les divers implications et biais d'analyse que peut causer cette datation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Chapitre 4 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cleardoublepage
\begin{minipage}[t,leftmargin=5em]{1.5\linewidth}%
\begin{adjustwidth}{-0.5cm}{}
\chapter{Traces Discrétisées et Temporalité Figée} 
\label{chap:4}
\end{adjustwidth}
\end{minipage}
\hfill

\noindent Au cours de ce chapitre, nous amorcerons un changement du point de vue, glissant du regard de l'archiviste vers celui de l'explorateur d'archives Web. 

Un explorateur d'archive est une personne ayant l'intention de découvrir ou d'étudier un corpus d'archives Web donné (fini ou toujours en construction). Son geste pourra tout autant être motivé par une question de recherche précise que par sa seule curiosité. Ce faisant, l'explorateur devra démêler les traces d'un Web passé pour faire émer\-ger une information ou un savoir de cette masse de données. 

Dans un premier temps, nous déconstruirons la structure des ar\-chives Web pour en saisir les règles et la grammaire interne. En effet, selon N. Brügger \citep{brugger_website_2009}, le Web archivé n'est déjà plus le Web, c'est un autre espace, un autre environnement. Lorsqu'un site du Web vivant est sélectionné, stocké et collecté, il subit une série de transformations qui forcent les archivistes à recréer une partie du système d'information du Web. Pour explorer les archives, il faut se détacher des automatismes acquis en parcourant le Web vivant. Sur ce point, nous présenterons ici certaines propriétés et certains biais inhérents au Web archivé qu'il faudra prendre en considération avant toute analyse. 

Pour l'explorateur, les archives Web se présentent d'abord comme des traces discrétisées du Web vivant, arrachées à un flux d'information en continu ou à un territoire en expansion. La discrétisation du Web par les archives est le fruit d'une sélection, mais surtout d'un ensemble de destructions, comme le souligne J. Derrida \citep[p.60]{derrida_trace_2014}. Archiver c'est avant tout détruire ce que l'on ne peut conserver.

Par ailleurs, la collecte propulse les ressources archivées dans une nouvelle temporalité. Les pages du Web passé n'appartiennent plus au temps du Web vivant mais au temps des archives : une temporalité faite d'instantanés figés et sans possibilité d'extension. Il n'y a pas de continuité absolue entre deux versions d'une même page archivée. D'un crawl à l'autre tout peut changer (Section \ref{sec:3_constituer}). Ainsi, il nous faudra discuter des phénomènes de leurres et de cécité des collectes\footnote{\textit{crawl blindness} en anglais}, de la notion de cohérence entre pages et de la présences de contenus sur-archivés qui peuvent être source de nombreux biais d'analyse. 

Passé ces mises en gardes, nous décrirons le développement de notre propre moteur d'exploration d'archives Web. Un moteur adapté au format DAFF et suffisamment flexible pour être le support de nos futures expérimentations. Nous détaillerons notre chaine d'extraction et d'enrichissement des archives, ainsi que la pièce maitresse de tout système de fouille : le schéma d'indexation et ses implications. 

Enfin, nous constaterons que les archives Web ne sont pas des traces directes du Web vivant, mais plutôt les traces directes des crawlers. Nous donnerons ainsi des exemples d'artéfacts de crawl, présents dans les archives de l'Atlas e-Diasporas et qui, à nos yeux, sont des freins majeurs à toute exploration large des corpus. Ce sera l'occasion de porter un regard critique sur les archives telles que nous les connaissons et d'ouvrir la voix à une exploration fragmentée du Web passé.\\


\section{Détruire pour mieux archiver}
\label{sec:4_derrida}

\noindent J.L. Borges ouvre la seconde partie de son recueil de nouvelles \textit{Fictions} \citep{borges_fictions_1974} par un court texte intitulé \textit{Funes ou la mémoire}. Il y fait le compte rendu concis de la rencontre entre son narrateur et le mystérieux Irénée Funes, personnage ayant la capacité de ne rien oublier, jamais. Funes a une mémoire prodigieuse : \\

\begin{fullwidth}
"\textit{En effet, non seulement Funes se rappelait chaque feuille de chaque arbre de chaque bois, mais chacune des fois qu'il l'avait vue ou imaginée. Il décida de réduire chacune de ses journées passées à quelque soixante-dix mille souvenirs, qu'il définirait ensuite par des chiffres. Il en fut dissuadé par deux considérations : la conscience que la besogne était interminable, la conscience qu'elle était inutile. Il pensa qu'à l'heure de sa mort il n'aurait pas fini de classer tous ses souvenirs d'enfance.}" --- \citep[p.~116-117]{borges_fictions_1974}\\
\end{fullwidth}

\noindent L'esprit de Funes est engorgé de souvenirs d'une infinie précision, enregistrés en continus. Mais la mémoire pour fonctionner, nous dit Borges, a besoin d'oublier, de sélectionner et de généraliser. C'est en substance la thèse soutenue par J. Derrida qui décrit le geste de l'archiviste comme un geste de pouvoir : le pouvoir de choisir ce qui doit être préserver ou non. L'archivage est le résultat d'une sélection féroce qui doit détruire avant de sauver : "\textit{Il n'y a pas d'archives sans destruction, on choisit, on ne peut pas tout garder.}" \citep[p.~60]{derrida_trace_2014}. C'est ainsi que l'organisation légitime de l'héritage collectif revient aux seuls archivistes qui définissent au présent la mémoire de demain\footnote{Internet Archive décide, suite à l'élection de D. Trump en 2016, de créer une nouvelle copie de ses corpus d'archives Web et de les déplacer au Canada (\url{https://frama.link/hgBbtPp6})}, en classifiant et hiérarchisant dans les bibliothèques les traces de nos expériences passées. Ce faisant, pour Derrida "\textit{l'archive commence là où la trace s'organise, se sélectionne}" \citep[p.~61]{derrida_trace_2014}, car toute expérience finit tôt ou tard par s'effacer, il en va de sa nature même. Ainsi, pour maintenir le lien qui nous renvoie à ce qui n'est plus là, il faut archiver nos traces avant qu'elle ne disparaissent. 

Le Web vivant est tout autant un flux continu d'information qu'un territoire en perpétuelle expansion (Section \ref{sec:2_web}). Pour en archiver les traces, il faut procéder par \textbf{discrétisation}, c'est à dire : diviser une forme continue en une ou plusieurs valeurs individuelles. Les systèmes de stockage WARC et DAFF (Section \ref{sec:3_constituer}) réduisent le Web en un ensemble discret de pages archivées. Or, depuis le lancement d'Alta Vista en 1995\footnote{Alta Vista fut le plus important moteur de recherche pré-Google, capable d'indexer une grande partie des pages du Web et de les rendre accessibles via des requêtes plein-texte (\url{https://en.wikipedia.org/wiki/AltaVista})}, la page Web est considérée comme valeur élémentaire d'indexation, de fouille et d'exploration de la toile. Il en va de même pour les archives Web pour qui la page demeure l'unité de base de toute collecte. Ainsi, dans la suite de ce manuscrit, nous schématiserons une campagne de crawl comme telle : 

Un site Web archivé consiste en \textit{n} pages Web numérotées $\{p_1$,...,$p_n\}$. Un corpus d'archives Web est le résultat d'un ou plusieurs crawls successifs $\{c_1$,...,$c_l\}$. Nous appelons crawl $c_i$ le processus de collecte des pages Web $\{p_1$,...,$p_n\}$ d'un site Web donné. Le temps nécessaire au téléchargement des pages est supposé négligeable. Nous appelons $t_i(p_j)$ la date de téléchargement de la page $p_j$ au cours du crawl $c_i$. La première date de téléchargement d'une page $p_j$ est, enfin, notée $\min\limits_{i} t_i(p_j)$. La figure \ref{fig:discret} illustre cette mécanique pour les pages $p_1, p_2, p_3$. 

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/discretisation}
  \caption{Archivage des pages $p_1, p_2, p_3$ au cours du crawl $c_i$}
  \label{fig:discret}
\end{figure*}

\section{Un temps sans durée}
\label{sec:4_temporalite}

\noindent Au cours d'une collecte, les pages archivées sont propulsées dans une nouvelle temporalité. Elles n'ap\-partiennent plus au temps présent du Web vivant, mais au temps figé des archives passées. Comment dès lors capturer le temps présent ? Cette question nous ramène à Saint Augustin, dont l'expression du  présent à partir de l'instant influence encore aujourd'hui la pensée occidentale. Pour Saint Augustin, le présent est une suite infinie de points élémentaires, des instantanés sans étendue :\\ 

\begin{fullwidth}
"\textit{(...) Et cette même heure se compose elle-même de parcelles fugitives. Tout ce qui s'en détache, s'envole dans le passé; ce qui en reste est avenir. Que si l'on conçoit un point dans le temps sans division possible de moment, c'est ce point-là seul qu'on peut nommer présent. Et ce point vole, rapide, de l'avenir au passé, durée sans étendue; car s'il est étendu, il se divise en passé et avenir. Ainsi, le présent est sans étendue.}" ---  \citep[livre XI, chap. XV, 20, p.~195]{saint_augustin_confessions_1993}\\
\end{fullwidth}

\noindent Le présent se déploie sous nos yeux comme un temps insaisissable qui, à peine éprouvé, cesse déjà d'exister pour se diluer dans le passé. La seul manière de le capturer reste donc de le diviser et de le réduire à ses plus petits éléments. Ainsi en va-t-il des archives Web qui sont, par construction, des instantanées du Web vivant : une suite de blocs DAFF régulièrement collectés et associés à des date de téléchargement. 

Mais dans le temps des archives il n'y pas de durée. Toute page collectée n'a d'étendue temporelle que sa seule date de téléchargement. Sur ce point, l'un des enjeux de l'exploration sera justement de réinstaller de la durée dans les corpus archivés. Les phénomènes que nous souhaitons observer et étudier ont besoin d'être rapportés à une durée. Que l'on parle de l'évolution lente d'une communauté de bloggeurs ou de l'éruption soudaine d'un événement dans un forum de discussion, il faudra à chaque fois pouvoir en éprouver l'étendue dans le temps. 

Pour réintégrer de la durée dans les archives, nous nous proposons de discuter de la notion de \textbf{persistance}. Une page archivée sera dite persistante si d'une version à l'autre, son contenu reste inchangé. Dans la formalisme DAFF, les données des pages sont identifiées par des clés SHA-256 (Section \ref{sec:3_constituer}). Ces clés sont des signatures uniques construites à partir du contenu même des pages archivées. Ainsi, en comparant les deux clés SHA-256 de deux versions successivement crawlées d'une même page, il est possible de savoir si cette page a évolué ou non. Par se procéder, nous pouvons identifier des chaînes de persistance entre différents collectages.

Chaque chaîne de persistance s'ouvre sur une date de \textit{dernière modification}. Nous appelons ainsi $\mu_i(p_j)$ la date de \textit{dernière modification} d'une page $p_j$ au cours d'un crawl $c_i$, avec $\mu_i(p_j) \leq t_i(p_j)$. Par définition, au sein d'un même crawl, la date de dernière modification d'une page précédera toujours (ou sera égale à) sa date de téléchargement. La figure \ref{fig:last_modified} donne à voir des chaînes de persistance entre les multiples captures de la page $p_1$.

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/last_modified}
  \caption{Chaînes de persistance entre captures (bleu) et dates de dernière modification (rouge) pour la page $p_1$}
  \label{fig:last_modified}
\end{figure*}

\noindent Intuitivement, il devient alors possible de dire d'une page archivée qu'elle n'a pas évolué depuis telle ou telle collecte, qu'elle a duré dans le temps. De plus, grâce aux chaînes de persistance, la datation des corpus d'archives Web s'affine, se fait plus précise. Une page ne sera plus maintenant seulement rapportée à sa seule date de téléchargement mais également à sa date de dernière modification, potentiellement bien antérieure. La table \ref{tab:datation_1} propose ainsi une échelle de datation, utile pour évaluer la précision historique d'un élément du Web passé dont l'unité d'analyse reste, pour le moment, la page Web :\\

\begin{table}
\hspace{2em}%
  \label{tab:datation_1}
  \begin{tabular}{lll}
    \toprule
    Unité & Nature de la date &\\
    \midrule
    page&lancement du crawl & \tikzmark{start}\\
    page&téléchargement &\\
    page&dernière modification & \tikzmark{end}\\         
  \bottomrule
\end{tabular}
  \bigskip
  \caption{Échelle de datation d'une page Web archivée}
\end{table} 

\begin{tikzpicture}[overlay,remember picture]
\draw[->] let \p1=(start), \p2=(end) in ($(\x1,\y1)+(0.8,0.2)$) -- node[label=right:précision historique] {} ($(\x1,\y2)+(0.8,0)$);
\end{tikzpicture}

\noindent En nous appuyant sur cette grammaire, nous souhaitons maintenant discuter de trois biais majeurs dont il faut prendre connaissance avant de débuter toute exploration.

\subsection{Cécité de crawl}

\noindent À ce que nous appelons cécité de crawl\footnote{\textit{Crawl blindness} en anglais} correspondent l'ensemble des changements subis par une page Web mais non captés par le crawler ou, tout au moins, mal daté par ce dernier. C'est une notion assez intuitive, dont nous donnons une illustration avec la figure \ref{fig:crawl_blind}. Dans cet exemple, une page $p_1$ subit quatre évolutions successives $e_1, e_2, e_3, e_4$ correspondant respectivement à : la publication d'une image accompagné d'un texte ($e_1$ puis $e_2$), la publication d'une seconde image directement suivie par sa suppression ($e_3$ puis $e_4$). Aux yeux de l'explorateur seul le résultat de $e_2$ restera gravé dans les archives (points bleus). Jamais il n'aura connaissance de l'état $e_1$ ni même de l'existence de $e_3$.     

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/crawl_blind}
  \caption{Cécité de crawl pour une page $p_1$}
  \label{fig:crawl_blind}
\end{figure*}

\noindent Ces loupés sont essentiellement dus à la difficulté de calibrer un crawler vis à vis de la fréquence de mise à jour d'un site (Section \ref{sec:3_constituer}).

\subsection{Cohérence entre pages}

\noindent Dans les archives discrétisées du Web, deux pages collectées ne sont pas forcément cohérentes l'une envers l'autre. Prenons l'exemple de deux pages du Web vivant connectées deux à deux par un lien de citation hypertexte. L'une citant l'autre. Depuis la présentation de l'Atlas e-Diasporas (Section \ref{sec:2_atlas}), nous savons à quel point la nature de ces liens est importante aux yeux des sociologues et historiens.

Mais qu'en est-il, si dans les archives la capture de ces sites est espacée de plusieurs mois ou de plusieurs années ? Ce lien a-t-il encore du sens ? Peut on dire que ces pages sont toujours cohérentes entre elles ? Sur ce point, M. Spaniol \citep{spaniol_data_2009} propose une définition générale de la \textbf{cohérence} entre deux pages archivées, ainsi :

\begin{itshape}
\begin{enumerate}[leftmargin=*]  
\item Une page est toujours cohérente avec elle même
\item L'intervalle d'invariance $[\mu_i(p_j),\mu_i(p_j)^*]$ de la page $p_j$ est borné par la date de dernière modification $\mu_i(p_j)$ par rapport à $ t_i(p_j)$ et le prochain changement $\mu_i(p_j)^*$ subit par $p_j$ directement après $t_i(p_j)$
\item Deux pages ou plus sont cohérentes si il existe un seul point dans le temps (ou un intervalle) $t_{\mathrm{coherence}}$ tel que l'on puisse trouver une intersection non vide des intervalles d'invariance de toutes ces pages :
\end{enumerate}
\[
	\forall p_j, \exists t_{\mathrm{coherence}}:t_{\mathrm{coherence}} \in \bigcap^n_{i=j}[\mu_i(p_j),\mu_i(p_j)^*] \neq \emptyset
\]
\end{itshape}

\noindent La cohérence, telle qu'énoncée ici, est une cohérence absolue. Il suffit d'un unique chevauchement d'invariance, même en dix années de collecte, pour dire de deux pages qu'elles sont cohérentes.

Or l'explorateur d'archive est avant tout un observateur, son point de vue est situé : dans l'espace (une URL donnée) autant que dans le temps (une date, un intervalle). Au cours d'une exploration, nous serons plus souvent amené à nous demander si deux pages sont cohérentes par rapport à notre point d'observation $t_i(p_j)$ plutôt que dans le cas général. Cette focalisation du regard est ce que M. Spaniol nomme \textbf{cohérence par observation}\footnote{\textit{Observable coherence} en anglais et dans la littérature \citep{spaniol_data_2009}} et qu'il définit comme suit :\\

\begin{itshape}
\noindent Deux pages ou plus sont cohérentes par observation, si il existe un seul point dans le temps $t_{\mathrm{coherence}}$ tel que l'on puisse trouver une intersection non vide d'intervalles couvrant respectivement la date de téléchargement $t_i(p_j)$ et la date de dernière modification correspondante $\mu_i(p_j)$ (avec $\mu_i(p_j) \leq t_i(p_j)$) :
\[
	\forall p_j, \exists t_{\mathrm{coherence}}:t_{\mathrm{coherence}} \in \bigcap^n_{i=j}[\mu_i(p_j),t_i(p_j)] \neq \emptyset
\]
\end{itshape}

\noindent La figure \ref{fig:coherence} illustre pour deux points d'observation successifs, la notion de cohérence par observation. Dans le premier cas $p_1$ et $p_2$ sont effectivement cohérentes. Dans le second, les intervalles d'invariance ne se chevauchant malheureusement pas, il n'est pas possible de dire de $p_1$ et $p_2$ qu'elles sont cohérentes. 

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/coherence}
  \caption{Cohérence par observation entre les pages $p_1$ et $p_2$}
  \label{fig:coherence}
\end{figure*}

\subsection{Contenus dupliqués}

\noindent L'une des particularités du formalisme DAFF est justement de ne pas dupliquer dans les archives des ressources Web qui n'auraient pas évolué (Section \ref{sec:3_constituer}). Seules les pages ayant subi une transformation sont ainsi re-collectées. Néanmoins, d'un crawl à l'autre, il est possible qu'une partie du contenu de la page soit similaire à la version précédemment capturée et ce malgré les divers changements qu'elle aurait pu subir. On pense notamment aux pages d'accueil des sites d'actualités ou des forums qui présentent des informations publiées sous la forme de listes où chaque nouvel élément est inséré en en-tête. Mécaniquement certains éléments peuvent se retrouver à plusieurs enregistrés dans les archives comme le présente la Figure \ref{fig:duplicate}.

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/duplicate}
  \caption{Contenu d'une page (en rouge) collecté plusieurs fois}
  \label{fig:duplicate}
\end{figure*}

\noindent Cela peut poser de lourds biais d'analyse si l'on cherche par exemple à connaitre la distribution d'un mot clé extrait des pages archivées. Ce dernier pourra, du fait de la structure même du corpus, être artificiellement sur-représenté dans les résultats. 

\section{Construire un moteur d'exploration d'archives Web}
\label{sec:4_moteur}

\noindent Ces différents biais maintenant présentés, nous pouvons nous tourner vers la description de l'architecture de notre moteur d'exploration d'archives Web. Nos corpus étant en DAFF, il n'a pas forcément été possible de réutiliser des éléments open-source issus d'autres moteurs (quasiment tous conçus pour accueillir du WARC). De fait cette section est intéressante pour qui souhaite mettre en place ou comprendre dans le détail la mécanique d'une tel système. Notre architecture suit néanmoins la structure classique d'une chaîne d'extraction, d'analyse et de visualisation de données à grande échelle \citep{marz_big_2015}. La figure \ref{fig:architecture} donne à voir une illustration de son fonctionnement.  Il s'agira donc ici d'une description plutôt orientée ingénierie dont la majeure partie a fait l'objet d'une publication démonstration\footnote{Lobbé, Q. (2018), \textit{Revealing Historical Events out of Web Archives}, TPDL 2018}.

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/architecture}
  \caption{Architecture de notre moteur d'exploration d'archives Web}
  \label{fig:architecture}
\end{figure*}

\subsection{Extraction et enrichissement}

\noindent Nous suivons ici l'hypothèse que notre moteur doit rendre accessible les archives Web d'une seule e-Diaspora à la fois. Nous prendrons comme exemple les archives du corpus marocain.
 
La première étape consiste à extraire les informations contenues dans les fichiers DAFF. Rappelons que chaque corpus est divisé en deux fichiers DAFF : les données d'une part (\textit{data}) et les méta données (\textit{metadata}) d'autre part. Pour ce faire, nous commençons par adapter une librairie JAVA fournie par les équipes de l'INA (\textit{dlweb-commons}) qui cherche à transférer les archives des fichiers DAFF vers le système de stockage d'Hadoop\footnote{Voir \url{https://hadoop.apache.org/} et \url{https://fr.wikipedia.org/wiki/Hadoop}}, le Hadoop Distributed File System (HDFS). Le HDFS est un système de fichiers qui permet de manipuler de larges volumes de données, de manière distribuée (ie : réparti entre plusieurs machines) et relativement scalable (ie: pouvant supporter une forte montée en charge). Le format DAFF a ceci de limitant, qu'il reste pensé pour le stockage et non pour la manipulation des données. Filtrer un fichier DAFF par URL ou date de téléchargement n'est, par exemple, pas trivial.

Une fois chargées dans le HDFS, nos data et metadata sont envoyées dans un pipeline de traitement nommé Spark\footnote{\url{https://spark.apache.org/}}. Spark permet de travailler par batchs (ie: par petits lots de données) dans un environnement distribué : c'est à dire que les data et metadata seront segmentées en sous ensembles plus facilement manipulables, puis répartis sur plusieurs machines où elles subiront toutes les mêmes traitements en parallèle (filtres, jointure, groupement, etc). Spark est un outil flexible dans lequel nous pouvons définir une suite d'instructions ayant pour finalité la fusion des data et metadata en une seule et même source de données. La figure \ref{fig:spark} décrit la manière dont s'enchainent ces diverses transformations. Les metadata sont traitées en premier et peuvent suivant la configuration du système être filtrées par date de téléchargement ou nom de domaine (ie: par site Web). Puis, en nous rappelant que les metadata possèdent chacune un pointeur vers le bloc de data dont elles sont l'extension (champ \textit{content} en DAFF, Section \ref{sec:3_constituer}), nous remplaçons l'identifiant des metadata pour l'identifiant de la data correspondante. Cette manipulation nous permet ensuite de grouper les metadata par identifiants communs, c'est à dire, toutes les metadata d'une seule et même chaine de persistance (Section \ref{sec:4_temporalite}). C'est à cette étape que nous identifions notamment les dates de dernière modification. De là, nous opérons une jointure entre les metadata et data afin de rassembler enfin les méta données du crawler et le contenu même des pages archivées. Pour terminer, nous préparons nos données à être envoyées dans le moteur de recherche, sans oublier de les enrichir avec des informations tirées de l'Atlas e-Diasporas (type de sites, langue, etc).  

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/spark}
  \caption{Transformation des data et metadata dans Spark}
  \label{fig:spark}
\end{figure*}

\noindent Deux configurations différentes ont été testées pour Spark, l'une distribuée entre plusieurs machines d'un même cluster (ie: groupe de machines), l'autre distribuée sur l'ensemble des cœurs d'une seule machine puissante. Si la première configuration s'est révélée la plus rapide (traitement de l'ensemble du corpus marocain en 3 jours), il a fallut néanmoins s'en détourner. En effet, Spark étant particulièrement dur à piloter sur un réseau souvent instable, il était régulier de voir le traitement des données s'arrêter après avoir perdu connexion avec une ou plusieurs machines. De fait, nous avons préféré nous contenter de la seconde configuration, plus lente (une dizaine de jours) mais garantissant la totalité du traitement. 

\subsection{Adapter un moteur de recherche}

\noindent Au cours de la Section \ref{sec:3_constituer}, nous avons présenté l'ensemble des méthodes aujourd'hui utilisées pour fouiller dans les archives. La plupart d'entre elles, s'appuient sur l'utilisation de moteurs de recherches qui offrent la possibilité de requêter des documents\footnote{Les données manipulées par des moteurs de recherche sont de manière générale appelées \textit{documents}} en plein texte. De notre côté, nous avons choisi d'adapter une solution open-source existante Solr/Lucene à la nature particulière de nos archives Web. Solr\footnote{Voir \citep{grainger_solr_2014} et \url{http://lucene.apache.org/solr/}} est un serveur de \textit{search}, c'est à dire qu'il permet de faire le lien entre une requête utilisateur (un mot clé, une dimension, etc) et un ensemble de documents préalablement indexés. Solr est construit au dessus de la librairie d'indexation Lucene\footnote{Voir \citep{hatcher_lucene_2004} et \url{http://lucene.apache.org/index.html}} dont le principe de base est de stocké un texte dans un \textbf{index inversé}.

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/index}
  \vspace*{0.2cm}  
  \caption{Principe de base d'un index inversé}
  \label{fig:index}
\end{marginfigure} 


Le fonctionnement basique d'un index inversé est illustré par la figure \ref{fig:index}. Un premier tableau de données (a) renferme le contenu de deux documents $doc 1$ et $doc 2$ dont les identifiants (\textit{doc id}) sont respectivement $0$ et $1$. Un second tableau de données (b) contient ce que l'on appelle un dictionnaire de termes (\textit{term dict}). Dans ce dictionnaire, sont répartis l'ensemble des mots uniques (numérotés de $1$ à $5$) contenus dans $doc 1$ et $doc 2$. Ces mots sont identifiés par des \textit{term id} et sont, de plus, associés à une \textit{posting list} qui fait correspondre à chaque mot la liste des documents où il est présent. 

Ainsi, pour réaliser une recherche plein texte, il suffira de parcourir l'ensemble du dictionnaire jusqu'à trouver les mots correspondants à la requête de l'utilisateur et, par extension, les documents associés. Ces deux tableaux de données (a) et (b) forment ce que l'on appelle un \textit{segment}, soit le bloc de base de tout index inversé. On nomme \textbf{indexation} l'action de stocker un texte dans un index inversé. Différentes stratégies peuvent être mises en place pour accélérer la recherche dans un index, en optimiser la taille, etc.\footnote{Pour de plus amples détails sur l'indexation via Lucene/Solr, voir mon cours sur le fonctionnement interne des moteurs de recherche (Lobbé, Q. 2016, Voyage au cœur d'un index Lucene, \url{http://qlobbe.net/ressources/search.pdf})}. 

Un moteur de recherche se doit d'ordonner ses résultats avant de les retourner à l'utilisateur. On nomme cette étape le \textit{ranking}. Pour cela, il fait appel à une \textbf{fonction de similarité} qui trie les documents résultants en leur attribuant à chacun un score. Parmi les nombreux critères de ranking, les systèmes de search favoriseront souvent les documents où les mots clés recherchés sont les plus fréquents. Cette mesure sera pondérée par l'ajout d'une prime aux termes rares, c'est à dire : peu présents dans l'ensemble des indexes. C'est tout le sens du fameux \textit{tf-idf}\footnote{\textit{Term frequency-inverse document frequency}, fonction de similarité qui évalue l'importance d'un terme dans un document au regard d'un corpus donné (\url{https://fr.wikipedia.org/wiki/TF-IDF})} dont nous réutilisons ici une version légèrement modifiée : la \textit{defaultSimilarity} de Lucene\footnote{En plus du simple tf-df, cette fonction de similarité prend en compte la taille du document et la taille des champs vérifiant la requête (\url{http://lucene.apache.org/core/4_0_0/core/org/apache/lucene/search/similarities/TFIDFSimilarity.html})}. 

Nous n'avons pas eu l'occasion de tester une fonction de similarité propre aux archives Web, la nôtre est finalement très générique. C'est pourtant une question  intéressante, puisque comme le suggère G. Weikum \citep{weikum_longitudinal_2011}, les moteurs d'exploration d'archives pourraient prendre en compte l'aspect temporel des documents collectés. En se basant sur les dérivées premières et secondaires du rapport entre deux dates de téléchargement, il serait ainsi possible de traduire une forme de vitesse ou d'accélération de certains termes dans les archives Web.  

\subsection{Le schéma d'indexation}

\noindent Tout document destiné à l'indexation doit d'abord passer au crible du \textbf{schéma} qui est considéré comme la pierre angulaire de tout moteur de recherche. 

Le schéma est un fichier décrivant dans le détail la façon dont tout document sera indexé, il forme l'ossature de l'indexation. En effet, un document n'est pas indexé d'un seul tenant. Pour maximiser les chances de le voir matcher une requête, il peut être nécessaire de le découper en plusieurs champs (\textit{fields}), ayant chacun des attributs particuliers. Par exemple, un article issu d'un site de news pourra être segmenté suivant son titre, sa date, l'auteur et finalement le cœur du texte. Ce dernier sera indexé de manière classique en vue d'une recherche plein texte, l'auteur en revanche pourra être destiné à une recherche par \textit{facet}, c'est à dire par dimension  (ie: Quels sont les textes de tel ou tel auteur ?). Si tel est le cas, son indexation se fera via des \textit{docValues}\footnote{\url{https://lucene.apache.org/solr/guide/6_6/docvalues.html}}. Dans notre schéma, la plupart des informations issues du fichier de méta données DAFF sont destinées à une recherche par facet, telles que :\\


\begin{fullwidth}
\small
\begin{verbatim}
<field name="id"                type="string"  indexed="true"    multiValued="false" required="true" />

<!-- archive fields -->
<field name="archive_active"    type="boolean" indexed="true"    multiValued="false"/>
<field name="archive_corpus"    type="string"  indexed="true"    docValues="true" multiValued="false"/>
<field name="archive_ip"        type="string"  indexed="true"    docValues="true" multiValued="false"/>
<field name="archive_length"    type="double"  indexed="true"    docValues="true" multiValued="false"/>
<field name="archive_level"     type="int"     indexed="true"    docValues="true" multiValued="false"/>
<field name="archive_referer"   type="string"  indexed="true"    docValues="true" multiValued="false"/>
<field name="archive_mime"      type="string"  indexed="true"    docValues="true" multiValued="false"/>
<field name="is_page"           type="boolean" indexed="true"    multiValued="false" default="false"/>    

<!-- client fields -->
<field name="client_country"    type="string"  indexed="true"    docValues="true" multiValued="false"/>
<field name="client_ip"         type="string"  indexed="true"    docValues="true" multiValued="false"/>
<field name="client_lang"       type="string"  indexed="true"    docValues="true" multiValued="true" />

<!-- crawl fields -->
<field name="crawl_id"          type="string"  indexed="true"    docValues="true" multiValued="true" />
<field name="crawl_id_f"        type="string"  indexed="true"    docValues="true" multiValued="false"/>
<field name="crawl_id_l"        type="string"  indexed="true"    docValues="true" multiValued="false"/>
<field name="crawl_date"        type="date"    indexed="true"    docValues="true" multiValued="true" />
<field name="crawl_date_f"      type="date"    indexed="true"    docValues="true" multiValued="fasle"/>
<field name="crawl_date_l"      type="date"    indexed="true"    docValues="true" multiValued="true" />
\end{verbatim} 
\end{fullwidth}

\newpage
\begin{figure*}
\begin{fullwidth}
\small
\begin{verbatim}
<!-- download fields -->
<field name="download_date"     type="date"    indexed="true"    docValues="true" multiValued="true" />
<field name="download_date_f"   type="date"    indexed="true"    docValues="true" multiValued="false"/>
<field name="download_date_l"   type="date"    indexed="true"    docValues="true" multiValued="false"/> 

<!-- page fields -->
<field name="page_site"         type="string"  indexed="true"    docValues="true" multiValued="false"/>
<field name="page_url"          type="string"  indexed="true"    docValues="true" multiValued="false"/>
<field name="page_url_id"       type="string"  indexed="true"    docValues="true" multiValued="false"/>      

<!-- extracted page fields -->
<field name="page_link"         type="string"  indexed="true"    docValues="true"  multiValued="true"/>     
<field name="page_meta_title"   type="string"  indexed="true"    docValues="false" multiValued="false"/>
<field name="page_meta_desc"    type="text"    indexed="true"    docValues="false" multiValued="false"/>
<field name="page_meta_img"     type="string"  indexed="true"    docValues="false" multiValued="false"/>
<field name="page_meta_date"    type="date"    indexed="true"    docValues="true"  multiValued="false"/>    
<field name="page_meta_author"  type="string"  indexed="true"    docValues="true"  multiValued="false"/>    
<field name="page_title"        type="text"    indexed="true"    docValues="false" multiValued="false"/>

<!-- searchable page fields -->
<field name="page_text"         type="text"    indexed="true"  stored="false" multiValued="true"/>
<field name="page_text_shingle" type="shingle" indexed="true"  stored="false" multiValued="true"/> 
\end{verbatim} 
\end{fullwidth}
\caption{Schéma d'indexation de notre moteur d'exploration d'archives Web}
\label{fig:schema_1}
\end{figure*}

\noindent Pour chaque champ, nous définissons un nom et un type : une date, un nombre (int), du texte (string et text), ... Tous les champs sont indexés (\textit{indexed="true"}). Ceux destinés à une recherche par dimension sont associés à une docValue. Certain champs, comme les dates de téléchargement, sont multivalués (une page peut avoir été crawlée plusieurs fois à l'identique). Le champ de recherche plein text par défaut est le champ page\_text qui couvre l'ensemble du texte d'une page archivée\footnote{Ce champ subit un traitement particulier (un découpage en bi-gram : page\_text\_shingle) dont nous reparlerons en section \ref{sec:retour_au_moteur}}. Dans Spark, lors de la transformation des DAFF en document Solr, nous extrayons les liens de citation hypertextes (page\_link) et les informations contenues dans l'en tête des pages\footnote{Tout ce qui est contenu dans les balises HTML \textit{<meta>} (\url{https://www.w3schools.com/Tags/tag_meta.asp})} (page\_meta\_img, page\_meta\_date, etc). Les diverses dates associées à une page sont également présentes : allant de la date de lancement du crawl (crawl\_date\_f), à la date de téléchargement (download\_date) et en passant par la date de dernière modification (download\_date\_f).

Mais les documents à indexer ne sont pas les seuls à devoir passer par le schéma. En effet, un moteur de recherche est un système à double entrée (Figure \ref{fig:architecture}), conjuguant des documents et des requêtes utilisateurs grâce à une fonction de similarité. Pour ce faire, documents et requêtes doivent parler la \textit{même langue}, c'est à dire qu'une requête utilisateur devra être traitée de la même manière que les documents à indexer, subir les mêmes transformations et interroger les bons champs. Cette suite de transformations est appelé \textit{analyzer}\footnote{Pour plus d'informations sur les analyzers et tokenizers dans Solr: \url{https://wiki.apache.org/solr/LanguageAnalysis}}. Sur ce point, documents et requêtes suivent les traitements suivants : tout d'abord, les majuscules deviennent minuscules (Figure \ref{fig:tokenizer}, (a)), puis le texte est découpé en termes distincts (Figure \ref{fig:tokenizer}, (b)) et les \textit{stopwords}\footnote{Mots qui ne sont généralement pas intéressants pour l'analyse : et, il, elle, ... (\url{https://fr.wikipedia.org/wiki/Mot_vide})} sont écartés (Figure \ref{fig:tokenizer}, (c)), s'en suit une phase appelée \textit{stemming} dans laquelle on ne garde finalement que la racine des termes restants (Figure \ref{fig:tokenizer}, (d)) avant indexation (Figure \ref{fig:tokenizer}, (e)).

\begin{figure}%
  \includegraphics[width=\linewidth]{graphics/tokenizer}
  \caption{Cycle de transformation d'un texte dans notre moteur de recherche}
  \label{fig:tokenizer}
\end{figure}

\noindent Dans le cas particulier des archives Web, l'exploration de pages collectées peut être focalisée autour d'un instant précis. Si l'utilisateur en fait la demande, notre moteur lui proposera différentes stratégies de recherche pour retrouver les pages les plus proches d'une date donnée: soit en amont (Figure \ref{fig:date-picker}, (a)) , soit en aval (Figure \ref{fig:date-picker}, (c)) ou soit autour de cette dernière (Figure \ref{fig:date-picker}, (b)). Internet Archive par exemple utilise la première option dans la WayBack Machine.

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/date-picker}
  \caption{Stratégies de choix d'un ensemble de pages par rapport à une date précise}
  \label{fig:date-picker}
\end{figure*}

\noindent Notre moteur de recherche est finalement déployé sur les serveurs. Les indexes sont distribués entre plusieurs machines pour améliorer les temps de réponse et d'indexation du système. On appelle \textit{sharding} l'action de scinder un indexe en plusieurs sous indexes avant de les distribuer. Nous suivons une configuration classique \textit{master-slave}\footnote{\url{https://lucene.apache.org/solr/guide/6_6/solrcloud.html}} où l'instance maître de notre moteur de recherche centralise les requêtes utilisateur avant de les dispatcher entre ses diverses instances esclaves qui, elles seules, sont habilitées à retourner des résultats. 

\subsection{Interface de visualisation}

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/gui}
  \caption{Capture d'écran de notre interface de visualisation}
  \label{fig:gui}
\end{figure*}

\noindent Une système de visualisation et d'interrogation des archives est dé\-veloppé\footnote{Nommé \textit{Peastee} en référence au narrateur de la nouvelle de H.P. Lovecraft \textit{The Shadow Out of Time} (1935), ce système est téléchargeable ici \url{https://github.com/lobbeque/peastee}} au dessus de notre moteur. Il s'agit en fait d'un \textit{service Web} permettant à tout utilisateur d'écrire une requête et de se voir présenter les résultats sous diverses formes. Un service Web est composé de deux briques logicielles distinctes : un serveur en \textit{node.js}\footnote{\url{https://nodejs.org/en/about/}} se charge tout d'abord de faire la liaison avec le moteur de recherche, une interface Web permet ensuite de visualiser les documents archivées. Les éléments de visualisation sont développés en \textit{d3.js} et l'architecture de l'interface en tant que telle se base sur \textit{angularjs}\footnote{D3 est une librairie Javascript de visualisation de données \url{https://d3js.org/}, Angular est un framework Javascript pour les applications Web \url{https://angularjs.org/}}. Notre interface suit un modèle \textit{en liste} très classique : les résultats sont présentés les uns à la suite des autres et des facets, à la marge, permettent de les filtrer ou de les trier après coup. Divers histogrammes offrent à voir une répartition dans le temps des pages archivées ayant matché la requête de l'utilisateur. La figure \ref{fig:gui} présente une capture d'écran de cette interface, une démonstration en vidéo permet de se faire une idée plus précise de son fonctionnement\footnote{\url{https://youtu.be/snW4O-usyTM}}

Notre service Web accueille les nombreux prototypes que nous avons pu expérimenter tout au long de ces trois années de travail. Nous ne reviendrons pas ici en détail sur leurs développements respectifs, mais bien qu'incomplets ou inabouties, ces prototypes restent des jalons qui nous ont permis de cheminer vers les résultats présentés au chapitre~\ref{chap:6}. On retiendra une visualisation \textit{en oursin} de l'arborescence d'un site archivé mois après mois (Figure \ref{fig:prototypes}, (a)) ou encore une distribution temporelle des liens hypertextes sortant d'une page (Figure \ref{fig:prototypes}, (b), différenciés par catégories : réseaux sociaux, sites migrants e-Diasporas et reste du Web). 

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/prototypes}
  \caption{Prototypes de visualisation d'archives Web}
  \label{fig:prototypes}
\end{figure*}

\section{Les archives ne sont pas des traces directes du Web}
\label{sec:4_legacy}

\noindent Notre moteur d'exploration maintenant présenté, nous voilà enfin en mesure d'interroger les archives de l'Atlas e-Diasporas. Depuis le chapitre \ref{chap:2}, le site \textit{yabiladi.com} et plus particulièrement son forum de discussion attire notre attention. De part la place qu'il occupe dans le corpus marocain depuis le début des années 2000, le site a su jouer un rôle clé pour l'ensemble de la diaspora en ligne. 

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/yabiladi-map}
  \vspace*{0.2cm}  
  \caption{\textit{yabiladi.com} (rouge) dans l'e-Diaspora marocaine}
  \label{fig:yabiladi-map}
\end{marginfigure} 

Notre première requête consiste donc à voir la répartition de \textit{yabiladi.com} dans les archives, saisir et comprendre la dynamique des publications postées sur le forum afin d'identifier des moments clés de l'histoire du site. Mais les résultats que nous retourne notre moteur d'exploration semblent très curieux, la figure \ref{fig:yabiladi-download} présente ainsi la répartition du nombre de pages collectées par jour pour \textit{yabiladi.com}\footnote{Pour être plus précis nous ne conservons que les pages de la section forum du site}, de Mars 2010 à Septembre 2014. Il semble que le site ait littéralement cessé de produire du contenu de Janvier 2013 à début 2014. Or, si l'on passe maintenant par la Wayback Machine\footnote{Voir \url{https://web.archive.org/web/20130801000000*/http://yabiladi.com/}}, on se rend rapidement compte que chez, Internet Archive, des ressources Web ont bel et bien été capturées à ces même dates. Où se situe donc notre erreur ?

\begin{figure}%
  \includegraphics[width=\linewidth]{graphics/yabiladi-download}
  \caption{Distribution du nombre de pages archivées par jours pour \textit{yabiladi.com}}
  \label{fig:yabiladi-download}
\end{figure}

\noindent Nous interprétons nos résultats de la mauvaise manière. Depuis le début de ce chapitre, nous cherchons à mettre en évidence les nombreux biais d'analyse possibles pour qui souhaite mener à bien une exploration d'ar\-chives Web. Nous avons notamment évoqué les cécités de crawl (Section \ref{sec:4_temporalite}), mais sans nous attendre à trouver dans nos propres corpus une telle défaillance de la collecte. Après enquête auprès des équipes de l'INA, il s'avère que le crawler de l'institution a stoppé sa collecte de \textit{yabiladi.com} pendant toute une année, avant qu'archivistes et chercheurs ne s'en rendent compte et relancent le robot. Ce que la figure \ref{fig:yabiladi-download} donne à voir est un \textit{artéfact de crawl}. C'est à dire un effet mécanique du crawler qui aura influencé la forme même du collectage, au delà du simple décalage ou de l'imprécision inévitable pour ce type de campagne. 

Les archives Web ne sont pas les traces directes du Web, elle sont les traces directes des crawlers. Les outils de collecte façonnent l'image de ce qu'ils sont supposés préserver. Ils en modifient la forme, potentiellement le fond, et par là même, la nature des interprétations que nous ferons du corpus si l'on n'y prend pas garde. Ce que l'on voit dans les archives reste avant le geste de l'archiviste et des ses dispositifs de capture. 

\begin{center}
	\textbf{***}
\end{center}

\noindent En 1838, L. Daguerre réalise un daguerréotype\footnote{Procédé photographique basé sur l'exposition à la lumière d'une surface d'argent pure (\url{https://fr.wikipedia.org/wiki/Daguerréotype})}, le "\textit{Boulevard du Temple}", qui est aujourd'hui reconnu comme l'une des première photographie figurant un être humain : deux hommes seuls dans une rue vide (Figure \ref{fig:boulevard}). En réalité, le boulevard était ce jour là bondé. Demandant un temps d'exposition particulièrement long, les seuls sujets (en plus des bâtiments et des arbres) que le dispositif a su capturer sont ceux qui étaient restés pratiquement immobiles : un cireur de chaussures et son client assis devant lui. Cet exemple illustre parfaitement ce que nous rencontrons dans les archives Web. Associées à des dates de téléchargement qui les arrachent à leur temporalité, les archives Web sont des objets discrétisés et figés. Sans lien direct avec la réalité dont elles sont pourtant censées être le reflet. 

Afin d'améliorer la pertinence historique de nos analyses à venir et pour s'affranchir des crawlers et de leurs artéfacts, nous proposerons, dans la suite de ce manuscrit, de descendre au delà du niveau des pages Web capturées et de s'appuyer sur une nouvelle unité d'explo\-ration des archives : le \textbf{fragment Web}. 

\begin{figure*}
  \centering
  \includegraphics[width=\linewidth]{graphics/boulevard}
  \caption{"Boulevard du Temple", Louis Daguerre, 1838}
  \label{fig:boulevard}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Chapitre 5 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Fragmenter les Archives Web}
\label{chap:5}

\par\noindent Les artefacts de crawl sont indissociables des archives Web telles que nous les connaissons (Section \ref{sec:4_legacy}). Ils sont liés organiquement à la structure même des ressources collectées (Section \ref{sec:3_constituer}), issus de l'as\-sociation d'une page Web et d'une date de téléchargement. Ces artefacts et leurs effets induisent nombres de biais pour qui souhaite explorer le Web passé : collectages non régulier, sur-représenta\-tion de certaines parties d'un site, incohérences entre les contenus préservés, etc. 

Nos travaux portant sur l'exploration de corpus d'archives Web déjà existants ou constitués de longue date, nous ne proposerons pas ici d'alternative aux formats WARC et DAFF. Nous chercherons plutôt à définir, partant d'une collecte terminée, une stratégie d'analyse capable de s'affranchir de l'héritage pesant des crawlers ou, tout au moins, d'en atténuer les effets. Par ailleurs, nous souhaitons mener une exploration large (en terme de pages à visiter) et profonde (en terme de durée à balayer) de nos corpus d'archives Web. Mais ce faisant, nous voulons aussi garder la possibilité de nous appuyer, au besoin, sur une analyse plus fine de certains éléments. Cela implique le développement d'une méthodologie hybride capable de débrayer du quantitatif vers le qualitatif. Cette approche s'articulera autour d'une nouvelle entité qui pourra faire cohabiter traitements algorithmiques à grande échelle et campagnes de validation humaine.

Sur ce point, nous proposerons dans ce chapitre de changer d'unité d'exploration en introduisant les \textbf{fragments Web}. Nous pensons, en effet, qu'il peut être bénéfique de mener une analyse au dessous du niveau des pages Web archivées. Pour valider cette intuition, le fragment Web offrira aux explorateurs une plus grande souplesse et de nouveaux outils pour interroger les archives. Il se voudra également object d'étude à part entière. À travers les fragments Web, nous questionnerons directement le geste des auteurs et des lecteurs des sites collectés, en suivant les indices de leur passage sur la toile. Revenir à l'humain dissimulé sous les archives. Pour ce faire, nous porterons notre réflexion sur la question de la datation des archives Web en associant à chaque fragment une date d'édition. Ainsi nous nous approcherons, au mieux, du Web tel qu'il a été de son vivant. Enfin, nous reviendrons en miroir sur les modalités techniques et théoriques d'un moteur d'exploration basé, cette fois ci, sur le fragment Web comme unité principale d'indexation. Un cas simple de détection d'événements dans les archives Web nous permettra d'en faire la démonstration.

\section{Au dessous des pages Web}
\label{sec:5_dessous}

\noindent Comme le résume G. Weikum \citep{weikum_longitudinal_2011}, les archives Web sont de véritables mines d'or pour qui souhaite étudier l'histoire du Web passé\footnote{"\textit{These archives host a wealth of information, providing a gold mine for sociological, political, business, and media analysts.}" \citep{weikum_longitudinal_2011}}. Mais tout trésor est difficile d'accès et nous avons déjà évoqué, au regard de l'état de l'art (Section \ref{sec:3_20ans}), à quel point les corpus archivés restaient pour nous des territoires inexplorés, repliés et fortifiés. 

L'archive est pourtant une matière qui ne doit pas rester fermée \citep{ketelaar_de_2006}. Toujours prête à être questionnée. C'est au travers des lectures, discussions et interprétations successives des archives que s'écrit l'histoire. Pour plonger au cœur des archives Web, essayons d'ouvrir une brèche dans nos corpus afin d'y extraire une nouvelle entité. Ce fragment Web, comme nous le nommons, est issue du fractionnement des pages Web collectées. Sa construction s'appuie sur plusieurs éléments, plusieurs inspirations. Tout d'abord, il s'agira pour nous d'adopter une attitude plus souple vis à vis des archives en cherchant à les décomposer pour mieux les explorer. Ensuite, nous inscrirons les fragments dans la droite lignée des strates du Web au sens où les décrit N. Brügger. Nous nous attarderons alors sur la question de la datation des ressources collectées en introduisant les dates d'édition à notre grammaire. Nous nous servirons de ces dates, pour finalement descendre vers une plus grande précision historique et ramener les archives vers la temporalité du Web passé.

\subsection{Découper, déplacer, monter}

\noindent Funes, vit dans l'indexation d'un présent perpétuel (Section \ref{sec:4_derrida}). Condamné à ne plus jamais rien oublier, il lui devient impossible de penser, de raisonner et de s'inventer :

\begin{fullwidth}
"\textit{[Funes], ne l’oublions pas, était presque incapable d’idées générales, platoniques. Son propre visage dans la glace, ses propres mains, le surprenaient chaque fois. (...) Penser c'est oublier des différences, c'est généraliser, abstraire. Dans le monde surchargé de Funes il n'y avait que des détails, presque immédiats.}" --- \citep[p.~117-118]{borges_fictions_1974}\\
\end{fullwidth} 

\noindent Pour mémoriser il faut oublier. Ré-arranger et faire du montage. Nos souvenirs sont des sélections qui, mises bout à bout, collées, accélérées ou ralenties forment le fil de nos histoires et de nos vies. Nous décrivions en section \ref{sec:3_constituer}, comment les conditions d'accès aux archives Web rendaient difficile leur exploration par les chercheurs\footnote{Notons  néanmoins l'existante du projet \url{https://archivesunleashed.org/} et des outils de l'Omilab \url{https://github.com/omilab/internet-archive-link-extractor}}. Chevillées aux niveaux des seules pages Web les outils d'analyse existants (la Wayback Machine tout autant que notre propre moteur d'exploration, Section \ref{sec:4_moteur})) nous nous permettent pas de manipuler les résultats de nos requêtes. Les archives sont consultables, certes, mais restent enfermées dans des \textit{interfaces-vitrines} plutôt que de nous être restituées sur des tables de montage. 

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/marker}
  \caption{C. Marker, 1977, Le Fond de l'Air est Rouge, (\url{https://youtu.be/dO1E4GYjF1s})}
  \label{fig:marker}
\end{marginfigure}

En achevant \textit{Le Fond de l'Air est Rouge} en 1977, le cinéaste C. Marker revient amère sur l'avènement des mouvements contestataires et révolutionnaires dans années 1960, événements dont il a été le témoin direct. Il remonte et assemble 15 années de ses propres archives filmiques qu'il aborde sous un angle inédit : "\textit{on ne sait jamais ce que l'on film, on ne sait jamais ce qu'il y a derrière une image}" (Ibid, Partie II, 14mn 22s) nous dit il en voix off. Détachées de lui et faisant désormais partie de l'histoire, ses archives peuvent enfin être confrontées et ré-interrogées. En cela la posture de l'historien face à un document archivé se rapproche de celle du monteur de cinéma face à une matière filmée. Leurs outils sont semblables. Lorsqu'il invente l'histoire, l'historien découpe, isole et rapproche des sources archivées potentiellement très éloignées. 

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/godard}
  \caption{J.L. Godard, 1993, Je~Vous Salue, Sarajevo, (\url{https://youtu.be/WKbfu8rRrho})}
  \label{fig:godard}
\end{marginfigure}

Dans son court métrage \textit{Je Vous Salue, Sarajevo}, réalisé en 1993 pendant la Guerre de Bosnie-Herzégovine, J.L. Godard déconstruit une photographie du reporteur de guerre R. Haviv. Il fragmente cette image pour faire se correspondre des inserts éclatés à la manière d'un collage-poème ou d'un cinétract\footnote{Mini-films non signés à caractère militant, réalisés en mai et juin 1968 (\url{https://fr.wikipedia.org/wiki/Cin\%C3\%A9tract})}. Par le collage, les fondus et les découpes Godard rompt la continuité de l'archive qu'il utilise comme source première. Il peut ainsi rendre compte, image après image, de la cruauté qui frappe les rues Sarajevo. Le film finit par dévoiler entière, l'image dans toute son horreur., \textbf{décomposer} pour mieux \textbf{recomposer}.\\

\noindent Il y a dans les travaux de Godard et de Marker une souplesse d'action vis à des archives que nous pourrions appliquer à nos propres corpus. Chercher à avoir en main des éléments fragmentés de pages Web éloignées, que nous pourrions associer, à souhait, afin de traiter plus largement d'un moment particulier de l'histoire du Web. Comment se donner la possibilité de rapprocher automatiquement deux contenus archivés hors du carcan de leurs pages Web respectives ? Peut on ralentir ou accélérer le cours de nos archives ? 

\subsection{Les strates du Web}

\noindent Le glissement d'un niveau d'analyse à un autre, vers un en-dessous de la page archivée, est formulé pour la première fois par l'historien du Web N. Brügger lorsque, cherchant à définir le site Web comme objet potentiel de recherches historiques \citep{brugger_website_2009}, ce dernier en vient à introduire la notion de \textbf{strates analytiques du Web}\footnote{En anglais : \textit{analytical Web strata}.}.

Brügger suggère de construire un système d'analyse dynamique pour réajuster, au besoin, le périmètre d'une recherche portant sur le Web. L'observateur doit ainsi pouvoir passer d'un ensemble de sites, à une page unique, voire descendre jusqu'aux éléments constitutifs de cette dernière (un texte, une image, etc)\footnote{"\textit{One can distinguish the following five analytical strata: the web as a whole; the web sphere; the individual website; the individual webpage; and an individual textual web element on a webpage, such as an image}", \citep[p.19]{brugger_website_2009}}. 
Cette approche, notons le, n'est pas confinée au Web archivé, elle peut très bien s'adapter au Web vivant. Brügger définit ainsi 5 niveaux d'analyses, allant du  plus englobant au plus élémentaire, comme l'illustre la figure \ref{fig:web_strata}.

\begin{figure*}
  \centering
  \includegraphics[width=\linewidth]{graphics/strata}
  \caption{Les 5 strates analytiques du Web, d'après \citep{brugger_website_2009}}
  \label{fig:web_strata}
\end{figure*} 

\noindent Le premier niveau englobe l'entièreté des sites du Web vivant (Figure \ref{fig:web_strata}, (1)). Il inclut également les éléments de back-end (base de données, code côté serveur, etc) et plus généralement l'ensemble de l'infrastructure physique du Web (serveurs, câbles réseax, supports numériques, etc). Une sphère Web désigne un ensemble de sites Web sélectionnés par un chercheur (Figure \ref{fig:web_strata}, (2)). C'est une construction ad hoc motivée par une question de recherche donnée, une thématique précise\footnote{La notion de sphère Web est inspirée des travaux de K. Foot sur le volet numérique des campagnes électorales états-uniennes du début des années 2000 \citep{foot_web_2006}}. Les acteurs Web regroupés au sein de ces sélections n'ont pas forcément conscience d'appartenir à un tel groupe. Par exemple, les réseaux de sites e-Diasporas (Section \ref{sec:2_atlas}) peuvent être considérés comme des sphères Web. Sites et pages Web (Figure \ref{fig:web_strata}, (3-4)) sont ensuite définis de manière égale à ce que nous proposions en section~\ref{sec:4_temporalite}. L'élément Web, quant à lui, est considéré comme l'élément textuel minimal d'une page Web\footnote{"\textit{The Web element is the minimal textual element on a webpage}", \citep[p.20]{brugger_website_2009}} (Figure \ref{fig:web_strata}, (5)). Ce peut être un ensemble de caractères écrits sur une page, des images fixes ou mobiles, ainsi que des sons. Brügger en revanche écarte de cette liste les menus, barres d'informations et autres éléments de navigations. \\

\noindent Nous voulons penser le futur fragment Web comme un \textbf{sous ensemble cohérent} d'une page Web. Il s'inscrira dans la continuité des strates du Web, en se situant quelque part entre l'élément Web et la page Web. Un fragment pourra, en fonction des cas, être un élément Web seul, un groupe de plusieurs éléments, voire la page Web dans son entièreté\footnote{Nous reviendrons dans le détail sur la question de l'étendue du fragment Web dans la section~\ref{sec:5_scraping}}. 

Dès à présent, pour tout site Web composé de \textit{n} pages Web $\{p_1$,...,$p_n\}$, nous assumons que chacune de ses pages $p_j$ consiste en \textit{m} fragments Web numérotés $\{f_{j1},...,f_{jm}\}$ (Figure \ref{fig:fragment}).

\begin{figure}%
  \includegraphics[width=\linewidth]{graphics/fragment}
  \caption{Une page $p_1$ et ses fragments Web $f_{11}, f_{12}, f_{13}$}
  \label{fig:fragment}
\end{figure}

\subsection{Dater une page archivée}

\noindent La datation des pages archivées peut être réévaluée à l'aune du fragment Web. Depuis la fin du précédent chapitre une question demeure : Comment tendre vers une plus grande précision historique ? Comment s'affranchir des seules dates de téléchargement ? Comment \textit{bien} dater une page Web et son contenu ?

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/yabiladi-wayback}
  \caption{Répartition des archives de \textit{yabiladi.com} dans la WayBack Machine (\url{https://web.archive.org/web/*/www.yabiladi.com})}
  \label{fig:yabiladi-wayback}
\end{marginfigure}

Les archives Web sont les traces directes des crawlers (Section \ref{sec:4_derrida}). En DAFF ou en WARC, une page archivée sera toujours adressée par sa seule et unique date de téléchargement. Dans la plupart des moteurs d'exploration (par exemple la WayBack Machine, Figure \ref{fig:yabiladi-wayback}), cette date est l'unique dimension temporelle interrogeable. Il est néanmoins possible d'établir une échelle de datation plus complète en introduisant la notion de date de dernière modification (Section \ref{sec:4_temporalite} et Table \ref{tab:datation_1}). Échelle, dont nous pensons maintenant pouvoir à nouveau améliorer la précision, en associant aux futurs fragments Web une \textbf{date d'édition}. 

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/yabi-date-com}
  \caption{Date d'éditions (rouge) d'un post de forum sur \textit{yabiladi.com}}
  \label{fig:yabi-date-com}
\end{marginfigure}  

Une page Web évolue (Section \ref{sec:3_constituer}) dès que son contenu est édité par un tiers : humain ou robot. Par \textit{édition}, nous entendons ici la création, la modification ou la suppression d'un élément d'une page. Comme les actes de modification et de suppression demandent, pour être datés (même approximativement), de comparer deux versions archivées d'une même page \citep{rocco_page_2003, nunes_using_2007}, leur détection semble de prime abord compliquée à intégrer à notre moteur d'exploration. La création d'un message ou d'un commentaire peut en revanche être plus facilement datée. Des indices sont souvent dispersés à même la page (Figure \ref{fig:yabi-date-com}), reste alors à les interpréter et à les formater avant indexation \citep{de_jong_temporal_2005,kanhabua_using_2009}. Si l'en-tête HTTP d'une page Web a été archivé, celui-ci peut nous renseigner sur une date de dernière modification qui ne dépende pas directement du crawler  \citep{amitay_trend_2004}. À défaut, la création d'un contenu donné sera rapportée à sa première apparition sur l'ensemble des versions archivées d'une même page \citep{jatowt_detecting_2007}, cette comparaison peut être affinée si des URIs ont été par ailleurs collectées\footnote{Le système Memento propose de voir une page archivée comme la concaténation de toutes les URIs qu'elle agrège. Cette vue est appelée TimeMaps  \citep{van_de_sompel_http_2013} et peut être exploitée pour comparer les dates de certaines URIs d'images par exemple.} \citep{aturban_difficulties_2017}. Notons enfin qu'il existe des stratégies de datation adaptées à la nature interdépendante de certains contenus archivés, comme un réseau de citation d'articles de blogs par exemple \citep{toyoda_whats_2006,spitz_predicting_2018}. Quoi qu'il en soit, l'identification et l'extraction d'une telle date d'édition reste possible et nous nous y emploierons en section \ref{sec:5_scraping}. \\

\noindent Mais dès à présent, faisons par avance l'hypothèse d'être en capacité de doter chaque fragment Web d'une date d'édition. Ainsi, à tout fragment $\{f_{j1},...,f_{jm}\}$ d'une page $p_j$ nous associons maintenant une date d'édition $\phi(f_{j1}),...,\phi(f_{jm})$. De plus, nous nommons \textbf{date de création} de tout la page $p_j$ la plus ancienne date d'édition de l'ensemble de ses fragments telle que $\min\limits_{k} \phi(f_{jk})$. La figure \ref{fig:edition_creation} décrit l'imbrication de ces nouvelles datations. 

\begin{figure*}
  \centering
  \includegraphics[width=\linewidth]{graphics/edition_creation}
  \caption{Dates d'édition des fragments Web $\{f_{11}, f_{12}\}$ et date de création de la page $p_1$}
  \label{fig:edition_creation}
\end{figure*} 

\noindent Ainsi, nous pouvons mettre à jour de notre échelle de datation en y ajoutant dates de création et d'édition, telles que :

\begin{table}
\hspace{2em}%
  \label{tab:datation_2}
  \begin{tabular}{lll}
    \toprule
    Niveau & Nature de la date &\\
    \midrule
    page&lancement du crawl & \tikzmark{start}\\    
    page&téléchargement &\\
    page&dernière modification &\\
    page&création & \\
    fragment&édition & \tikzmark{end}\\     
  \bottomrule
\end{tabular}
  \bigskip
  \caption{Échelle (actualisée) de datation d'une page Web archivée}
\end{table} 

\begin{tikzpicture}[overlay,remember picture]
\draw[->] let \p1=(start), \p2=(end) in ($(\x1,\y1)+(0.8,0.2)$) -- node[label=right:précision historique] {} ($(\x1,\y2)+(0.8,0)$);
\end{tikzpicture}

\noindent En pratique, tout fragment Web devra être associé à une date d'édition. Dans le cas contraire, sa datation sera rapportée à la date de création de la page Web à laquelle il appartient. Et si bien dater une page archivée participe de son émancipation vis à vis du crawler, cela donne, par la même occasion, corps aux acteurs qui l'ont fait vivre. 

Un article de blog ne s'écrit pas de lui même, il est le fruit du geste d'un auteur (unique ou collectif, humain ou robot) qui l'a mis en ligne. Derrière les dates d'édition des fragments Web, peuvent transparaitre les gestes de divers auteurs : blogueurs, commentateurs ou contributeurs qui deviennent dès lors objets ou dimensions possibles d'une exploration d'archives Web\footnote{Pour le philosophe V. Flusser les gestes sont des séries de mouvements significatifs dont le but est déchiffrable, ils "\textit{montrent la façon dont nous sommes au monde}", \citep[p.319]{flusser_les_2014}}. Serait-il alors possible, comme le suggère l'historien J. Morsel, d'écrire une histoire \textit{symptomale}\footnote{Alors que la trace, telle que nous la décrivions jusqu'ici (Section \ref{sec:4_derrida}), suggère l'absence de l'agent qui l'a produite (elle s'en est détachée), le symptome, selon Morsel, suppose la présence latente de l'agent, coprésent à ce dont il est le signe \citep{morsel_traces?_2016}} \citep{morsel_traces?_2016} à partir de nos corpus d'archives Web ? Cela reviendrait à considérer que certains fragments Web se trouvent chargés de la présence l'attente d'un auteur, dissimulée sous la surface des pages archivées et prête à être questionnée. Cette nouvelle perspective d'exploration nous mènera à considérer, depuis les archives Web, le devenir de communautés d'utilisateurs ou de collectifs d'auteurs tel que nous l'illustrerons dans le Chapitre \ref{chap:6}. Avec le fragment Web, une nouvelle dimension d'analyse des archives s'offre donc à nous : l'exploration par acteur (auteur, contributeur, commentateur, etc), plutôt que la simple exploration par page ou site.

\subsection{Désagréger pour changer de temporalité}

\noindent Le Web est un flot grandissant d'information tout autant qu'un territoire en perpétuelle expansion. Par l'action des crawlers, les archives Web sont arrachées à la temporalité continue du Web vivant pour rejoindre celle figée et discrétisée des corpus collectés (Section \ref{sec:4_derrida}). Les archives, malheureusement, ne peuvent revenir au temps du Web vivant, mais grâce au fragment Web, nous pouvons les faire basculer dans la \textbf{temporalité du Web tel qu'il a été}. Temporalité que nous allons essayer de caractériser ci dessous.  

Commençons par une expérience. Au cours de la section \ref{sec:4_legacy}, nous avions visualisé, dans le temps, la répartition des pages archivées de la section forum du site \textit{yabiladi.com}, et ce, par rapport à leurs seules dates de téléchargement. Maintenant, essayons plutôt de nous focaliser sur les dates d'édition des fragments Web de chacune de ces pages et tentons une comparaison. 

Faute de ne pas avoir encore définit clairement la nature d'un fragment Web, nous nous contenterons, à ce niveau du manuscrit, de l'approximation suivante : chaque \textit{post} (ie: message individuel) publié sur le forum de \textit{yabiladi.com} sera considéré comme fragment de la page dont il dépend. Un post est écrit par un unique auteur (identifié comme tel) et associé à une date d'édition (Figure \ref{fig:yabi-date-com}). 

D'un point de vue pratique, nous procédons à une extraction focalisée\footnote{Cette extraction de données n'est donc pas générique, voir la section~\ref{sec:5_scraping} pour une approche plus générale} dans nos archives, afin de ne conserver que les dates d'édition des posts collectés. Dans Spark, nous modifions le moteur en conséquence, les dates d'édition étant identifiées dans le code des pages Web par un nœud HTML unique : 
\begin{quote}
\small
\begin{verbatim}
<div class="com-date">17 Novembre 2009</div>
\end{verbatim}
\end{quote}
Nous construisons ensuite un index dédié dans Solr. Ne reste plus alors qu'à visualiser les deux distributions côte à côte : d'une part la répartition des pages par date de téléchargement (Figure \ref{fig:edition_vs_download}, bleu) et d'autre part la répartition des fragments correspondants par date d'édition (Figure \ref{fig:edition_vs_download}, rouge) :

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/edition_vs_download}
  \caption{Distribution, pour \textit{yabiladi.com}, du nombre de pages et de fragments archivés par jours et suivant leurs dates de téléchargement (bleu) et d'édition (rouge) respectives}
  \label{fig:edition_vs_download}
\end{figure*}

\noindent Tout d'abord, la répartition par date d'édition (rouge) semble \textit{gommer} l'artéfact de crawl précédemment observé sur l'année 2013 (bleu). La distribution des fragments est linéaire et ne souffre d'aucune cécité remarquable. Détachée de l'influence du crawler, elle n'en subit plus les effets. 

Par ailleurs, nos archives Web semblent chargées d'une mémoire plus étendue que celle initialement prévue. Ainsi, partant d'une collecte débutée en Mars 2010, nous voilà maintenant capable de considérer et d'analyser des fragments Web édités 7 années plus tôt, jusqu'en 2003 pour les plus anciens. Les pages archivées contiennent, en elles même, les traces sédimentaires de publications antérieures.

Par l'extraction et l'étude des fragments Web, nous nous donnons les moyens d'un saut dans le passé considérable. Ces fragments sont potentiellement porteurs d'une mémoire préexistante à chaque collecte et dont nous pouvons dater avec précision l'apparition. En désagrégeant les archives Web, en les fragmentant, nous changeons une nouvelle fois de temporalité pour entrer dans le temps du Web tel qu'il a été. Là ou le temps des archives était figé et fait de séries de captures discrètes d'une même page, le temps du Web tel qu'il a été est un temps fragmenté. C'est à dire un temps éclaté, où chaque fragment se voit définit relativement par rapport à lui même.

Nos expérimentations pratiques ne portent que sur les seules dates d'édition, mais dans le temps du Web tel qu'il a été, chaque fragment Web suit sa propre temporalité, détachée de celle des autres. Une ligne allant de son apparition sur le Web (date d'édition) jusqu'à sa possible disparition de la toile. Isolées les unes des autres, c'est à l'explorateur d'archives que revient le rôle de naviguer entre ses lignes de temps éclatées. L'explorateur sélectionne, découpe et assemble des fragments pour construire ce que l'anthropologue T. Ingold nomme un \textbf{trajet}, support de l'exploration à venir : \\ 

\begin{fullwidth}
"\textit{Dans le cas du trajet, en revanche, on s'engage dans une voie qu'on a déjà explorée avec d’autres, ou qui a été explorée par d'autres, en reconstruisant l'itinéraire au fur et à mesure de sa progression.}" --- \citep[p.26]{ingold_breve_2013}\\
\end{fullwidth} 

\noindent Un trajet est fait de détours, de contours et de bifurcations. À mesure qu'il se conçoit, le trajet se développe et s'inscrit dans le temps. Suivant le cours de son analyse, c'est par le montage que le chercheur chemine d'un fragment Web à l'autre, dans le sens et l'ordre qu'il juge pertinent. En conjuguant les multiples lignes de temps il s'affranchit ainsi des formes classiques et linéaires d'accès aux archives Web. Ouvrant la voie à de nouveaux degrés de liberté, les fragments pourront être associés sur la base d'un lien hypertexte partagé, d'une présence sur la même page à un instant donné, d'une filiation commune, etc. Ces trajets entre fragments deviennent sous la plume de J. Bashet des \textbf{lignes processuelles} \citep[p.227]{baschet_defaire_2018}. L'historien cherche, ce faisant, à rompre avec une vision linéaire de l'histoire dont il faudrait faire éclater la continuité : \\

\begin{fullwidth}
"\textit{En effet, il ne s'agit en aucun cas de penser l'Histoire tout entière comme un seul processus unifié, mais de saisir, dans l'histoire, un entrelacement complexe de multiples processus.}" --- \citep[p.227]{baschet_defaire_2018}\\
\end{fullwidth} 

\noindent En suivant le devenir historique de multiples lignes processuelles, l'écri\-ture de l'histoire revient à raisonner autour de \textbf{moments} singuliers où convergent et se croisent temporalités et processus hétérogènes :\\

\begin{fullwidth}
"\textit{Et on proposera plutôt d'explorer diverses manière de penser l'événement - le surgissement, le nouveau, la rupture mais aussi l'imprévu, l'imprévisible, l'improbable - à partir d'une pensée des processus. Ainsi, outre qu'elle peut naître ou disparaître, une ligne processuelle connaît par elle-même  des variations de rythme et des moments singuliers de concentration ou d'expansion des forces à l'œuvre : l'événement tient alors à une étape particulière de maturation ou correspond, peut-être à un seuil d'ébullition ou de cristallisation.}" --- \citep[p.227-228]{baschet_defaire_2018}\\
\end{fullwidth} 

\noindent Avec le passage de la page au fragment, nous basculons d'une unité d'exploration à l'autre. Le fragment Web nous invite à un changement d'échelle temporelle et spatiale dans le rapport que nous entretenons aux archives Web. Situé entre la page et l'élément Web, le fragment peut contenir en lui la trace du Web tel qu'il a été : une mémoire jusqu'ici retenue dans les fichiers archivés. Le chercheur associe alors un à un les fragments qu'il juge pertinents et conduit, chemin faisant, son exploration pour saisir l'histoire du Web et ses cristallisations autour de moments singuliers. 

Dans notre méthodologie, la place du chercheur est donc centrale. C'est lui qui, par ses choix de montage (basés sur sa propre expertise ou sur des indices qu'il aura recueilli en amont) définit les fragments Web à explorer et la manière de les parcourir. Il peut, dans cette tâche, se faire aider de scripts informatiques pour automatiser certains traitements. Le fragment Web doit ainsi être interprétable par une machine : un programme pourra l'analyser, le manipuler, le stocker, etc. Mais le fragment doit aussi rester compréhensible, en lui même, afin d'être étudié par un chercheur (sociologue, historien, ...). Nous discuterons, en section \ref{sec:5_scraping}, de l'implication ou non du chercheur dans le choix même de la forme des fragments Web. Nous donnerons, enfin, dans le chapitre \ref{chap:6}, deux exemples d'explorations désagrégées de nos corpus et basées sur le fragment Web.

\section{Le fragment Web : définition}
\label{sec:5_fragment}

\noindent La définition suivante est intentionnellement générique. Nous souhai\-tons par là, que d'autres chercheurs puissent se saisir après nous du fragment Web. Par ailleurs, la nature des fragments dépendant beaucoup du contexte de l'analyse et de la sensibilité propre à chaque chercheur, soit qu'il voudra une fragmentation plus ou moins englobante, soit qu'il se satisfera d'éléments abstrait, nous ne donnerons pas ici de définition technique précise du fragment. Nous proposerons, dans la section \ref{sec:5_scraping}, notre propre système d'extraction des fragments Web depuis une page archivée, d'autres approches et stratégies peuvent naturellement exister.\\

\begin{itshape}
\noindent Considérant la page web comme unité de consultation de base du World Wide Web, bâtit sur des modalités d'écriture propre au support numérique et constatant que du point de vue de la perception humaine \citep{bernard_criteria_2003, michailidou_visual_2008} une page web est le résultat de l'agencement logique d'éléments sémantiques distincts, alors nous nommons \textbf{fragment Web} un sous ensemble sémantique et syntaxique d'une page Web donnée.

\begin{enumerate}[leftmargin=*]  
\item Il y a une relation d'échelle entre une page Web et ses fragments Web. Ceux-ci peuvent couvrir l'entièreté de la page ou n'être qu'un élément unitaire de cette dernière
\item Un fragment Web est un assemblage cohérent d'éléments textuels, visuels, sonores ou logiciels extraits d'une page Web. Le fragment Web doit ainsi être compréhensible par lui même.
\item Au sein d'une même page Web, deux fragments Web ne peuvent pas se superposer, même partiellement
\item Certains éléments d'un fragment Web peuvent faire l'objet d'une catégorisation lors de l'extraction. Un fragment Web peut ainsi être associé à un titre, à un auteur, à une date d'édition, etc
\item Le fragment Web capture l'ensemble des dispositifs d'écriture ( nœuds HTML, CMS widgets, éditeurs de texte ... ) et de partage ( liens hypertextes, liens de syndications, liens de publications … ) utilisés pour publier son contenu sur le Web
\end{enumerate}
\end{itshape}

\section{Scraping et méthodologie d'extraction}
\label{sec:5_scraping}

\subsection{Extraire de l'information issue d'une page Web}

\noindent On appelle \textbf{scraping} l'ensemble des techniques et méthodes employées pour extraire de l'information depuis une page Web. Un \textbf{scraper} est, de fait, un robot chargé de scraper une page ou un ensemble de pages données. En pratique, un scraper ne travaille jamais au hasard, il est focalisé, c'est à dire qu'il est paramétré pour ne conserver que certaines parties ou éléments distincts d'une page : des noms, des adresses, des images, des mots clés, etc. La recherche de nos fragments Web passera obligatoirement par une étape d'extraction, il faudra alors scraper l'ensemble des pages archivées.

De prime abord, les scrapers ne considèrent pas les pages Web telles que nous les voyons depuis nos écrans, interprétées par les navigateurs Web. Lors d'une extraction, les scrapers parcourent, d'abord et avant tout des portions de code issues des fichiers HTML et CSS qui forment l'ossature de ces pages. 

Le \textbf{HTML}\footnote{\textit{HyperText Markup Language}} est un langage de programmation décrivant, à la fois, la structure et le contenu d'une page Web. Le HTML est la grammaire de l'hypertexte. Écrites en HTML, les pages sont alors \textit{rendues} par le navigateur Web qui transcrit visuellement les instructions codées.  

\begin{figure}%
  \includegraphics[width=\linewidth]{graphics/html}
  \caption{Ajout successif d'élément HTML, CSS et JavaScript à une page Web et transcription sur l'écran d'un internaute}
  \label{fig:html}
\end{figure}

\noindent Le HTML est un langage à \textbf{balise}, c'est à dire que chaque élément d'une page Web est délimité par une balise ouvrante à une extrémité (ex: \textit{<p>}) et fermante à l'autre bout (ex: \textit{</p>}). La nature de ces balises, leur syntaxe et leur agencement permettent d'enrichir le contenu textuel de l'élément qu'elles définissent (Figure \ref{fig:html}, (a)). Une balise HTML est identifiée par un \textit{tag} qui donne une indication sur le type de l'élément caractérisé (du texte \textit{<p>}, un lien \textit{<link>}, ...). Elle peut être complétée (entre autres) par un \textit{id} (ie: identifiant unique) et une ou plusieurs \textit{class} (ie: attribut qualifiant un ou plusieurs éléments). Classes et id peuvent servir à associer un comportement spécifique à un ou plusieurs éléments cibles.

Une page Web est un document structuré (Figure \ref{fig:html}, (b)). Les éléments HTML s'agencent entre eux suivant la forme particulière d'un arbre : l'\textbf{arbre DOM}\footnote{\textit{Document Object Model Tree}, en anglais. En informatique un arbre est une structure de données où chaque élément (nœud) est codé hiérarchiquement par rapport aux autres (\url{https://fr.wikipedia.org/wiki/Arbre_binaire})}. Par convention, on appelle \textbf{nœud} HTML tout élément d'un arbre DOM. Il existe ainsi un seul et unique nœud racine, plusieurs nœuds parents, enfants, etc. Un scraper peut accéder à un nœud HTML donné soit en l'adressant directement via son id ou sa classe, soit en parcourant l'arbre DOM.  

Le \textbf{CSS}\footnote{\textit{Cascading Style Sheets}, en anglais} est un langage pensé pour décrire l'aspect visuel (rendu et animation) d'un nœud HTML à l'écran. Le CSS transcrit ainsi des règles de style directement depuis le HTML de la page Web ou dans un fichier dédié. Une correspondance est alors faite entre l'id (et/ou la classe) d'un nœud et la règle à lui appliquer (Figure \ref{fig:html}, (c)). On jouera ainsi sur la couleur, la police d'écriture, la marge, les bordures,~... de chaque éléments.  

Les éléments dynamiques d'une page Web sont générés, majoritairement, par du code \textbf{JavaScript} (Figure \ref{fig:html}, (d)). Le JavaScript peut s'écrire dans un ou plusieurs fichiers séparés ou être directement ajouté au HTML, au sein de balises dédiées. Il est alors possible de manipuler des nœuds HTML et de leur attribuer à chacun comportement. En réaction au geste d'un internaute, par exemple : un clic, un défilement, etc. D'autres langages ont, par le passé, également su gérer ces aspects dynamiques. Le \textbf{PHP}\footnote{\textit{Hypertext Preprocessor}, en anglais} a, ainsi, abondamment été employé tout au long des années 2000. Mais il semble, aujourd'hui, en perte de vitesse\footnote{Voir le dernier rapport de StackOverFlow sur les préférences des développeurs en 2018 (\url{https://insights.stackoverflow.com/survey/2018/})}. Au final, HTML, CSS et JavaScript forment le triptyque le plus courant face auquel doivent se débattre les scrapers.\\

\noindent En effet, un scraper doit savoir s'adapter à la complexité des pages qu'il parcourt. Par exemple, une page contenant du JavaScript devra être préalablement interprétée par le scraper (et non juste parcourue), afin de prendre en compte les éléments dynamiques qui ne se chargeraient qu'à l'affichage écran\footnote{Les librairies Beautifulsoup (python, \url{https://www.crummy.com/software/BeautifulSoup/bs4/doc/}) et After-load (nodeJs, \url{https://www.npmjs.com/package/after-load}) peuvent ici nous aider}. Une campagne de scraping se prépare donc en amont, il s'agira alors de définir une stratégie adaptée au contexte de notre extraction. Le scraper peut, ainsi, s'attarder sur les éléments visuels d'une page Web \citep{cai_vips:_2003}, ou se contenter de la seule information sémantique présente dans le HTML \citep{jatowt_detecting_2007}. Là où la première solution sera en quête d'une vision humaine des pages Web, la seconde privilégiera la recherche d'un temps de réponse réponse acceptable. 

La vitesse des traitements est, de fait, une composante essentielle de tout scraping, notamment dans ses applications industrielles. Faut-il parcourir toute la page ? Attendre qu'elle se charge intégralement~? Ou retourner des résultats partiels ? Ce faisant, les travaux portants sur l'extraction d'information depuis une page Web sont souvent pensés pour s'intégrer, d'abord et avant tout, à de vastes campagnes d'a\-nalyses \citep{weninger_text_2008, adar_web_2009, oita_forest:_2015}. La validation humaine n'est pas la finalité de ces recherches. De notre côté, nous préférons nous rapprocher de méthodes plus qualitatives, conçues sciemment pour être utilisées, en premier lieu, par des êtres humains. 

L'extension Readability\footnote{\url{https://github.com/mozilla/readability}} du navigateur Web Firefox propose, par exemple, aux internautes d'expérimenter une forme de lecture \textit{zen} sur la toile. Le système cherche ainsi à identifier le contenu principal de chaque page (le corps d'un article), à l'extraire et à le présenter dépouillé des publicités, menus et autres suggestions qui pourraient nuire à la lecture. Bien que Readability se limite seulement à certains types de sites  (les sites de news notamment), une partie de son fonctionnement pourra être adaptée à notre propre moteur. 

\subsection{Des pages bruitées à nettoyer}

\noindent Notre tâche consiste ici à fragmenter une page Web donnée. C'est à dire (suivant les termes nouvellement introduits), à extraire de nos fichiers archivés, des sous ensembles cohérents de nœuds HTML. Pour ce faire nous prendrons chaque page une par une, nous la nettoierons, puis, nous définirons une mesure censée traduire la cohérence entre des nœuds HTML deux à deux, avant de les grouper en un ensemble de fragments Web distincts. Soit une page Web $p_1$ composée de $m$ nœuds HTML $\{n_1,...,n_m\}$, organisés en un arbre DOM $t$ et associés à des règles CSS.

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/bruit}
  \caption{Processus de nettoyage, nœud par nœud, d'une page Web}
  \label{fig:bruit}
\end{figure*}

\noindent Nous commençons par \textit{nettoyer}\footnote{Voir la section suivante pour une discussion sur le nettoyage des pages} $p_1$ de tous les nœuds qui ne nous semblent pas pertinents : publicités, menus de navigation, scripts~... D'expérience, nous faisons l'hypothèse que le Web est une source de données très bruitée, tout l'enjeu est ici de trouver un juste milieu entre nettoyage et conservation des données pertinentes. Des méthodes très efficace, par apprentissage supervisé, existent \citep{kohlschutter_boilerplate_2010}, cherchant à définir des masques de nettoyage pour chaque site.  Or comme nous ne connaissons pas, à priori, les évolutions structurelles et stylistiques subies par un site Web au cours de son histoire, la définition d'un set d'apprentissage nous semble ici compromise. 

De notre côté, nous préférons favoriser une approche par heuristiques \citep{jatowt_detecting_2007}. Ainsi, nous définissons pour chaque nœud de $p_1$ un label, résultat de la concaténation de son tag, de son id et de sa classe : 
\[
label = tag + id + classe
\]
\noindent L'idée est ici de vérifier la valeur informative d'un nœud via un ensemble d'expressions régulières appliquées à son label \ref{fig:bruit}. Pour ce faire, nous parcourons l'arbre DOM $t$ et si l'un des labels contient les termes \textit{menu} ou \textit{navbare}, par exemple\footnote{Voir la liste complète dans le code source (l.72-73) : pattern\_avoid, pattern\_remove}, alors celui-ci ne sera pas conservé dans la suite du processus d'extraction. À la fin de cette étape nous ne retenons, à titre illustratif, que 30\% des nœuds des pages forums de \textit{yabiladi.com}, ce taux descend autour de 15\% pour les pages \textit{actualités}, plus bruitée donc. 

\subsection{Clustering de nœuds}

\noindent Transposé au champ d'action de la recherche d'information, un fragment Web peut être vu comme un groupe cohérent de nœuds HTML. Toute la difficulté revient à traduire informatique cette notion de cohérence entre nœuds. Sur ce point, la librairie open-source Fathom\footnote{\url{https://github.com/mozilla/fathom}} a été conçue pour identifier des \textit{clusters} (ie: groupes) de nœuds HTML au sein d'une page Web donnée. L'idée de Fathom est simple : deux nœuds proches l'un de l'autre sont placés dans un même groupe, un nœud proche d'un groupe déjà formé le rejoindra alors, et ainsi de suite. L'algorithme réalise un clustering par agglomérations successives de l'ensemble des nœuds de l'arbre DOM~$t$. 

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/fathom}
  \caption{Processus de nettoyage, nœud par nœud, d'une page Web}
  \label{fig:fathom}
\end{figure*}

\noindent Dans la figure \ref{fig:fathom}, la page $p_1$ contient cinq nœuds HTML identifiés par les tags de leurs balises : $A, B, C, D, E$. Deux nœuds $n_1, n_2$ sont considérés proches l'un de l'autre si la distance $d(n_1,n_2)$ les séparant est inférieure strictement à une variable $minDist$ : la distance minimal autorisant le rapprochement de deux nœuds (ici valant arbitrairement 4). Dans cet exemple\footnote{Les distances sont ici données à titre illustratif, voir la section suivante pour une discussion sur les fonctions de distance}, la distance $d(A,D)$ vaut $1$, $A$ et $D$ ne sont donc pas considérés comme proches. L'ensemble des $m$ nœuds de $p_1$ sont codés, dans Fathom, sous la forme d'une matrice d'adjacence $m \times m$ nommée $M$ (ici de taille 5). À chaque ligne (\textit{row}) de $M$ correspond un nœud et sa distance respective vis à vis des autres nœuds de $p_1$. À chaque itération de Fathom, une fonction \textit{closestRows} détermine les deux nœuds ou groupes de nœuds les plus proches. Dans notre exemple, au premier passage ce sont $A$ et $B$ qui sont considérés les plus proches avec $d(A,B)=1$, puis vient le tour de $C$ et $E$ situés à une distance $2$ l'un de l'autre. À la troisième itération plus aucun groupe ne peut être aggloméré, Fathom a donc identifié trois clusters distincts de nœuds. Le pseudo code, ci dessous, décrit avec plus de détail le fonctionnement de Fathom\footnote{Implémentation originale disponible ici : \url{https://github.com/mozilla/fathom/blob/master/clusters.js\#L156}} :  

\begin{algorithm}
 \While{rows(M) > 1 and closestRows(M) < minDist }{
  $\{r_i,r_j\} = closestRows(M)$\\  
  $newRow = \{ \}$\\
  \For{$r\in rows(M)$}{
   \If{$r \neq r_i \mathrm{ and } r \neq r_j$}{
	$newRow[r] = \min(d(r_i,r),d(r_j,r))$   
   }
  }
  remove($M[r_i]$)\\ 
  remove($M[r_j]$)\\
  remove($M[*][r_i]$)\\
  remove($M[*][r_j]$)\\
  append($M,newRow$)
 }
\end{algorithm}

\noindent Fathom a besoin d'être paramétré pour fonctionner correctement. Il faut ainsi établir en amont la valeur de $minDist$ et définir une fonction de distance qui fasse sens. Dans notre cas nous voulons que cette dernière traduise la cohérence sémantique, syntaxique et visuelle d'un fragment Web. 

\subsection{Définir une fonction de distance}

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/depth}
  \vspace*{0.2cm}  
  \caption{Différence de profondeur entre deux nœuds HTML au sein d'un même arbre DOM. $d(n_1,n_2)=0$ s'ils sont frères. $d(n_1,n_2)=1$ si l'un est le parent de l'autre}
  \label{fig:depth}
\end{marginfigure}

\noindent Une \textbf{fonction de distance} est un objet mathématique qui traduit et calcule l'éloignement entre deux entités, données en variable. Dans notre cas, deux nœuds HTML $n_1,n_2$ seront considérés comme \textit{proches} si le résultat de la fonction de distance $d(n_1,n_2)$ est strictement inférieure à la valeur de $minDist$. Ces nœuds pourront alors être groupés par Fathom et formeront un seul et même fragment Web. 

Par défaut, Fathom réutilise la fonction de distance implémentée dans Readability et designée pour grouper ensemble les nœuds appartenant au contenu principale d'une page Web. Cette fonction s'appuie, d'une part, sur la différence de profondeur entre deux nœuds de l'arbre $t$ (Figure \ref{fig:depth}, $depth(n_i,n_j)$) et, d'autre part, sur le nombre d'éléments les séparant une fois l'arbre $t$ mis à plat ($length(n_i,n_j)$). Un malus est ensuite ajouté si les nœuds ne possèdent pas les mêmes balises HTML ($tag(n_i) \neq tag(n_j)$). Le pseudo code suivant décrit l'intégration de cette fonction de distance\footnote{Voir l'implémentation originale : \url{https://github.com/mozilla/readability/blob/master/Readability.js\#L760}} à Fathom, lors de la création de la matrice d'adjacence $M$ :  

\begin{algorithm} 
 \For{$n_i \in nodes$}{
  \For{$n_j \in nodes$}{
    $d(n_i,n_j) = 0$\\
    $d(n_i,n_j) = d(n_i,n_j) + depth(n_i,n_j)$\\
	$d(n_i,n_j) = d(n_i,n_j) + length(n_i,n_j)$\\    
    \If{$tag(n_i) \neq tag(n_j)$}{
		$d(n_i,n_j) = d(n_i,n_j) + malus$    
    }
    return $d(n_i,n_j)$      
  } 
 }
\end{algorithm}

\noindent En l'état, cette fonction ne traduit qu'une forme de cohérence structurelle (\textit{depth}, \textit{length}), voire sémantique si l'on considère la différence de balises telle quelle. Notre définition du fragment Web se veut plus complète. Nous allons donc enrichir cette fonction de distance en partant d'un principe simple : par défaut deux nœuds sont considérés comme proche, tout motif d'éloignement sera sanctionné d'un malus.

Tout d'abord, la cohérence visuelle jouant un rôle important dans la segmentation d'une page Web donnée\footnote{"\textit{constatant que du point de vue de la perception humaine \citep{bernard_criteria_2003, michailidou_visual_2008} une page web est le résultat de l'agencement logique d'éléments sémantiques distincts}",  section \ref{sec:5_fragment}},nous éloignerons dont la couleur d'arrière plan ($color(n_i) \neq color(n_j)$, issue du CSS) n'est pas la même.

Puis, nous ajouterons un malus aux éléments séparés par des par des \textit{breaking lines} ($vips(n_i,n_j)$). Dans l'algorithme de segmentation Vips \citep{cai_vips:_2003}, les breaking lines sont des nœuds HTML identifiés comme potentiels séparateurs de contenu sur le Web. Ce sont des sauts visuels entre deux portions de texte d'une même page. Les balises \textit{<hr>}, \textit{<br>}, etc sont des breaking lines.   

Enfin, nous savons grâce aux travaux de J. Goody \citep{goody_raison_1979}, que la liste, comme forme d'organisation graphique de l'écriture, est fortement utilisée par l'Homme depuis qu'il a ressentit le besoin de stocker et d'organiser ses données\footnote{Des listes de contage des tablettes sumériennes aux listes de résultats de Google}. La liste abstrait par un jeu de discontinuités (retour à la ligne) et de continuités (récurrences) les données ainsi présentées, permettant un classement de ces dernière suivant de multiples critères. 

Les pages Web, dans leur organisation à l'écran, n'échappent pas à cette règle : listes d'articles, de commentaires, de suggestions, de liens, ... Une manière de traduire la cohérence d'un fragment Web serait d'identifier chaque fragment à un élément d'une de ces listes, quelque soit sa taille. Pour ce faire, nous définissons, par l'observation des \textit{masques de continuité} entre nœuds HTML. La figure \ref{fig:masques} illustre cette intuition. La page $p_1$ peut se voir comme une liste de nouvelles écrites par Borges, chacune présentée par un titre et un phrase faisant office de texte. Nos masques de continuité prennent en compte, d'une part, la nature des nœuds HTML (par une catégorisation de ces derniers\footnote{Voir la section suivante pour une discussion sur la catégorisation des nœuds HTML}) et, d'autre part, une relation de hiérarchie entre ces mêmes nœuds : Le second est il plus profond que le premier ? Sont ils au même niveau~? Nous faisons l'hypothèse, que dans l'arbre DOM $t$ un nœud de profondeur moindre (le titre) subordonnera la réception d'un nœud plus éloigné de la racine (le texte). Ainsi, dans cette exemple nous définissons comme masque l'énoncé suivant : un nœud titre suivit d'un nœud texte plus profond. Si deux nœuds ne valident pas ce masque (ici un texte suivit d'un titre moins profond), un malus leur sera attribué dans la fonction de distance. Cela nous permet, au final, de fragmenter la page $p_1$ comme une liste de deux nouvelles, de deux fragments Web. 

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/masques}
  \caption{Ségmentation d'une page Web suivant des masques de continuité}
  \label{fig:masques}
\end{figure*}

\noindent Nous définissons ainsi plusieurs masques (Figure \ref{fig:coherente-nodes}), supposés englober la majorité des cas de figure rencontrés sur le Web. Une condition ($incoherent(n_i,n_j)$) est ajoutée à notre fonction de distance, qui traduit maintenant une forme de cohérence structurelle, visuelle et syntactique entre deux nœuds HTML. Fathom est maintenant capable de fragmenter les pages Web archivées. Le pseudo code suivant présente l'agencement de l'ensemble de nos malus au sein de la fonction\footnote{Voir l'implémentation détaillée : \url{https://github.com/lobbeque/rivelaine/blob/master/nodejs/cluster.js\#L41}} $d$ :

\newpage

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/coherente-nodes}
  \vspace*{0.2cm}  
  \caption{Masques de continuité entre deux types de nœuds HTML de profondeur variable}
  \label{fig:coherente-nodes}
\end{marginfigure}

\begin{algorithm} 
 \For{$n_i \in nodes$}{
  \For{$n_j \in nodes$}{
    $d(n_i,n_j) = 0$\\
    $d(n_i,n_j) = d(n_i,n_j) + depth(n_i,n_j)$\\
	$d(n_i,n_j) = d(n_i,n_j) + length(n_i,n_j)$\\      
    \If{$tag(n_i) \neq tag(n_j)$}{
		$d(n_i,n_j) = d(n_i,n_j) + malus$    
    }
    \If{$color(n_i) \neq color(n_j)$}{
		$d(n_i,n_j) = d(n_i,n_j) + malus$    
    } 
    \If{$vips(n_i,n_j)$}{
		$d(n_i,n_j) = d(n_i,n_j) + malus$    
    }
    \If{$incoherent(n_i,n_j)$}{
		$d(n_i,n_j) = d(n_i,n_j) + malus$    
    }
    return $d(n_i,n_j)$                        
  } 
 }
\end{algorithm}

\subsection{Catégorisation}

\noindent Pour définir nos masques de cohérence, il nous catégoriser certains nœuds HTML par rapport à la nature de leur contenu. La catégorisation se fait ici au niveau des nœuds et non à l'intérieur du texte de l'élément HTML observé. Ces catégories interviennent, donc, au moment de la segmentation d'une page Web en fragments, mais elles deviendront aussi, à terme, dimension d'interrogation à part entière des archives Web. Pour trouver l'ensemble des fragments Web écrits par un seul et même auteur, il faut, en amont, avoir déterminé les nœuds HTML susceptibles de nous renseigner sur l'identité de cette personne. Nous définissons ainsi six catégories de nœuds :

\begin{enumerate}[leftmargin=*]  
\item Les nœuds titres : titre d'une page, d'un article ...
\item Les nœuds auteurs : auteur d'un post de blog, de forum, ...
\item Les nœuds dates : toute information temporelle 
\item Les nœuds textes : tout élément textuel qui ne soit ni un auteur, ni une date, ni un titre
\item Les nœuds d'expression : nœuds permettant de partager, promouvoir, éditer du contenu en ligne (liens hypertextes, éditeur intégrés, bouton partager, ...)
\item Les nœuds autres : ceux qui n'ont pas pu être catégorisés 
\end{enumerate}

\noindent La catégorisation en elle même se fait sur la base d'expressions régulières et d'heuristiques ad hoc. Nous cherchons, ainsi, à faire correspondre le label\footnote{Pour rappel, le label est la concaténation du tag, de l'id et de la classe d'un nœud } de chaque nœud à un ensemble de termes (balises, classe, etc) caractéristiques d'une catégorie donnée. Par exemple, trouver le terme "\textit{title}" associé à une balise \textit{<h1>}\footnote{Balise HTML définissant un titre} dans le label d'un nœud, nous permettra de le placer dans la famille des titres. Chaque label passe alors au crible de nos expressions régulières\footnote{Voir le détail des patterns recherchés : \url{https://github.com/lobbeque/rivelaine/blob/master/nodejs/utils.js}}, allant de la plus discriminante à la plus englobante, comme l'illustre la figure \ref{fig:categorie-nodes} : 

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/categorie-nodes}
  \caption{Chaine d'expressions régulières permettant la catégorisation d'un nœud HTML sur la base de son label}
  \label{fig:categorie-nodes}
\end{figure*}

\noindent Au sortir de cette étape, nous souhaitons normaliser les nœuds dates. En effet, ces nœuds renferment les dates d'édition que nous cherchons à extraire depuis le début de ce manuscrit. Mais il y a ici une différence importante à faire entre une date encapsulée dans un nœud date et une date trouvée dans le corps d'un nœud texte ou titre. 

La première peut être vue comme une information extradiégétique à la page et à son contenu. Elle caractérise par exemple une date de publication, d'écriture d'un commentaire ou de réponse à un article\footnote{Ces nœuds dates sont souvent crées automatiquement par les CMS et autres éditeurs de sites}. La seconde, en revanche, peut faire référence à n'importe quel aspect du contenu textuel des pages analysées : la date d'un événement narré dans un article, la date d'une référence bibliographique, etc. La structure même du HTML ne nous renseigne pas sur le contexte de ces dates. Pour nous, elles ne sont pas directement liées à un geste d'édition en ligne. Ces dates pourront, par contre, faire l'objet d'une analyse ultérieure.

Ainsi, nos nœuds dates passent finalement au travers d'une fonction de normalisation, qui sur la base d'un dernier jeu d'expressions régulières\footnote{Voir le détail des patterns : \url{https://github.com/lobbeque/rivelaine/blob/master/scala/src/main/scala/qlobbe/Patterns.scala}}, transforme une date écrite en langage naturelle en une date formatée\footnote{ISO 8601 Time zone} et compréhensible par notre moteur de recherche Solr.

\subsection{Discussions}

\noindent Depuis le début de cette section, nous suivons le parcourt d'une page Web $p_1$ d'une étape à l'autre de son processus de segmentation. La page est d'abord nettoyée, pour ne conserver que les nœuds à haute valeur informative. Les éléments HTML restants sont ensuite catégorisés suivant la nature de chacun. Grâce à l'algorithme de clustering Fathom, associé à une fonction de distance enrichie, nous groupons les nœuds entre eux et segmentons alors $p_1$ en une suite de fragments Web distincts. Nous appelons \textbf{Rivelaine} notre librairie d'extraction des fragments Web\footnote{Open-source et téléchargeable ici \url{https://github.com/lobbeque/rivelaine/tree/master/scala}}.

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/rivelaine}
  \caption{Fragment Web de la page \url{https://www.yabiladi.com/forum/semoule-fine-pour-couscous-54-9290786.html}, tel que retourné par Rivelaine}
  \label{fig:rivelaine}
\end{figure*}

\noindent Une première implémentation de Rivelaine est développée en Scala et intégrée à notre moteur d'exploration, au niveau de l'analyse des archives par Spark\footnote{Voir \url{https://www.scala-lang.org/}} (Section \ref{sec:4_moteur}). Plus précisément, c'est une fois la jointure réalisée entre données et méta données DAFF, que nous procédons à la segmentation des pages archivées. Les fragments Web sont ensuite envoyés à notre moteur de recherche qui les place dans un index dédié. En sortie de Rivelaine, les fragments Web sont retournés suivant le format\footnote{\textit{JavaScript Object Notation}, format de données textuelles (\url{https://fr.wikipedia.org/wiki/JavaScript\_Object\_Notation})} JSON (Figure \ref{fig:rivelaine}), de nombreux champs sont exploitables.

Le champ \textit{node} renferme l'ensemble des nœuds HTML d'un fragment Web. Le contenu textuel est, quant à lui, présenté dans le champ \textit{text}. Le fragment de la figure \ref{fig:rivelaine} est constitué d'un auteur, d'une date, d'un texte et d'un nœud autre (champ \textit{type}), identifiés grâce à leurs labels (champ \textit{label}). Deux liens hypertextes ont été extraits (champ \textit{href}) et nous savons que ce fragment est le quatrième a avoir été identifié sur cette page (champ \textit{offset}). Enfin, ce fragment ne représente que $0.008\%$ de l'ensemble du HTML de la page (champ \textit{ratio}). Au regard du texte et de notre connaissance des pages de \textit{yabiladi.com}, ce fragment semble se rapprocher d'un message posté sur le forum du portail marocain (Figure \ref{fig:semoule}). \\

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/semoule}
  \vspace*{0.2cm}  
  \caption{Fragment Web de la page \url{https://www.yabiladi.com/forum/semoule-fine-pour-couscous-54-9290786.html}, tel qu'affiché à l'écran}
  \label{fig:semoule}
\end{marginfigure}

\noindent À mesure que se construisait Rivelaine, un changement de stratégie s'est opéré. Il s'agissait au début de définir une méthode de fragmentation universelle, capable de s'adapter à tous types de pages et à tous les âges du Web. Mais force est de constater, que les heuristiques, non génériques, ont pris une place grandissante dans l'architecture de Rivelaine. L'automatisme, premièrement visé, s'est effacé au profit d'un agencement compliqué de \textit{bricolages} par heuristiques et d'ajustements manuels. En effet, Fathom (pour ne citer que cet élément), est une méthode infiniment paramétrable. La forme d'un fragment Web, dé\-pend ainsi de la valeur de $minDist$ ou du rapport de grandeur entre les divers malus de la fonction de distance. Doit on favoriser la cohérence visuelle ? Structurelle ? Sémantique ? Au détriment d'une autre ? 

Le Web est ainsi fait qu'il reste, pour tout chercheur, pour tout explorateur d'archives, un terrain d'étude non trivial. Le Web est vaste et sauvage. Les sites ne se ressembleront jamais, certains seront \textit{mieux} construits que d'autres et aucune méthode de fragmentation ne pourra satisfaire l'ensemble des chercheurs. Ainsi, lors d'une présentation de Rivelaine\footnote{Lobbé Q., 2017, \textit{Workshop : Introducing Web Fragments, Computational tools for the social study of Web archives}, Open University Of Israel, Tel Aviv}, l'historien Y. Scioldo-Zürcher suggéra d'ajouter plus de contexte aux fragments Web, d'élargir la segmentation des pages. La forme d'un fragment est, au final, dépendante de la question de recherche qui nous motive et de ce que l'on aimerait trouver dans les archives Web.

Sur les conseils de T. Drugeon, il a été décidé de replacer les cher\-cheurs au cœur de la définition des fragments Web. Pour ce faire, une seconde implémentation de Rivelaine a été développée, cette fois en nodeJs\footnote{\url{https://github.com/lobbeque/rivelaine/tree/master/nodejs}}, afin de la rendre autonome et interrogeable en tant que Web service. Nous construisons, par dessus cette application, un \textit{addon}\footnote{Voir \url{https://github.com/lobbeque/rivelaine/tree/master/addon}, les addon ne sont malheureusement plus compatibles avec les version récentes de Firefox} (ie: une extension) au navigateur Firefox permettant de tester, sur le Web vivant, la fragmentation d'une page affichée à l'écran. Il est ainsi possible de jouer directement sur les réglages de Fathom et de sa fonction de distance pour apprécier la forme des futurs fragments Web. Une fois le chercheur satisfait, le paramétrage de Rivelaine est ajouté à la configuration du moteur d'exploration des archives et l'extraction peut débuter.     

La fragmentation des archives Web, nous invite à une forme de souplesse et d'agilité vis à vis des données qui ne doit pas être le seul fait des outils que nous développons. Toute notre méthodologie d'exploration doit pouvoir débrayer, au besoin, d'un traitement large et automatisé des archives vers une analyse plus focalisée où une grande part du travail se fera à la main. Au cas par cas. Il en va ainsi de toute étude sur le Web, soit une alliance à dimensions variables, entre le chercheur, l'algorithme, l'heuristique, le moteur, ...  

Ainsi, Rivelaine et la stratégie d'extraction qu'elle renferme, ne doi\-vent pas être vues comme la seule et unique manière de construire des fragments Web. Rivelaine n'est qu'une possibilité, parmi d'autres qui, nous l'espérons, arriveront bientôt. Ce que nous défendons, dans ce manuscrit, est la fragmentation des archives Web comme principe d'exploration et non la forme particulière de tel ou tel fragment. Il nous faudra, en revanche, être très clair, dans le Chapitre \ref{chap:6}, sur la définition de nos espaces d'explorations à venir.    

\section{Penser une exploration désagrégée}
\label{sec:desagreger}

\noindent Voyons, maintenant, comment les fragments Web peuvent nous aider à améliorer certains biais d'analyse identifiés en section \ref{sec:4_temporalite}. Nous nous plaçons dans l'hypothèse d'une exploration désagrégée d'un corpus d'archives Web. Nous rappelons qu'un site Web archivé se compose de \textit{n} pages Web numérotées $\{p_1$,...,$p_n\}$. Mais désormais, une page $p_j$ consiste elle même en \textit{m} fragments Web numérotés $\{f_{j1},...,f_{jm}\}$. Nous supposons connaitre et avoir identifié la date d'édition de chacun de ces fragments $\phi(f_{j1}),...,\phi(f_{jm})$.

\subsection{Atténuer les cécités de crawl}

\noindent En désagrégeant les archives Web, nous faisons l'hypothèse qu'une date d'édition sera toujours antérieure ou égale à une date de téléchargement. C'est une hypothèse plus ou moins forte et qui tient essentiellement à la nature des sites explorés. Une date d'édition peut, en effet, être falsifiée si elle est directement écrite par un humain. Mais, pour un site comme \textit{yabiladi.com}, les dates d'édition, extraites de la partie forum, sont à la base automatiquement générées par le CMS Phorum. Sur l'ensemble de l'e-Diasporas marocaine, nous décomptons plus de $55\%$ de sites associés à un CMS. Nous pouvons donc avoir raisonnablement confiance en la véracité des dates d'éditions manipulées, ainsi :

\[
	\forall p_j,f_{jk} \exists \phi(f_{jk}) : \phi(f_{jk}) \leq \mu_i(p_j) \leq t_i(p_j)
\] 
\[
	\textrm{where } c_i \textrm{ is a crawl in which } f_{jk} \textrm{ exists   }
\]

\noindent Dans la section \ref{sec:5_dessous}, nous révélions que, par la désagrégation des archives, il était possible d'accéder à une mémoire antérieure à toute collecte, comme le rappelle la figure \ref{fig:frag_blindness}. 

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/frag_blindness}
  \caption{La fragmentation de la page $p_1$ permet d'accéder à des éléments antérieurs à la date de collecte $t_1(p_1)$}
  \label{fig:frag_blindness}
\end{figure*}

\noindent Mais, essayons maintenant de quantifier ce potentiel gain de mémoire. Quelle peut être la différence, en jours, entre une date de téléchargement et une date d'édition ? 

Pour ce faire, reprenons le cours de l'expérience,  débutée en section \ref{sec:5_dessous}, où nous comparions la distribution (pour \textit{yabiladi.com}) du nombre de pages et de fragments archivés suivant leurs dates de téléchargement et d'édition respectives. Nous sélectionnons les 109,534 pages archivées de la section forum de \textit{yabiladi.com} que nous segmentons\footnote{Malus maximal sur la cohérence visuelle et les masques de continuité, pour qu'à tout post du forum corresponde un fragment Web}, via notre moteur, en 422,906 fragments Web associés à une date d'édition. Nous considérons, ensuite, la plus ancienne date d'édition de chaque page $\min\limits_{k} \phi(f_{jk})$ pour s'approcher au plus près de leurs dates de création (Section \ref{sec:5_dessous}). 

Nous calculons alors la différence $\min\limits_{i} t_i(p_j) - \min\limits_{k} \phi(f_{jk})$  entre dates de création et dates de première collecte. La figure \ref{fig:timeDiffFull} donne à voir le gain en jours pour chaque page archivée. 
\newpage

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/timeDiffFull}
  \caption{La fragmentation de la page $p_1$ permet d'accéder à des éléments antérieurs à la date de collecte $t_1(p_1)$}
  \label{fig:timeDiffFull}
\end{figure*}

\noindent Les quartiles correspondants sont donnés par la table \ref{tab:quartiles_1}. Pour $50\%$ des pages archivées de \textit{yabiladi.com} (Table \ref{tab:quartiles_1}, Q2) le gain est estimé à plus de deux années, la maximum étant de 3131 jours.

\begin{table}
\hspace{2em}%
  \label{tab:quartiles_1}
  \begin{tabular}{ccl}
    \toprule
    quartiles&différence en jours\\
    \midrule
    Q1 & 256\\
    Q2 & 777\\
    Q3 & 1340\\    
    max & 3131\\  
  \bottomrule
\end{tabular}
  \bigskip
  \caption{Quartiles de la différence $\min\limits_{i} t_i(p_j) - \min\limits_{k} \phi(f_{jk})$ en jours}
\end{table} 

\noindent Mais les bénéfices, que nous constatons ici, peuvent être simplement liés à la date de commencement du crawl. En effet, la collecte menée par l'INA a débuté bien après la création de \textit{yabiladi.com}. L'écart entre date de création et date de première collecte ne doit donc pas nous surprendre outre mesure.

Aussi, concentrons nous plutôt sur des moments singuliers de cette campagne de crawl. D'une part, les six premiers mois de crawl qui correspondent à l'initialisation du corpus, où l'on cherche capter des pages potentiellement antérieur de 7 ou 8 ans\footnote{\textit{yabiladi.com} est crée fin 2002 et la campagne de crawl débute, elle, en 2010}. D'autre part, une année dite de routine (2012-2013), où le crawl n'a plus rien à rattraper et doit simplement se contenter d'archiver le pages nouvellement crées. Nous calculons alors, pour chaque moment, la même différence $\min\limits_{i} t_i(p_j) - \min\limits_{k} \phi(f_{jk})$, les résultats sont donnés par la table \ref{tab:quartiles_2}.

\begin{table}
\hspace{2em}%
  \label{tab:quartiles_2}
  \begin{tabular}{ccc}
    \toprule
    quartiles&différence cas n°1&différence cas n°2\\
    \midrule
    Q1 & 428 & 39\\
    Q2 & 875 & 49\\
    Q3 & 1340 & 1229\\
    max & 2628 & 2389\\      
  \bottomrule
\end{tabular}
  \bigskip
  \caption{Quartiles de la différence $\min\limits_{i} t_i(p_j) - \min\limits_{k} \phi(f_{jk})$ en jours, pour les 6 premiers mois de crawl (cas n°1) et les années 2012-2013 (cas n°2)}
\end{table} 

\noindent Dans le premier cas, la différence de dates est, comme prévue, plus importante : celle-ci passant à 2 ans et 4 mois pour $50\%$ des pages archivées (Table \ref{tab:quartiles_2}, cas n°1, Q2). Pour les crawls routiniers, les gains sont bien moins importants et oscillent majoritairement entre un et deux mois (Table \ref{tab:quartiles_2}, cas n°2, Q1 et Q2). Fragmenter les archives Web permet ainsi d'atténuer les diverses cécités de crawl. Mais ces bénéfices seront plus marqués dans le cadre d'une collecte déclenchée après la création du site ciblé (notre cas), que lors d'un crawl routinier (le cas d'Internet Archive).    

\subsection{Cohérence relative entre pages}

\noindent Lorsque nous explorions les archives Web au niveau des seules pages (Section \ref{sec:4_temporalite}), nous avions définit la cohérence par observation comme l'existence d'un unique instant $t_{\mathrm{coherence}}$ où se croisent les intervalles d'invariance respectifs des pages considérées. \citep{spaniol_data_2009}. 

Avec le fragment Web, nous pouvons dépasser cette définition et introduire la notion de \textbf{cohérence par observation relative}. Entre deux pages, la cohérence telle que nous la connaissons est absolue, l'entièreté des pages est ainsi considérée. Or, nous nous plaçons maintenant dans le cadre d'une analyse focalisée sur un élément ou un ensemble d'éléments particuliers.

Par exemple, si un chercheur souhaite vérifier la cohérence entre deux articles collectés, il pourrait vouloir connaitre la nature précise de l'in\-tervalle d'invariance. À ses yeux, la cohérence serait hors sujet ou abusive, si le seul élément d'invariance entre les deux pages se révèlait être une barre de navigation plutôt que le corps des articles. Comment, dès lors, considérer la cohérence relativement à une question de recherche donnée ? 

Sur ce point, nous définissons un sous ensemble discret de fragments d'intérêt $\{f^*_{j1},...,f^*_{jl}\}$ (avec $l \leq m$). Le chercheur sélectionne lui même les fragments Web qui lui semblent pertinents pour son analyse et pour vérifier la cohérence, ainsi :

\newpage

\[
	\forall p_j, \exists f^*_{jk} \in \{f_{j1},...,f_{jm}\}, \exists t^*_{\mathrm{coherence}}:
\] 
\[
	t^*_{\mathrm{coherence}} \in \bigcap^n_{j=1}[\phi(f^*_{jk}),t_i(p_j)] \neq \emptyset
\] 

\noindent En désagrégeant les archives Web, le chercheur peut reprendre la main sur les analyses qu'il entend mener au cœur des archives. Il peut focaliser, à souhait, son expérience d'explo\-ration et déplacer son point d'observation relativement à son sujet. La figure \ref{fig:frag_coherence} décrit, de manière graphique, la différence entre cohérence par observation absolue et cohérence par observation relative pour deux pages $p_1, p_2$ et deux fragments Web choisis $f^*_{11}, f^*_{21}$. 

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/frag_coherence}
  \caption{Cohérence par observation absolue et cohérence par observation relative}
  \label{fig:frag_coherence}
\end{figure*}

\noindent Nous n'avons pas eu ici l'occasion de proposer une expérimentation pratique de la cohérence par observation relative. Mais nous pensons qu'il est tout a fait faisable de l'intégrer à un système d'exploration des archives Web et encourageons la mise en place future de travaux voulant traiter ce sujet.  

\subsection{Dédupliquer les corpus}

\noindent En section \ref{sec:4_temporalite}, nous nous sommes inquiétés de la possibilité de voir plusieurs fois le même contenu archivé dans un corpus. Cela peut provoquer des biais d'analyse, notamment si l'exploration est basée sur une recherche plein texte. 

Mais par la désagrégation des archives, il est possible de dédupliquer des éléments qui auraient été re-collectés d'un crawl à l'autre. En utilisant le fragment Web comme unité d'exploration, nous pouvons définir une \textbf{fonction d'identité} nommée $id$. Cette fonction compare l'invariance dans le temps, d'un fragment $f_{jk}$ extrait d'une page $p_j$, au cours de deux crawls consécutifs $c_1$ et $c_2$ à $t_1(p_j)$ et $t_2(p_j)$ tel que :    
\[
	id(t_1(f_{jk})) = t_2(f_{jk})
\]
\noindent La figure \ref{fig:frag_doublon} illustre cette idée. D'un point de vue technique, la fonction d'identité ne peut être mise en place qu'au moment de retourner les résultats d'une requête depuis le moteur de recherche (Section \ref{sec:4_moteur}). En effet, cela serait trop couteux de maintenir, lors de l'extraction des fichiers DAFF, un index dynamique des fragments Web déjà identifiés. Nous laissons donc, à dessein, des doublons dans les indexes de Solr.  C'est lors de la restitution de ces résultats que nous décidons de les grouper via la fonction d'identité.  

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/frag_doublon}
  \caption{Dédupliquer les archives Web grâce à une fonction d'identité}
  \label{fig:frag_doublon}
\end{figure*}

\noindent Pour ce faire, nous associons à chaque fragment Web un champ unique\footnote{Voir la section suivante pour une discussion sur le schéma de l'indexation des fragments Web} appelé \textit{frag\_text\_id}. Ce champ est le résultat du passage de l'ensemble du contenu textuel du fragment Web dans une fonction de hachage\footnote{Fonction déjà évoquée en section \ref{sec:3_constituer} pour les identifiants des fichiers DAFF} SHA-256. Nous utilisons la fonction \textit{group by}\footnote{Voir \url{https://lucene.apache.org/solr/guide/6_6/result-grouping.html}} de Solr pour grouper les fragments potentiellement dupliqués par \textit{frag\_text\_id} unique. \\

\noindent Dans notre modèle d'exploration désagrégé, une page Web archivée et observée à un instant $t$ donné, ne sera plus que le résultat de l'assem\-blage de fragments Web précédemment publiés. La page Web, en tant que telle, disparait de notre modèle de données. 

\section{Intégration au moteur d'exploration}
\label{sec:retour_au_moteur}

\noindent Notre moteur d'exploration d'archives Web, tel que nous l'avions décrit en section \ref{sec:4_moteur}, prend la page Web comme unité principale d'exploration. Mais depuis, nous avons opéré un changement analytique de la page vers le fragment Web. Expliquons maintenant comment intégrer le fragment à notre moteur et proposons un premier cas d'usage, basé sur la détection d'événements dans les archives.

\subsection{D'un schéma à l'autre}

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/rivelaine_moteur}
  \vspace*{0.2cm}  
  \caption{Intégration de la fragmentation aux restes des traitements Spark (Voir Figure \ref{fig:spark})}
  \label{fig:rivelaine_moteur}
\end{marginfigure}

\noindent D'un point de vue pratique, l'extraction des fragments s'intègre à l'ensemble des traitements supervisés par Spark (Section \ref{sec:5_scraping}). Une fois la jointure effectuée entre méta données et données, notre moteur demande à Rivelaine de segmenter les pages archivées avant de les envoyer dans Solr pour indexation (Figure \ref{fig:rivelaine_moteur}).

Un nouvel index doit alors être créé pour accueillir les fragments, le premier étant pensé pour les pages Web uniquement. Deux stratégies s'offrent ici à nous. Tout d'abord, conserver la page comme élément de référence, à laquelle nous subordonnons les fragments (Figure \ref{fig:schema_vs}, (a)). Ou éliminer la notion même de page et n'indexer que les fragments (Figure \ref{fig:schema_vs}, (b)). 

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/schema_vs}
  \caption{Différentes stratégies d'indexation du fragment Web dans un moteur de recherche et complexité de la recherche (bleu)}
  \label{fig:schema_vs}
\end{figure*} 

\noindent La première option, plus intuitive, conserve le lien ontologique entre la page et ses fragments Web. Elle nécessite la mise en place dans Solr, d'une structure dite de \textit{nested documents}\footnote{\url{http://yonik.com/solr-nested-objects/}}. Dans un même index, deux types de documents cohabiteraient, un document page et des documents fragments. Malheureusement, cette stratégie est conteuse, notamment lorsqu'il s'agit d'identifier et de retourner des résultats. En effet, dans les moteurs de recherche, il est toujours conseillé (surtout lorsque le nombre de document est important) de dupliquer au besoin les documents indexés. L'espace disque occupé par l'index sera plus important, mais les performance du \textit{search}, en tant que tel, seront améliorées, la complexité d'une recherche par documents à plat étant moindre que celle par documents subordonnés pour laquelle toute la structure doit être retournée (Figure  \ref{fig:schema_vs}, tracés bleus). 

Nous nous orientons donc vers la seconde option, qui a pour effet de dupliquer les informations associées à une page à l'intérieur de chaque document fragment. Si l'utilisateur veut, au besoin retrouver l'ensemble des fragments d'une même page, il pourra s'orienter vers une requête \textit{group by} sur le \textit{page\_id} dans Solr. 

Soit le schéma d'indexation des fragments Web présenté par la figure \ref{fig:schema_2}. Dans ce schéma, l'\textit{id} de chaque document correspond à l'identifiant unique d'un fragment. Les champs issus de Rivelaine (\textit{frag\_type}, \textit{frag\_offset}, ...) sont intégrés à l'index et la recherche plein texte est maintenant réalisée sur le seul champ \textit{frag\_text}. Pour retrouver l'ensemble des fragments d'une même page, on se servira du champ \textit{page\_url\_id} et pour dédupliquer les fragments (Section \ref{sec:desagreger}), on s'appuiera sur la valeur de \textit{frag\_text\_id}\footnote{Clé SHA-256 unique}. Les différents niveaux de notre échelle de datation (Table \ref{tab:datation_2}) sont indexés, le champ \textit{page\_date} correspondant à la date de création d'une page (Section \ref{sec:5_dessous}). Le champ \textit{frag\_date} est, lui, supposé contenir les dates d'édition de chaque fragment. Néanmoins, si nous sommes dans l'impossibilité d'associer une date d'édition à un fragment, le champ \textit{frag\_date} se verra attribuer la valeur de \textit{page\_date}, voire de la date de téléchargement \textit{download\_date} dans le pire des cas. Sur ce point, le type \textit{dateLvl} nous renseigne sur le niveau de précision alloué au champ \textit{frag\_date}.

\begin{figure*}
\small
\begin{verbatim}
<field name="id"                type="string"  indexed="true"    multiValued="false" required="true" />

<!-- archive fields -->
<field name="archive_active"    type="boolean" indexed="true"    multiValued="false"/>
<field name="archive_corpus"    type="string"  indexed="true"    docValues="true" multiValued="false"/>
<field name="archive_country"   type="string"  indexed="true"    docValues="true" multiValued="false"/>
<field name="archive_lang"      type="double"  indexed="true"    docValues="true" multiValued="false"/>
<field name="archive_mime"      type="string"  indexed="true"    docValues="true" multiValued="false"/>  

<!-- crawl fields -->
<field name="crawl_id"          type="string"  indexed="true"    docValues="true" multiValued="true" />
<field name="crawl_id_f"        type="string"  indexed="true"    docValues="true" multiValued="false"/>
<field name="crawl_id_l"        type="string"  indexed="true"    docValues="true" multiValued="false"/>
<field name="crawl_date"        type="date"    indexed="true"    docValues="true" multiValued="true" />
<field name="crawl_date_f"      type="date"    indexed="true"    docValues="true" multiValued="fasle"/>
<field name="crawl_date_l"      type="date"    indexed="true"    docValues="true" multiValued="true" />

<!-- download fields -->
<field name="download_date"     type="date"    indexed="true"    docValues="true" multiValued="true" />
<field name="download_date_f"   type="date"    indexed="true"    docValues="true" multiValued="false"/>
<field name="download_date_l"   type="date"    indexed="true"    docValues="true" multiValued="false"/> 
\end{verbatim} 
\end{figure*}

\newpage

\begin{figure*}
\small
\begin{verbatim}
<!-- page fields -->
<field name="page_domain"       type="string"  indexed="true"    docValues="true" multiValued="false"/>
<field name="page_url"          type="string"  indexed="true"    docValues="true" multiValued="false"/>
<field name="page_url_id"       type="string"  indexed="true"    docValues="true" multiValued="false"/>      
<field name="page_link"         type="string"  indexed="true"    docValues="true"  multiValued="true"/>      
<field name="page_title"        type="text"    indexed="true"    docValues="false" multiValued="false"/>
<field name="page_date"         type="date"    indexed="true"    docValues="true"  multiValued="true" />

<!-- fragment fields -->
<field name="frag_type"         type="string"  indexed="true"    docValues="true"  multiValued="true" />    
<field name="frag_author"       type="string"  indexed="true"    docValues="false" multiValued="true" />
<field name="frag_date"         type="date"    indexed="true"    docValues="true"  multiValued="true" />
<field name="frag_date_level"   type="dateLvl" indexed="true"    docValues="true"  multiValued="false"/> 
<field name="frag_href"         type="string"  indexed="true"    docValues="false" multiValued="true" />
<field name="frag_label"        type="string"  indexed="true"    docValues="true"  multiValued="true" />
<field name="frag_ratio"        type="int"     indexed="true"    docValues="true"  multiValued="true" />
<field name="frag_node"         type="text"    indexed="false"   docValues="false" multiValued="true" />  
<field name="frag_offset"       type="int"     indexed="true"    docValues="true"  multiValued="true" />
<field name="frag_text_id"      type="string"  indexed="true"    docValues="true"  multiValued="false"/>

<!-- searchable fragment fields -->
<field name="frag_text"         type="text"    indexed="true"    stored="false"  multiValued="true" />
<field name="frag_text_shingle" type="shingle" indexed="true"    stored="false"  multiValued="true" />   
\end{verbatim} 
\caption{Schéma d'indexation des fragments Web}
\label{fig:schema_2}
\end{figure*}

\subsection{Détection d'événements}

\noindent Comme cas d'usage pratique du fragment Web, nous souhaitons maintenant ajouter à notre moteur d'exploration un système de détection d'événements dans les archives. Cette proposition a fait l'objet d'une publication démonstration\footnote{Lobbé, Q. (2018), \textit{Revealing Historical Events out of Web Archives}, TPDL 2018} dont l'application est, jusqu'à présent, limitée aux seules archives du site \textit{yabiladi.com}. 

La recherche par événements est une alternative aux méthodes d'ex\-ploration classiques qui se font principalement par URLs (Section \ref{sec:3_constituer}). Des travaux récents tentent également de s'en affranchir en proposant des analyses par catégories \citep{holzmann_tempas:_2016}, par entités nommées \citep{spaniol_tracking_2012} ou basées sur des tendances issues des réseaux sociaux \citep{risse_arcomem_2014}. Dans l'ensemble, tout indique que, face à des corpus d'archives Web si vastes, une exploration dirigée ou guidée (par exemple sur la base d'événements) peut être bénéfique. 

Nous pensons que tout explorateur d'archives (Web ou autre) poursuit à moment donné la recherche d'événements singuliers qu'il puisse mettre au regard de l'histoire \citep{chaney_who_2015}. Ainsi, pour J. Baschet (Section \ref{sec:5_dessous}) l'étude historique de lignes processuelles pas par une pensée de l'évé\-nement comme "\textit{surgissement}" ou "\textit{rupture}" \citep[p.227]{baschet_defaire_2018}.  

Au sein d'une distribution temporelle d'éléments donnés, un événement peut être caractérisé de trois manières différentes comme le propose T. Viard \citep[p.106]{viard_link_2016} : par sa détection, par son identification et par son explication. 

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/event}
  \vspace*{0.2cm}  
  \caption{Détection d'événements à partir d'un threshold}
  \label{fig:event}
\end{marginfigure}

Un événement \textbf{détecté} est une anomalie présente dans une distribution. Il peut s'agir d'un instant singulier ou d'une période entière durant laquelle le niveau de messages postés sur un blog est supérieure à une valeur de référence (une moyenne par exemple). Un événement \textbf{identifié} est un événement pour lequel un élément de causalité aura été trouvé dans les données sources. Un pic d'activité dans un forum en ligne est, par exemple, causé par un nombre important de messages postés. Enfin, un événement \textbf{expliqué} est un événement pour lequel une explication aura été trouvée et validée à l'extérieur des données sources. Ici, une expertise et une analyse humaine est nécessaire. Un pic de messages dans un forum peut, ainsi, être expliqué par un contexte social ou politique particulier qui aura fait réagir certains membres de la communauté.  

Continuant sur notre logique exploratoire, nous ne voulons pas ici cibler une forme particulière d'événements, nous éviterons donc les méthodes de détection par pattern \citep{chaney_detecting_2016} ou clustering  \citep{dodds_temporal_2011}. Nous nous orientons plutôt vers une méthode de détection par \textit{threshold} \citep{fung_parameter_2005} à l'intérieur d'une fenêtre glissante d'une semaine\footnote{Sur \textit{yabiladi.com}, la durée de vie moyenne d'un thread de messages est d'un peu plus d'un jour, le choix de prendre la semaine comme granularité nous a donc semblé judicieux}. Nous définissons ici un événement comme une valeur aberrante détectée au sein d'une distribution de fragments Web (Figure \ref{fig:event}).

Pour ce faire, le contenu textuel de chaque fragment Web indexé est divisé en \textbf{bigrams}\footnote{Le champ \textit{frag\_text\_shingle} de notre schéma d'indexation (Figure \ref{fig:schema_2})}. Un bigram est une séquence de deux mots consécutifs extraite d'un même ensemble textuel (Figure \ref{fig:bigram}). 

\begin{figure}%
  \includegraphics[width=\linewidth]{graphics/bigram}
  \caption{Division d'une phrase en bigrams}
  \label{fig:bigram}
\end{figure} 

\noindent Dans le cadre de cette démonstration, la recherche plein texte porte sur le contenu textuel des bigrams. Le moteur ne retourne donc plus que les fragments Web dont certains bigrams auraient matché des mots clés proposés par un chercheur. Dans notre cas, identifier un événement revient, en réalité, à détecter un pic soudain de bigrams dans le temps. Les bigrams sont souvent utilisés pour observer des tendances ou des évolutions lexicales au sein de grands corpus de textes. On citera par exemple, le système \textit{ngrams viewer}, conçu par les équipes de Google books, qui permet de suivre l'évolution temporelle de l'utilisation de certains mots dans leur base de données de livres numérisés \citep{michel_quantitative_2011}.

Pour terminer, nous essayons d'expliquer nos événements en trouvant des corrélations avec certains titres d'articles de news, extraits des archives de la section actualités du site \textit{yabildai.com}. En effet, nous faisant l'hypothèse que les utilisateurs du forum sont susceptibles de réagir, par le biais de messages, à un événement précis de l'actualité (avant ou après que celui-ci ait été rapporté dans la presse). Nous construisons ainsi, à la volée, un nouvel index d'événements potentiels en utilisant le titre et la date d'édition de ces articles archivés. Lorsqu'un pic de messages est détecté, une requête secondaire est automatiquement adressée à notre index d'événements possibles. Si l'un des titres matche également la recherche du chercheur et que sa date d'édition se trouve à moins d'une semaine de la date du pic détecté, alors nous proposons ce titre comme potentielle explication de cette soudaine recrudescence de messages.

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/ben-ali}
  \caption{Ajout de la détection d'événements à notre interface d'exploration}
  \label{fig:ben-ali}
\end{figure*} 

\noindent Notre interface de visualisation, est modifiée en conséquence et permet maintenant au chercheur de choisir la granularité d'explo\-ration qui l'intéresse le plus, entre la page et le fragment (Figure \ref{fig:ben-ali}). Dans la partie fragment Web, apparait un nouvel histogramme affichant la distribution dans le temps des bigrams ayant matchés la requête du chercheur. C'est sur cet histogram que sont affichés les événements détectés et leur possible explication. Dans une vidéo de démonstration\footnote{Consultable ici : \url{https://youtu.be/snW4O-usyTM}}, nous décrivons différents cas d'usages de ce système et, plus généralement, nous y présentons le fonctionnement du moteur d'exploration. Nous donnons ainsi à voir, comme exemple d'événements détectés dans les archives, des pics de discussion autour de diverses actualités liées au roi du Maroc Mohammed VI et des réactions à la destitution de l'ancien dirigeant tunisien Z. Ben-Ali, au début de l'année 2011 (Figure \ref{fig:ben-ali}, tracé vert). Dans le Chapitre \ref{chap:6}, nous reviendrons en détail sur la manière avec laquelle le forum de \textit{yabiladi.com} a réagit à ce moment historique particulier qu'a été le Printemps arabe.

\begin{center}
	\textbf{***}
\end{center}

\noindent Malgré notre idée première, l'extraction des fragments Web n'a pas pu se faire sans une forte dose d'heuristiques non génériques. Le Web est ainsi fait qu'il nous oblige, sans cesse, à ré-adapter l'échelle de nos analyses. Passant de vastes traitements automatisés à des moments de pur travail manuel. Au cas par cas. Nos explorations à venir ne contrediront pas cet état de fait : notre méthode d'exploration chemine dans un entre deux constant entre approche quantitive et validation qualitative. Il nous faut ainsi mettre les mains dans les archives, les ouvrir et y plonger.

C'est, au final, toute l'ambition du fragment Web, tel que nous l'avons introduit dans ce chapitre. Redonner au chercheur les moyens théoriques et techniques d'une plus grande maniabilité des archives. Le terrain théorique ayant été préparé, dans le courant des années 2000, par les travaux pionniers de N. Brügger, il existait un espace analytique à explorer entre l'élément Web et la page Web. Nous définissons ainsi le fragment Web comme un sous ensemble sémantique et syntaxique d'une page Web.

Avec le fragment Web et la grammaire qu'il introduit, le chercheur quitte les interfaces d'exploration vitrines et s'assoit à la table de montage où il peut découper, déplacer et mettre en relation des éléments épars du Web passé. Nous avons ainsi montré comment, en s'appuyant sur les dates d'édition plutôt que sur les seules dates de téléchargement, les archives en arrivent à basculer d'une temporalité à l'autre. Elles quittent le temps des crawlers pour retrouver le temps du Web tel qu'il a été. Le chercheur accède alors à une mémoire antérieur aux collectes des archivistes, mémoire jusqu'ici retenue derrière le verrou des fichiers sauvegardés. Le fragment témoigne ainsi directement du geste des auteurs, lecteurs et bloggers du Web passé et replace, de fait, l'humain au cœurs de l'étude des archives Web.

Intégrés à notre moteur d'exploration, nous nous appuierons sur les fragments Web pour mener, dans le chapitre suivant, deux explorations à travers notre corpus de sites marocains. Nous étudierons l'histoire de collectifs migrants en ligne, depuis longtemps éteints et dont la trace ne subsiste aujourd'hui plus que dans les archives.  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Chapitre 6 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cleardoublepage
\begin{minipage}[t,leftmargin=5em]{1.5\linewidth}%
\begin{adjustwidth}{-0.5cm}{}
\chapter{Exploration de Collectifs Migrants Éteints} 
\label{chap:6}
\end{adjustwidth}
\end{minipage}
\hfill

\noindent Le processus de cartographie de l'e-Diaspora marocaine s'est étalé sur prêt de deux ans, de 2008 à 2010. L'archivage de ces sites s'est alors présenté comme une nécessité, leurs traces commençant déjà à disparaitre de la toile. Dix ans plus tard, en 2018, sur les 153 sites observés à l'époque, seuls 53 peuvent encore être qualifiés de vivants, c'est à dire : toujours en ligne et ayant été mis à jour au moins une fois au cours de la dernière année passée. Les deux autres tiers sont, pour la plupart hors ligne, tombés à l'abandon ou récupérés par des cybersquatteurs\footnote{Technique de hacking consistant à prendre possession d'un nom de domaine existant et de publier à sa place un contenu autre (e-commerce, publicités, arnaques, pornographie, ...)}.      

Le Web est un environnement éphémère (Section \ref{sec:3_constituer}) et les e-Diaspo\-ras ne sont, au final, que des images instantanées d'un espace qui, une fois la capture effectuée, n'est déjà n'est plus tout a fait le même. Il a continué à évoluer et s'est transformé. 

Parmi les nombreux sites du corpus marocain, les plateformes institutionnelles (Section \ref{sec:2_atlas}) sont celles qui ont le mieux résisté au passage du temps. En effet, $50\%$ d'entre elles sont toujours vivantes et leur encrage durable en ligne semble résulter d'une stratégie planifiée. La petite dizaine de sites consulaires, identifiés en 2008, s'est ainsi muée en un seul et même portail Web \textit{consulat.ma}\footnote{Voir \url{http://www.consulat.ma/fr/aide.cfm}, site lancé en 2011 dans le cadre du Plan Maroc Numeric 2013 (\url{http://www.egov.ma/sites/default/files/Maroc\%20Numeric\%202013.pdf})} sous l'impulsion du gouvernement marocain. Puis, derrière elles, viennent les sites des associations et ONG dont $30\%$ sont encore accessibles. Mais, c'est le cas de la blogosphère qui, en ce début de chapitre, nous interpelle le plus : sur les 48 blogs cartographiés, seuls 5 sont aujourd'hui vivants (soit à peine $10\%$).

Au sein de l'e-Diaspora marocaine, la blogopshère avait été caractérisée\footnote{Voir l'intervention de M. Renault (\url{https://www.youtube.com/watch?v=1sE5PZVG6iM})} comme une communauté densément liée entre chacun de ses membres. Soit un espace d'expression et de représentation où circulaient, tout autant, commentaires politiques que témoignages de la vie quotidienne des marocains de l'étranger. Pourquoi ce graoupe de blogs s'est-il à ce point réduit perdant, au cours du temps, son influence et son dynamisme ? Les blogueurs ont-ils simplement disparu et quitté le Web ? Ou ont-ils migré vers d'autres territoires du Web ? Peut-on remonter le fil des potentielles traces archivées de cette cybermigration et en déterminer la destination ?

Au cours de ce chapitre, nous mènerons deux explorations successives, proposant chacune une utilisation différente des fragments Web. La première nous permettra de comprendre la mutation progressive des blogs marocains vers les plateformes de réseaux sociaux à la fin des années 2010. Cette transformation, nous le découvrirons, est autant le fait d'une évolution technologique inhérente au Web, que le résultat de l'influence d'événements socio-politiques extérieurs à la toile. Parmi ces événements, le printemps Arabe et, plus particulièrement, la manifestation marocaine du 20 février 2011 semblent avoir été des moments déterminants de l'histoire de la blogosphère. 

En suivant cette nouvelle piste, nous questionnerons la réception de ce même événement par les membres d'un autre collectif en ligne, central dans la vie de l'e-Diaspora marocaine : le forum de \textit{yabiladi.com}. Nous comprendrons, la manière avec laquelle les membres de ce site se sont momentanément organisés et retrouvés emportés par la vague révolutionnaire qui déferlait alors sur le Maghreb et le Moyen Orient. 

Comme un bilan de ces deux explorations nous introduirons, pour terminer, la notion de \textbf{moments pivots du Web} : des instants particuliers de l'histoire de la toile où, celle ci, change soudainement de direction par la rencontre d'une avancée technologique et d'un groupe d'utilisateurs capables de s'en saisir.

Mais, avant de débuter notre analyse, nous souhaitons revenir sur les aspects théoriques de ce que nous nommons \textbf{exploration} depuis le début de ce manuscrit. Il s'agit pour nous de présenter comment, ce courant des statistiques qu'est l'analyse exploratoire de données (AED), peut aujourd'hui servir de cadre méthodologique à toute re\-cherche portée sur le Web passé\footnote{Ce chapitre a fait l'objet d'une publication (en soumission) : Q. Lobbé, (2018), \textit{Where the dead blogs are}}.\\

\section{À la recherche de l'étonnement}
\label{sec:6_eda}

\noindent Dans le courant du 19e siècle, la théorie statistique prend un tournant rigoriste majeur en préférant, à la longue tradition exploratoire\footnote{Les statisticien ont longtemps été des explorateurs de données, palliant certaines lacune théorique, par l'utilisation d'outils de recueil des données simples et visuels, comme le comptage par Quipu chez les civilisations précolombiennes (\url{https://fr.wikipedia.org/wiki/Quipu})}, des analyses et des inférences toutes tournées vers la seule quête de l'optimalité, de la moyenne et de la loi normale \citep{ladiray_laed_1997}. Les statisticiens tombent alors dans une logique \textbf{confirmatoire}, faisant des hypothèses fortes sur la nature même des données étudiées qui, dès lors, ne sont plus sources de calculs statistiques, mais un moyens vers la validation d'un modèle. La donnée est subordonnée au modèle et le réel doit venir confirmer la théorie. La domination des statistiques confirmatoires s'établit ainsi au détriment des statistiques exploratoires, souvent décriées par l'appellation péjorative de statistiques \textit{descriptives}.

\subsection{L'analyse exploratoire de données}

\noindent L'\textbf{analyse exploratoire de données} (AED) est un courant de la statistique moderne introduit, au cours des années 60-70, par le statisticien J.W. Tukey \citep{tukey_exploratory_1977}. Voulant réintroduire le réel au cœur des statistiques, Tukey préfère la résolution (même approximative) d'un problème véritable à la recherche d'un indicateur optimal sur une question de peu d'intérêt~:\\

\begin{fullwidth}
"\textit{The most important maxim for data analysis to heed, and one which many statisticians seem to have shunned, is this: Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise. Data analysis must progress by approximate answers, at best, since its knowledge of what the problem really is will at best be approximate. It would be a mistake not to face up to this fact, for by denying it, we would deny ourselves the use of a great body of approximate knowledge, as well as failing to maintain alertness to the possible importance in each particular instance of particular ways in which our knowledge is incomplete.}" --- \citep[p13-14]{tukey_future_1962}\\
\end{fullwidth}

\noindent Pour Tukey, les statistiques doivent ainsi retrouver le gout de l'in\-certitude et traiter à nouveau de la réalité, ainsi tout modèle ou toute d'analyse doit, selon lui, partir d'une réflexion empirique, sensibles. Les données doivent guider le choix des méthodes utilisées pour les étudier, et non l'inverse. C'est par la connaissance intime des données, que l'explorateur peut faire émerger des pistes de recherche et des hypothèses de travail. Il faut éprouver (qualitativement) les données dans la durée et retrouver une forme de rapport artisanal vis à vis d'elle. En cela, l'AED est un processus fondamentalement itératif, basé des boucles d'explorations successives, où l'analyste est amené à conjuguer intuition, capacités visuelle et expérience des données.

Là où les statistiques confirmatoires s'inscrivent dans l'administra\-tion de la preuve, l'analyse exploratoire, elle, s'articule autour d'une logique d'observation et de découverte.  

\subsection{Suivre et s'attacher aux indices}

\noindent L'AED avance par investigation, guidée par la découverte de l'inat\-tendue. L'explorateur multiplie les facets et les vues lui permettant de décrire les données dans leur ensemble et se place ainsi dans une position d'étonnement. Il s'agit de s'attacher autant aux tendances générales qu'aux détails, pour progresser pas à pas, par hypothèses successives en suivant la piste des indices précédemment révélés (Figure \ref{fig:aed-1}). Mais dans le l'esprit de Tukey, l'AED ne doit pas remplacer l'analyse confirmatoire, il convient plutôt de conjuguer les deux : les boucles d'exploration (Figure \ref{fig:aed-1}, a) induisent toute modélisation (Figure \ref{fig:aed-1}, b). 

\newpage

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/aed-1}
  \caption{L'analyse exploratoire et les diverses phases de l'exploration préfigurent toute analyse confirmatoire}
  \label{fig:aed-1}
\end{figure*} 

\noindent L'AED rencontre rapidement un écho grandissant, porté par le dé\-veloppement des techniques de calculs et de visualisations informatiques. Plusieurs années après, enrichissant sa réfection par la lecture des travaux de J. Bertin\footnote{Inspirateur de la sémiologie graphique ou science de la représentation graphique des données (\url{https://visionscarto.net/la-semiologie-graphique-a-50-ans})} \citep{bertin_semiologie_1973}, B. Fry propose de définir ce qu'il nomme \textbf{design computationnel d'information}\footnote{\textit{Computational information design} en anglais} \citep{fry_computational_2004}, comme une version moderne de l'AED, explicitement orientée vers la visualisation de données et la programmation informatique. Tout autant processus d'exploration itératif (Figure \ref{fig:aed-2}), qu'art de la représentation de la donnée, la méthode de Fry accompagne et motive le dé\-veloppement d'outils neufs : tels que l'environnement de programmation Processing\footnote{\url{https://processing.org/}} ou encore la librairie Javascript D3 créée par M. Bostock\footnote{https://d3js.org/}.

\begin{figure*}%
  \includegraphics[width=\linewidth]{graphics/aed-2}
  \caption{Étapes du design computationnel d'information et ensemble des boucles d'analyses possibles selon \citep{fry_computational_2004}}
  \label{fig:aed-2}
\end{figure*} 

\noindent Ce mode d'analyse par explorations offre un cadre méthodologique adapté à l'étude que nous souhaitons ici mener sur les archives Web. Concernant le devenir des blogs marocains après 2008, nous ne disposons au début de notre étude que d'un faisceau assez faible d'indices, susceptibles d'expliquer la disparition de la blogosphère. De plus, la taille de l'espace d'exploration (48 blogs segmentés en un multitude de fragments Web) et son étendue (4 années d'archives) nous pousse à cheminer par à-coups et à inférer de nos trouvailles de nouvelles hypothèses de travail. En cela, l'AED est souvent comparée (par Tukey lui même\footnote{"\textit{(EDA) is detective work – numerical detective work – or counting detective work – or graphical detective work ... unless exploratory data analysis uncovers indications, usually quantitative ones, there is likely to be nothing for confirmatory data analysis to consider}" --- \citep[p.1-3]{tukey_exploratory_1977}}) au travail du détective qui tisse la trame de ses recherches à travers un réseau d'indices. 

Ce raisonnement par inférence, qui émerge de notre description de l'AED, peut être rapproché d'un autre mode de pensée, découvert par C. Ginzburg en 1979 : le \textbf{paradigme indiciaire} \citep{ginzburg_signes_1980}. En effet, l'historien révèle que, dans nombre de disciplines liées aux sciences humaines (psychanalyse, histoire de l'art, médecine ...), s'est installé au fil du temps, un paradigme permettant de révéler à partir d'effets ou d'éléments singuliers des événements plus généraux\footnote{"\textit{Si la réalité est opaque, des zones privilégiées existent (traces, indices) qui permettent de la déchiffrer.}" --- \citep[p290]{ginzburg_mythes_2012}}. Le psychanalyste progresse par l'étude des symptômes du patient et l'historien de l'art étudie les traces du geste du maître derrière le pinceau.

Née à la suite des travaux de Ginzburg, la micro-histoire s'attache à analyser historiquement des objets de tailles limitées. Les chercheurs cheminent d'indices en indices, à la manière d'un Sherlock Holmes, pour compléter l'étude de trajectoires et de motivations individuelles. La micro-histoire se veut, ainsi, le contre poids focalisé d'une pratique plus quantitative du travail de l'historien. 

Comme pour l'AED face aux statistiques confirmatoires, on reproche à la micro-histoire la non représentativité de ses travaux, trop précis et détachés de l'étude des tendances historiques normales et des grandes structures de masse. Mais, sur ce point, et c'est l'élément qui nous intéresse ici tout particulièrement, selon Ginzburg l'anomalie ou l'objet singulier peut se révéler porteur d'une résonance plus globale :\\

\begin{fullwidth}
"\textit{La possibilité de passer du cas isolé à la généralisation part d'une hypothèse qui a gagné en clarté à travers le temps. Aujourd'hui, je proposerai de considérer un individu comme le point d'intersection d'une série d'ensembles différents qui ont chacun une dimension variable. [...] L'historien doit partir de l'hypothèse que chez tout individu quel qu'il soit, coexistent des éléments partagés par un nombre variable (entre plusieurs milliards et zéro) d'individus. L'anomalie sera le résultats des réactions réciproques entre tous ces éléments.}" --- \citep[p359-360]{ginzburg_mythes_2012}\\
\end{fullwidth} 

\noindent Dans les explorations à venir nous nous attarderons autant sur les tendances générales que sur les détails singuliers, suivant la tradition de l'AED. Mais l'incomplétude des archives Web, leur caractère partiel et détaché (sous certains aspects) de la réalité du Web vivant, nous conduiront à affiner petit à petit notre échelle d'analyse, à nous concentrer sur de plus petits objets dont certains détails nous auront révélé l'importance. 

Ce faisant, il peut alors être vertigineux de penser que, partant d'une archive de 70 terra octets, on en vienne à explorer, \textit{seulement}, le devenir en ligne d'une poignée d'internautes. Mais, en nous inspirant des travaux de Ginzburg, nous chercherons à repositionner ces éléments singuliers par rapport aux grandes vagues historiques dont ils ont été témoins ou vecteurs privilégiés. 

\section{Les traces d'une mutation numérique}
\label{sec:6_blogs}

\noindent Concentrons nous maintenant sur le devenir de la blogosphère Marocaine, entre 2008 et 2018. Essayons de comprendre, à travers les archives Web, le ou les processus qui ont amené à sa disparition quasi complète. Sommes nous témoins d'une extinction ? Ou d'une migration d'un territoire du Web vers un autre ? Peut-on encore trouver, dans nos collectages, les traces archivées d'une possible mutation des blogs vers les plateformes de réseaux sociaux ? 

\subsection{D'une communauté de blogs ...}

\noindent Entre 2008 et 2010, D. Diminescu et M. Renault cartographient et recensent 47 blogs\footnote{Voir la carte \url{http://www.e-diasporas.fr/wp/moroccan.html}} qu'ils intègrent à l'e-Diaspora Marocaine, alors en construction. Cette blogosphère est l'une des trois communautés\footnote{En théorie des graphes, une communauté de nœuds est un groupe fortement lié en son sein et faiblement lié avec le reste du réseau \citep{wasserman_social_1994, scott_social_2017}} majeures du réseau de sites marocains tel que nous l'avons introduit en section \ref{sec:2_atlas}. \\

\noindent Le terme \textbf{blogosphère} est un néologisme forgé, au début des années 2000, pour désigner les communautés de blogs alors florissantes sur la toile. À cette époque, d'anciens concepts, comme logosphère ou graphosphère\footnote{Communautés humaines basées sur le langage oral d'une part, et l'écrit d'autre part}, ne suffisent plus à traduire la nature hypertextuelle des liens établis entre ces sites. La connexion par liens de citation étant l'une des mécaniques de base des blogs et de leurs environnements. En effet, les auteurs en ligne s'agrègent en listes et communautés de blogs amis, qu'ils affichent de manière ostensiblement visibles sur leurs sites, comme marque consciente d'appartenance à un groupe délimité \citep{keren_blogosphere:_2006}. On n'entre pas dans une blogosphère, on s'y intègre. On cherche à la rejoindre en se connectant à ses membres qui, en retour, nous reconnaissent comme l'un des leurs.   

Type particulier de site Web, un blog est une plateforme de publication, en ligne, de billets ou d'articles courts. Les blogs sont alimentés en contenu de manière régulière et se rapprochent en cela du journal intime : chaque billet est daté et signé par un auteur (le créateur du blog ou une personne partageant les droits d'édition), l'ensemble des articles se lit ensuite du plus récent au plus ancien. Ces textes (incluant aussi images, liens, vidéos, ...) peuvent être vus et commentés par d'autres blogueurs ou par de simples visiteurs. 

En cela, un blog est, avant tout, un médium destiné à une population d'ama\-teurs de la toile. Sa facilité de création et de maintenance a fortement joué en faveur de la diffusion de ce support d'écriture auprès d'un large public. Il était ainsi possible, en quelques clics, de créer via des plateformes dédiées et souvent gratuites\footnote{On peut citer Overblog, Blogger, Skyblog, ...} son premier blog, et ce, sans être un professionnel de l'informatique. De fait, les blogs ont permis une forme de démocratisation de la prise de parole sur le Web, atteignant leur apogée\footnote{Les statistiques varient d'une source à l'autre, mais on estime généralement qu'entre 150 et 200 millions de blogs avaient été crées en 2011 \url{https://www.statista.com/statistics/278527/number-of-blogs-worldwide/}} à la fin des années 2000. Les sujets de publication y sont divers et fonction du goût de chacun : on y parle de son quotidien, on écrit des billets d'humeur, voire même, des commentaires et des analyses politiques. Le tout partagé et diffusé au sein de sa ou de ses communautés d'appartenance.  

Sur ce point, de premières études ont rapidement montré le rôle grandissant des blogs quant à la diffusion et à la cristallisation d'opi\-nions politiques sur le Web, souvent en rupture avec les discours tenus par les médias classiques (télévision, radio, ...). Par exemple, L.A. Adamic caractérise les divergences de pensées démocrates et républicaines en ligne, lors de la campagne présidentielle de 2004 aux États-Unis \citep{adamic_political_2005, adar_implicit_2004}. En France, l'étude du collectif RTGI\footnote{\textit{Réseaux, Territoires et Géographie de l'Information}, collectif fondé à l'Université de Technologie de Compiègne (UTC) par F. Ghitalla (\url{https://web.archive.org/web/20060702021013/http://www.utc.fr:80/rtgi/})} révèle la forte structuration et l'influence, sur la toile, des partisans du "\textit{non}" avant la tenue de référendum sur la Constitution Européenne de 2005 \citep{fouetillou_web_2008}. 

Passé le tournant des années 2010, les blogs perdent peu à peu de leur influence, E. Weltevrede \citep{weltevrede_where_2012} constate ainsi, en 2012, la lente disparition de la blogosphère néerlandaise. Cette perte de vitesse se fait au profit des plateformes de réseaux sociaux (Facebook, MySpace, Instagram ...) et de micro-blogging (Twitter, Tumblr, Sina Weibo, ...). Ces dernières reprennent à leur compte, et améliorent, les mécaniques de publication introduites par les blogs, accélérant toujours la diffusion et le partage de contenu sur le Web.\\  

\begin{table*}
  \label{tab:blogs}
  \begin{tabular}{llll}
    \toprule
    nom de domaine&catégorie&langue principale&pays du blog\\
    \midrule
    7didane.org&intimiste&Français&USA\\
    9afia.blogspot.com&littérature&Français&USA\\
    adilski.blogspot.com&mixe&Anglais&USA\\ 
    al9adiya7amda.blogspot.com&social – politique&Français&USA\\
    anasalaoui.com&social – politique&Français&France\\ 
    badrryadi.centerblog.net&littérature&Français&France\\ 
    blogreda.blogspot.com&mixe&Français&USA\\ 
    boubouh.over-blog.com&social – politique&Français&France\\
    cabalamuse.wordpress.com&mixe&Anglais&USA\\ 
    eatbees.com/blog&intimiste&Anglais&USA\\ 
    emigrant.canalblog.com&intimiste&Français&France\\ 
    enmarruecos.blogspot.com&intimiste&Espagnol&USA\\ 
    fatima-salma.over-blog.com&cuisine&Français&France\\ 
    kalima.hautetfort.com&littérature&Français&France\\ 
    karlamassini.blogspot.com&littérature&Français&USA\\ 
    kennza.wordpress.com&mixe&Français&USA\\ 
    kingstoune.com&mixe&Français&France\\ 
    klamia.canalblog.com&mixe&Français&France\\ 
    kugelschreiber.canalblog.com&humeur&Français&France\\
    labelash.blogspot.com&mixe&Anglais&USA\\ 
    lailalalami.com&littérature&Anglais&USA\\ 
    lallamenana.free.fr&humeur&Français&France\\ 
    larbi.org&social – politique&Français&France\\ 
    lemythe.com&intimiste&Français&Maroc\\ 
    lesamismarocains.blogspot.com&humeur&Français&USA\\ 
    louladekhmissbatata.wordpress.com&mixe&Français&USA\\ 
    magiaenmarruecos.blogspot.com&intimiste&Espagnol&Espagne\\ 
    makhoudjit.blogspot.com&social – politique&Arabe&USA\\ 
    marocainsdalgerie.sosblog.com&social – politique&Français&Algérie\\ 
    marouki.joeuser.com&social – politique&Français&USA\\ 
    mlouizi.unblog.fr&littérature&Français&France\\ 
    mohblog.blogspot.com&littérature&Français&USA\\ 
    monagora.fr&humeur&Français&France\\ 
    murmures.net&intimiste&Français&USA\\ 
    myrtus.typepad.com&humeur&Anglais&USA\\ 
    nowbi.over-blog.com&intimiste&Français&France\\ 
    oef75.blogspot.com&littérature&Français&USA\\ 
    poliquonautemarocain.blogspot.com&social – politique&Français&USA\\ 
    purplemind-candysha.blogspot.com&intimiste&Français&USA\\ 
    saad.amrani.free.fr/blog&intimiste&Français&France\\ 
    sahara-libre.blogspot.com&social – politique&Français&USA\\ 
    sebti.fr&mixe&Français&France\\ 
    slimane.canalblog.com&humeur&Français&France\\ 
    sonofwords.blogspot.com&intimiste&Français&USA\\ 
    soumiaz.blogspot.com&intimiste&Anglais&USA\\ 
    supertimba.skynetblogs.be&intimiste&Français&Belgique\\ 
    taha.fr/blog&mixe&Français&France\\ 
  \bottomrule
  \end{tabular}
  \bigskip
  \caption{Liste des 47 blogs de la blogosphère marocaine}
\end{table*}  

\begin{figure}%
  \includegraphics[width=\linewidth]{graphics/blogs-years}  
  \caption{Répartition des blogs de la blogosphère marocaine par année de création}
  \label{fig:blogs-years}
\end{figure}

\noindent En 2008, la \textbf{blogosphère marocaine}\footnote{Par abus de langage, nous appelons ici blogosphère marocaine le groupe de blogs de l'e-Diaspora marocaine. Il existe et a existé, bien évidement, d'autres blogosphères au Maroc, ce pays comptant près de 30,000 blogs en 2008 (\url{https://mg.co.za/article/2008-01-08-moroccos-blogosphere-takes-off})} est une communauté fortement connectée de 47 sites Web crées ou maintenus par des citoyens marocains de l'étranger. Ce collectif est relativement jeune, la majorité des blogs ayant moins de trois années d'existante (Figure \ref{fig:blogs-years}) au moment de sa cartographie pour l'Atlas.

La table \ref{tab:blogs} recense les blogs par noms de domaine, type de publication, langue et localisation. Mais situer géographiquement un site Web, quel qu'il soit, est toujours une étape sujette à discussion. Durant la création de l'Atlas, les chercheurs se sont servis, à la fois, d'indications présentes sur les sites eux même, et d'une méthode de localisation par adresse IP\footnote{La recherche par IP renvoie le pays d'hébergement du site (là où sont situés les serveurs), pas le pays de résidence de son auteur}. Ainsi, beaucoup de blogs sont hébergés aux USA, du fait principalement des plateformes choisies pour créer ces sites. Mais la combinaison de cette information, associée à la langue principale de chaque site (Table \ref{tab:blogs}), nous donne tout de même une idée générale de la répartition géographique du réseau : partagé entre la France et les États Unis.

\begin{figure*}
  \includegraphics[width=\linewidth]{graphics/blogs-1}
  \caption{La blogosphère marocaine en 2008}
  \label{fig:blogs-1}
\end{figure*}

\noindent La figure \ref{fig:blogs-1} présente la blogosphère sous la forme d'un graphe, telle qu'elle fut cartographiée en 2008\footnote{Une version en ligne de cette visualisation est disponible ici}. La taille des nœuds y est fonction du degré de chacun et les notes en jaunes marquent ceux qui, parmi ces blogs, ont bénéficié d'une présence en ligne antérieure à la formation de la blogosphère. 

En son sein, le blog politique \textit{larbi.org} fait autorité : sa position est centrale et son degré est le plus élevé du réseau. En 2008, il est élu blog marocain et politique de l'année par un jury d'internautes\footnote{\url{https://fr.wikipedia.org/wiki/Maroc\_Web\_Awards\#Maroc\_Blog\_Awards\_2008}} et inaugure, de fait, un cycle de marques de reconnaissance des blogueurs entre eux. Cette forme de mise en avant des siens, pour promouvoir par extension l'ensemble du collectif, est monnaie courante dans la blogosphère. On y cite volontiers les blogs qui nous semblent influents\footnote{L'auteur de \textit{manal.over-blog.com} se réfère explicitement à \textit{larbi.org} comme source d'inspiration (\url{http://manal.over-blog.com/article-517669.html})}, ou  ceux que l'on souhaite promouvoir\footnote{Les lecteurs de \textit{larbi.org} citant \textit{makhoudjit.blogspot.com} parmi d'autres blogs crées par des femmes  (\url{https://web.archive.org/web/20130319234735/http://www.larbi.org/post/2006/03/05/183-hommage-aux-femmes})}.

La présence de certains membres sur le Web est parfois antérieure à la formation de la blogosphère, \textit{7didane.org} par exemple en est déjà à son second site\footnote{\url{https://web.archive.org/web/20110810230023/http://7didane.blogspot.com/
} et nœuds jaunes de la figure \ref{fig:blogs-1}}. Et si \textit{larbi.org} se propose, principalement, de commenter la vie politique marocaine à travers son regard de marocain émigré, les billets publiés sur les autres blogs sont divers et variés : allant de recettes de cuisines (\textit{fatima-salma.over-blog.com}), à des récits de vie quotidienne (\textit{murmures.net}), et en passant par des chroniques littéraires (\textit{lailalalami.com}).  Ainsi, la blogosphère se vie et se pense comme un environnement vibrant et dynamique où circulent des auteurs, des lecteurs et des visiteurs.

\subsection{... à un collectif éteint}

\noindent En 2015, dans un premier rapport \citep{khouzaimi_e-diasporas_2015}, J. Khouzaimi constate la disparition de 47 des 153 sites de l'e-Diaspora marocaine. La blogosphère est alors réduite à 17 membres actifs\footnote{Pour comprendre le décompte de J. Khouzaimi, un blog actif est (selon sa méthodologie) un blog toujours en ligne}. Par ailleurs, une série d'entretiens, enrichissant ce travail, nous apprennent que les blogueurs toujours \textit{actifs} ne sont pas tous de jeunes étudiants comme le pensait en premier lieu l'auteure. Nombre d'entre eux sont des quarantenaires ou des cinquantenaires qui ont choisi, malgré les évolutions du Web, de maintenir ouverts leurs sites sur la toile. 

Mais une frontière entre étudiants de 2015 et anciens blogueurs se dessine : chez les étudiants, l'attachement en ligne à un passé vécu au Maroc, se traduit au mieux par l'appartenance à certains groupes communautaires sur Facebook et Whatsapp\footnote{Tout comme Telegram, l'application de messagerie Whatsapp permet de créer des salon de discussions virtuels à plusieurs, voir \url{https://frama.link/JnjBqw8u}}, plus que par la consultation des anciens blogs d'émigrés, dont ils ne connaissent pas toujours l'existence d'ailleurs. \\

\noindent \textbf{Début 2018}, nous choisissons, à notre tour, de revenir en détail sur le cas des blogs de l'e-Diaspora marocaine. Pour ce faire, nous visitons chacune des 47 URLs associées et classons les sites selon trois catégories :

\begin{enumerate}  
\item vivant : le blog est en ligne et a été mis à jour au moins une fois au cours de l'année passée (courant 2017 donc)
\item abandonné : le blog est en ligne mais n'a pas été mis à jour récemment 
\item mort : le blog n'est plus en ligne ou a été victime de cybersquatting (le blog n'est plus celui qu'il a été au moment de sa cartographie, il a été volé ou squatté)
\end{enumerate}

\noindent La figure \ref{fig:blogs-2} présente l'état\footnote{La position des nœuds est conservée par rapport à la figure \ref{fig:blogs-1}} de la blogosphère, telle que nous la trouvons en 2018. Nous ne recensons plus que 5 blogs vivants (Figure \ref{fig:blogs-2}, (a)), 23 ont été abandonnés (Figure \ref{fig:blogs-2}, (b)) et les 19 autres sont morts. Parmi les blogs toujours en vie, deux sont, en fait, des transformations de blogs cartographiés en 2008 mais déplacés depuis par leurs auteurs : \textit{louladekhmissbatata.wordpress.com} devient \textit{louladekhmissbatataprise2.com} en 2009 et \textit{cabalamuse.wordpress.com} devient \textit{amoroccandrifter.wordpress.com} en 2012. Le collectif en ligne de 2008 s'est éteint.

\begin{figure*}
  \includegraphics[width=\linewidth]{graphics/blogs-2}
  \caption{La blogosphère en 2018, (a) blogs vivants, (b) blogs vivants et abandonnés, la position est conservée}
  \label{fig:blogs-2}
\end{figure*}   

\subsection{Définir l'espace d'exploration}

\noindent À ce niveau, nous pouvons, en rassemblant les informations collectées, commencer a proposer quelques hypothèses. Nous savons, qu'en 2015, certains étudiants marocains préféraient les groupes Facebook aux blogs d'émigrés pour maintenir le lien avec leur vie passée au Maroc \citep{khouzaimi_e-diasporas_2015}. Ces groupes Facebook étaient, par ailleurs, déjà contemporains de la blogosphère au moment de sa cartographie. En effet, S. Marchandise a réalisé, dans le cadre de l'Atlas, une e-Diaspora \citep{marchandise_facebook_2014} entièrement centrée sur la présence\footnote{\url{http://www.e-diasporas.fr/wp/marchandise.html}} des étudiants émigrés marocains au sein de groupes Facebook. De plus, les mécanismes des réseaux sociaux (Facebook, Twitter, ...) ne sont si éloignés que ça des fonctionnalités démocratisées par les blogs en leur temps \citep{kwak_what_2010} : ils en reprennent certaines fonctionnalités (publication, création de liens, création de communautés, ...), en améliorent d'autres (vitesse de diffusion, intégration mobile, ...) et introduisent leurs propre règles (partage, retweet, flux d'informations et timeline, ...).

Par ailleurs, nous nommons \textbf{auteur} la personne ou l'ensemble de personnes ayant crée ou maintenu l'un des 47 blogs de la blogosphère marocaine. De même, nous nommons \textbf{lecteur} une personne qui visite, réagit ou laisse un commentaire sur l'un de ces sites.

Notre tâche d'exploration a ici pour but de comprendre, autant que faire ce pourra, le devenir d'un \textbf{collectif en ligne éteint}, c'est à dire une communauté pour laquelle peu ou plus aucune trace ne subsiste encore sur le Web vivant. Deux grandes hypothèses peuvent être formulées : \textbf{1)} les blogs ont purement et simplement disparu de la surface du Web, les auteurs qui les ont crées n'ont pas voulu ou pu donner suite à cette présence sur la toile \textbf{2)} les blogs se sont mués en une autre entité, migrant d'un territoire du Web (la blogosphère) à un autre (ici, les plateformes de réseaux sociaux). Nous chercherons donc, dans un premier temps, à identifier les indices archivés de cette possible mutation. 

L'espace d'exploration est donc le suivant : un nombre connu et limité de sites Web observés pendant 10 ans, de 2008 à 2018. Grâce aux archives de l'atlas e-Diasporas, nous disposons de 4 années de collectes (2010-2014), que nous allons étendre à 6 années grâce aux dates d'éditions des fragments Web (2008-2014), la période 2014-2018 sera, elle, couverte par les collectages d'Internet Archive. 

Pour commencer, nous partons de la liste des 47 sites de la blogosphère que nous fragmentons (Section \ref{sec:5_scraping}) et envoyons dans note moteur d'exploration (Section \ref{sec:3_constituer}). L'idée est ici de réaliser une même requête plein texte sur deux champs différents de nos fragments Web. Cette requête, nous la construisons à partir de mots clés que nous pensons être en rapport avec les réseaux sociaux, tels que :

\begin{figure*}
\small
\begin{verbatim}
facebook, twitter, instagram, pinterest, youtube, flicker, medium, myspace, google+, googleplus, tumblr, 
share, like, retweet, tweet, partager, aimer, social, ... 
\end{verbatim}
\end{figure*}

\noindent Ces mots clés sont, ensuite, testés par rapport au contenu des champs \textit{frag\_text} et \textit{frag\_label} de chaque fragments. Les requêtes à \textit{frag\_text} permettent de détecter d'éventuelles références aux réseaux sociaux dans le corps d'un texte : par exemple, un blogueur parlant de Twitter dans l'un de ses billets. Les requêtes à \textit{frag\_label} sont supposées capturer et retourner les nœuds HTML dont les labels\footnote{Pour rappel, le label d'un nœud HTML est la concaténation de son tag, de son id et de sa classe} matchent l'un des mots clés : par exemple, un bouton "\textit{partager sur facebook}" présent sur une page Web. Nous cherchons donc, à la fois, du contenu textuel et des éléments de mise en forme.

\begin{figure*}
  \includegraphics[width=\linewidth]{graphics/blogs-fragments-1}
  \caption{Exemples de fragments Web associés à divers réseaux sociaux}
  \label{fig:blogs-fragments-1}
\end{figure*} 

\subsection{Migrer d'un territoire du Web à un autre}

\noindent Trois types de fragments Web répondent à nos requêtes : \textbf{1)} des fragments de textes extraits d'articles ou de billets \textbf{2)} des boutons et \textit{widgets} like, share, follow me, ... près construits et mis à disposition des blogueurs par les réseaux sociaux \textbf{3)} un mélange de nœuds HTML non conventionnels, confectionnés par les auteurs eux même, comme système de redirection Twitter ou Facebook. 

Le premier type sera examiné à la fin de cette section. En effet, nous préférons d'abord présenter les deux dernières sortes de fragments Web qui témoignent d'un age du Web balbutiant (2008-2010), où les auteurs de blogs devaient eux même \textit{bricoler}, avec un peu de HTML et de CSS, des connexions vers leurs nouveaux comptes sociaux. Avec le temps, cette pratique s'est faite oublier, supplantée par l'arrivée d'éléments hypertextes standardisés et prêts à l'emploi. La figure \ref{fig:blogs-fragments-1} donne à voir quelques exemples de ces fragments Web avec, en rouge, les parties ayant matchées nos requêtes. 

Mais en y regardant de plus prêt, ces fragments ont d'autres informations à nous offrir : il est possible de trouver, au cœur du HTML (Figure \ref{fig:blogs-fragments-1}, bleu), des noms de comptes sociaux liés aux blogs. En effet, lorsque l'on souhaite s'inscrire sur une plateforme de réseaux sociaux, il faut d'abord et avant tout se créer un avatar : le \textit{@user} de Twitter ou le \textit{pseudo} de Facebook par exemple. Ces avatars deviennent alors l'incarnation de notre identité sur les réseaux. Ce faisant, nous constituons rapidement une liste d'association entre blogs passés et noms de comptes sociaux. À la fin de cette étape, nous identifions 20 blogs, parmi nos 47 de départ, liés à un ou plusieurs réseaux sociaux.

Afin d'élargir nos résultats, nous faisons ici l'hypothèse que les avatars, pseudos et autres noms d'utilisateurs peuvent être réutilisés par un même auteur, d'une plateforme à l'autre, moyennant une légère modification. À titre d'exemple, le blogueur \textit{eatbees.com} se nomme \textit{@eatbees} sur Twitter et \textit{eatbees} sur Medium. Aussi, nous normalisons les 20 avatars déjà extraits avec un petite opération de stemming\footnote{Les majuscules deviennent minuscules et l'on ne conserve que la racine des pseudonymes (l'avatar moins les deux derniers caractères)}. L'objectif de cette nouvelle phase est maintenant d'explorer le Web vivant à la recherche de comptes sociaux (Youtube, Flickr, Medium, ...) liés, de prêt ou de loin, à l'un de ces 20 pseudonymes.   

Cette tâche ne peut malheureusement pas être automatisée. Si la recherche par fragment Web, dans notre moteur, nous a permis d'iden\-tifier rapidement quelques indices, c'est maintenant au chercheur d'en\-trer en scène. La fouille (via des moteurs de recherche : Google, Bing, ...) et la vérification des informations tirées du Web vivant se font à la main. Ainsi, nous récoltons un total de 33 comptes sociaux actifs différents, liés par leurs avatars aux 20 blogs ayant matché les requêtes par mots clés initiales. 

La figure \ref{fig:blogs-social} présente la répartition de ces 33 comptes, entre anciens blogs (gauche) et nouveaux réseaux sociaux (droite). Twitter et Facebook dominent cette liste, ils agrègent à eux seuls 23 comptes. L'usage de Youtube, Pinterest, etc reste plus confidentiel. Bien sûr, ce classement ne peut  se prévaloir d'une quelconque exhaustivité, par construction cela lui est impossible. En effet, Si l'un des auteurs des blogs a préféré utiliser un pseudonyme entièrement nouveau, sans en faire mention dans les pages de son ancien site, alors nous ne sommes pas ici en mesure de le retrouver.

\begin{figure*}
  \includegraphics[width=\linewidth]{graphics/blogs-social}
  \caption{Liste des blogs de 2008 liés à des réseaux sociaux en 2018}
  \label{fig:blogs-social}
\end{figure*} 

\noindent Si l'on s'en tient aux seuls blogs subsistants (Figure \ref{fig:blogs-2}), la blogosphère est bel et bien un collectif éteint. Crées entre 2005 et 2008, la quasi totalité de ces sites ont progressivement été fermés ou laissé à l'abandon par leurs auteurs. La blogosphère en tant qu'espace de communication n'a pas réussi à durer, du fait même, de ceux qui la constituaient. Mais en lieu et place d'une disparition, c'est en fait à une transformation que nous avons ici affaire. Les blogs se sont mués en une multitudes de comptes sur les réseaux sociaux et au cours de cette migration, les anciens blogueurs ont choisi de conserver leurs identités en ligne. En gardant leurs avatars, ils leur a été plus facile de se retrouver et de se reconnecter après coup.

En effet, en procédant à un rapide scraping des pages Facebook et Twitter, nous pouvons récolter l'ensemble des liens d'amitié et d'intérêt\footnote{Lien entre \textit{fellowers} sur Twitter} qui sont aujourd'hui tissés entre ces différents comptes. La figure \ref{fig:blogs-3} se veut, ainsi, une vue de la blogosphère, après sa mutation et telle que nous la redécouvrons en 2018. Nous plaçons les comptes sociaux là où se situaient leurs anciens blogs respectifs (Figure \ref{fig:blogs-2}) et nous traçons entre eux les nouveaux liens de citations (liens rouges).

\newpage

\begin{figure*}
  \includegraphics[width=\linewidth]{graphics/blogs-3}
  \caption{La blogosphère en 2018, les emplacements des blogs sont conservés}
  \label{fig:blogs-3}
\end{figure*} 

\noindent Sur la nouvelle blogosphère, l'expression en ligne se fragmente et se spécialise par type de réseau et de médium. Certains, comme \textit{7didane.org}, choisissent d'avoir un compte Facebook en parallèle d'un compte Twitter. D'autres utilisent Youtube ou Flickr pour publier des photos ou des vidéos comme \textit{larbi.org}. On peut également observer l'utilisation conjointe de Twitter et de Medium\footnote{Plateforme de partage de billets se voulant longs et argumentés} (ou d'un blog Médiapart) sur lequel l'auteur va choisir d'écrire un texte réfléchis et structurés avant de le promouvoir à sa communauté via Twitter, comme \textit{eatbees.com}. 

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/density}
  \vspace*{0.2cm}  
  \caption{Densité d'un graphe}
  \label{fig:density}
\end{marginfigure}

Mais, la migration vers les réseaux sociaux n'est pas, pour autant, synonyme d'éclatement ou d'isolation des anciens membres de la blogosphère. Prenons, ainsi, le cas des 16 auteurs ayant désormais un compte Twitter. Il est possible de comparer la densité du sous graphe qu'ils formaient en 2008, avec celle de leur graphe d'intérêt, tel qu'il se trouve sur Twitter en 2018. 

En théorie des graphes, un graphe est dit \textbf{dense} si le nombre de ses liens est proche du nombre de liens maximum possibles. Si les nœuds ne sont pas du tout connectés entre eux, alors la densité vaudra $0$, mais s'ils sont tous liés celle ci vaudra $1$ (Figure \ref{fig:density}). Entre 2008 et 2018, la densité du sous graphe des utilisateurs de Twitter, anciennement blogueurs, passe  ainsi$0.16$ à $0.24$. L'aspect \textit{communauté} de l'ancienne blogosphère est conservée et même renforcée par le passage sur Twitter.

\subsection{Communauté de lecteurs et identité}

\noindent Sur la figure \ref{fig:blogs-3}, la taille de chaque nœud social est fonction de l'étendue de sa communauté de followers (pour Twitter) ou d'amis (pour Facebook). À titre d'exemple, \textit{7didane.org} est suivi par 43,512 sur Twitter et a 141,947 amis sur Facebook. De son côté, \textit{lailalalalmi.com}  dépasse les 35,000 followers sur Twitter\footnote{Chiffres de Janvier 2018}.

À l'ère des réseaux sociaux, l'influence d'un auteur est souvent liée au nombre de lecteurs ou de visiteurs auxquels il est capable de s'adresser. La communauté devient une audience. De fait, la dynamique interne à la blogosphère évolue : \textit{larbi.org} n'est plus hégémonique, \textit{7didane.org} et \textit{lailalalalmi.com} le dépassent désormais.

\begin{figure*}
  \includegraphics[width=\linewidth]{graphics/blogs-community}
  \caption{Répartition géographique des followers Twitter (a) et des amis Facebook (b)}
  \label{fig:blogs-community}
\end{figure*} 

\noindent L'origine des followers Twitter et des amis Facebook peut nous renseigner sur le caractère diasporique de la communauté des lecteurs et des visiteurs de 2018. D'où lisent celles et ceux qui suivent les anciens blogueurs sur les réseaux sociaux ?

\noindent Pour répondre à cette question nous suivons une double approche. La première est adaptée à Facebook, la seconde à Twitter : \textbf{1)} nous utilisons l'application Netvizz qui associe à chaque personne, ayant aimé une page facebook donnée, une situation géographique\footnote{Cette information est issue des méta données renseignées lors de l'inscription à Facebook ou de la modification de sa page personnelle (\url{https://apps.facebook.com/netvizz/})} \textbf{2)} nous scrapons les pages de l'ensemble des followers des comptes Twitter. En effet, le champs \textit{location}\footnote{\url{https://help.twitter.com/fr/using-twitter/tweet-location}} de ces pages peut, s'il est renseigné, renfermer une information géolocalisée, dont nous faisons ici l'hypothèse qu'elle correspond au lieu, depuis lequel, une personne utilise Twitter. Nous croisons, ensuite, la valeur de ce champ avec la base de données d'entités géographiques Geonames\footnote{\url{http://www.geonames.org/}} afin de désambiguïser l'information extraite. 

Ces deux méthodes ne sont pas exhaustives, mais elles nous donnent à voir une coloration et une tendance générale. Nous essayons, néanmoins, de limiter au maximum les faux positifs dans nos résultats. Ainsi, la figure \ref{fig:blogs-community} présente pour chaque compte, la répartition géographique (si elle est connue) de ses followers Twitter ou amis Facebook. En pratique, \textit{7didane.org} est principalement suivi par des personnes situées au Maroc ($82\%$) mais aussi par quelques Français ($2\%$) et des Égyptiens ($2\%$). De son coté, \textit{lailalalami.com} continue de s'adresser à des Marocains ($24\%$), à des Américains ($15\%$) et à des Pakistanais ($8\%$). Au moins 19 pays\footnote{Algérie, Maroc, Égypte, Tunisie, Libye, France, Chine, Espagne, Pays-Bas, USA, Pakistan, Indonésie, Bangladesh, Nigeria, Inde, Belgique, Grande Bretagne, Canada et Corée du Sud} sont ainsi représentés, témoignant du caractère géographiquement dispersé de cette communauté de lecteurs. \\

\noindent Comme nous l'expliquions plus tôt, les auteurs ont su conserver leur identité en ligne en réutilisant les même pseudonymes, d'une plateforme à l'autre, et ce, depuis le temps de leurs premiers blogs. Nous faisons maintenant l'hypothèse de retrouver le même comportement chez leurs lecteurs. En effet, un blog est un médium tourné vers l'intéressement d'un public donné. Aussi, il nous semble raisonnable d'avancer que lorsqu'un lien fort se crée entre un auteur et ses lecteurs, ces derniers peuvent chercher à conserver à tous prix cette connexion. Nous supposons donc que les lecteurs ont réutilisé, eux aussi, leurs anciens avatars sur Twitter, pour suivre les auteurs qu'ils supportaient au temps des blogs.

Pour tester cette hypothèse, nous nous focalisons sur le site \textit{larbi.org} et sur le lien qu'il entretenait avec les personnes commentant ses articles, lorsqu'il faisait encore autorité (Figure \ref{fig:blogs-1}). Nous définissons alors un modèle de requête capable de nous retourner l'ensemble de ces commentaires archivés. Il s'agit ainsi de cibler des fragments Web ayant la forme suivante (champ \textit{frag\_type}) : un nom d'auteur, une date et un contenu textuel publié dessous l'article principale d'une page. Ce faisant, nous obtenons 4177 résultats dont ne conservons que les noms d'utilisateurs.

Nous croisons cette liste à celle des followers actuels de \textit{larbir.org} sur Twitter\footnote{\url{https://twitter.com/larbi_org/followers}} via une égalité stricte pour ne pas multiplier les faux positifs. Au final, nous obtenons une borne inférieure de 647 lecteurs ayant, à priori, choisi de suivre \textit{larbi.org} sur Twitter, soit prêt de $15\%$ de son audience passée. Rapportés à leur nombre de commentaires publiés, ces lecteurs représentent au final $26\%$ des réactions postées sur le site. Les auteurs des blogs ont été accompagnés dans leur migration vers les réseaux sociaux. En conservant leur identité en ligne, ils ont emporté, à leur suite, la part la plus fidèle de leurs lecteurs et de leurs communautés.

\subsection{Le Printemps Arabe comme un moment clé ?}

\noindent Avant de clôturer leurs blogs, seuls 6 auteurs ont déposé sur leur site un message d'adieu, ou d'explication\footnote{Pour \textit{7didane.org} \url{https://web.archive.org/web/20120415100250/http://www.7didane.org:80/ 
}, pour \textit{larbi.org} \url{https://web.archive.org/web/20140119233126/http://www.larbi.org/post/2013/12/A-nos-apprentis-dictateurs} \url{-redacteurs-du-Code-du-Numerique}}. Passé un certain temps, les blogs ne sont plus mis à jours et finissent alors par disparaitre sans crier gare. Le blogueur \textit{7didane.org} est de ceux qui ont souhaité indiquer à leur communauté qu'il partait. En avril 2012 la front page de son blog vire au noir (Figure \ref{fig:blogs-adieu}, a), une adresse mail est laissée pour ceux qui souhaiteraient garder le contact. Si \textit{7didane.org} n'a pas donné plus d'explication, \textit{larbi.org}, au contraire, argumente sa volonté de clôturer son blog par dernier article (Figure \ref{fig:blogs-adieu}, b).

\begin{figure*}
  \includegraphics[width=\linewidth]{graphics/blogs-adieu}
  \caption{Annonce de la fermeture de \textit{7didane.org} (a) et article précédant la clôture de \textit{larbi.org} (b)}
  \label{fig:blogs-adieu}
\end{figure*} 

\noindent Fin 2013, il y met en garde ses lecteurs contre l'application à venir, au Maroc, du \textit{projet de loi formant Code numérique}. Ce texte, selon lui, porte une atteinte grave au droit de citation hypertexte et, de manière plus générale, vise à limiter la liberté d'expression sur le Web\footnote{Voir \url{https://www.esi.ac.ma/Dossiers/20131217031227.pdf}}. Quelques semaines après, sans rien ajouter de plus, l'auteur décide de fermer son blog. Il ne s'exprime aujourd'hui plus que par son compte Twitter\footnote{\url{https://twitter.com/Larbi_org}}. Ce dernier billet témoigne d'un fort attachement aux libertés fondamentales et d'une crainte de voir le Web, comme espace d'expression, être de plus en plus censuré. Mais le contenu de ce texte, peut être désormais mis au regard d'un autre article. 

En effet, nous n'avons pas encore parlé des fragments de billets ayant, plus tôt, matché notre première liste de mots clés. Parmi ces résultats, les premiers fragments Web archivés faisant mention de Twitter chez \textit{larbi.org} et \textit{7didane.org} attirent particulièrement notre attention, la figure \ref{fig:blogs-fragments-2} transcrit ces morceaux de textes. 

\begin{figure*}
  \includegraphics[width=\linewidth]{graphics/blogs-fragments-2}
  \caption{Premiers fragments Web archivés faisant mention de Twitter chez \textit{larbi.org} \textit{7didane.org}}
  \label{fig:blogs-fragments-2}
\end{figure*}

\noindent En 2009, le blogueur \textit{7didane.org} dit suivre\footnote{\url{https://web.archive.org/web/20090627012354/http://www.7didane.org:80/2009/06/16/1453/
}}, via Twitter, le déroulé des manifestations\footnote{\url{https://fr.wikipedia.org/wiki/Soulèvement\_postélectoral\_de\_2009\_en\_Iran}} qui secouent alors l'Iran. Cette année là, la contestation des élections du mois de Juin provoque de nombreux mouvements de protestations à Téhéran et dans le reste du pays. Mais surtout, les manifestations trouvent une résonance inédite sur le Web. L'information étant contrôlée par les médias gouvernementaux, les ci\-toyens s'emparent de Twitter pour diffuser leurs témoignages, leurs photos et leurs vidéos. Cette vague de manifestations porte encore aujourd'hui le nom de \textit{Révolution Twitter}. 

Chez \textit{larbi.org}, Twitter fait pour la première fois son apparition dans un billet\footnote{\url{https://web.archive.org/web/20110319191709/http://www.larbi.org:80/post/2011/02/Morocco-Feb20-Maroc-20Fev
}}, au moment où le Printemps Arabe\footnote{\url{https://fr.wikipedia.org/wiki/Printemps_arabe}} commence à se pro\-pager au Maroc, début 2011. Dans cet article \textit{larbi.org} propose de suivre puis de revivre, via un hashtag dédié (Figure \ref{fig:blogs-fragments-2}, bleu), la manifestation marocaine du 20 Février 2011.

Pour ces deux auteurs, Twitter n'a pas véritablement été découvert dans le contexte très précis de ces deux moments révolutionnaires, \textit{larbi.org} et \textit{7didane.org} ayant tous deux ouverts leurs comptes Twitter en 2008. Mais, les manifestations Iraniennes et le Printemps Arabe ont pu jouer un rôle de révélateur, quant à un nouvel usage possible de Twitter, chez des personnes sensibles aux questions de libertés et de démocraties. À leurs yeux, Twitter est peut être, à ce moment là, devenu un nouvel espace d'expression libre sur la toile. 

Nous pouvons ici émettre l'hypothèse que le Printemps Arabe, dans le cas particulier du Maroc, s'est inscrit dans un faisceau complexe d'événements qui ont, à un moment donné, influencé le devenir de la blogosphère, voire du reste de l'e-Diaspora marocaine. Bien sur, à l'échelle de nos archives, ces deux textes sont des détails statistiques, mais ils sont suffisamment intéressants pour nous mettre maintenant sur la piste de la manifestation du 20 février 2011 et de ce hashtag \textit{\#20Fev}. Cet événement a-il-pu avoir un impacte visible sur d'autres espaces de notre corpus ?

\section{Un soulèvement en ligne éphémère}
\label{sec:6_printemps}

\noindent En section \ref{sec:retour_au_moteur}, partant d'une démonstration technique, nous avions commencé à entrevoir le contenu des archives du forum \textit{yabiladi.com}. Notre système de détection d'événements nous révélait un pic de conversations portant sur la démission de l'ancien dirigeant tunisien Z. Ben Ali, comme un trait d'union entre un fait du Web et un bouleversement du réel. En effet, cette démission marque l'un des moments forts de la vague de contestations populaires qui a déferlé, fin 2010, sur l'ensemble du monde Arabe.

Dans ce manuscrit, nous tenons à insister sur la nécessité d'identifier des points d'entrée et de trouver des indices pour orienter nos explorations des archives Web. Le corpus marocain à lui seul étant tellement vaste, qu'il convient selon nous de s'attacher d'abord à l'étude de moments singuliers, bien définis et cadrés, sur lesquels nous sommes capable, à notre échelle et avec nos outils, de porter un regard autant quantitatif que qualitatif. 

En cela, la manifestation du 20 février 2011 révélée dans la section précédente, nous semble être un bon fil conducteur, pour tenter de comprendre la manière dont les utilisateurs de \textit{yabiladi.com} ont pu réagir au Printemps Arabe. Peut-on remonter les traces archivées d'un potentiel soulèvement du forum en amont ou en aval de cette manifestation ? ses membres se sont-ils organisés d'une quelconque manière, ou ont-ils regardé passer les protestassions sans y prendre part ? 

\subsection{Yabiladi.com : porte d'entrée sur la diaspora}

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/yabiladi-map}
  \vspace*{0.2cm}  
  \caption{\textit{yabiladi.com} (rouge) dans l'e-Diaspora marocaine}
  \label{fig:yabiladi-map}
\end{marginfigure} 

\noindent Le site Web \textit{yabiladi.com} se présente donc comme le candidat idéal pour cette nouvelle exploration de notre corpus d'archives.  Son rôle est central dans l'organisation de l'e-Diaspora marocaine, puisqu'il fait le lien entre les trois grands clusters du réseau. C'est un hub, un pont entre des constellations de sites qui ne se parlent pas (Figure \ref{fig:yabiladi-map}).

Site hybride, mélangeant forum de discussion en ligne, actualités, petites annonces et rencontres amoureuses, \textit{yabiladi.com} fait figure de dinosaure à l'échelle du Web. Crée à la toute fin de l'année 2001, par un jeune ingénieur marocain travaillant en France\footnote{voir \url{http://lavieeco.com/news/portraits/mohamed-ezzouak-le-} \url{mre-qui-a-lance-
yabiladi-com-6596.html}}, le site s'inscrit dans la longue tradition des portails diasporiques en ligne. Dans son analyse de l'e-Diaspora Indienne\footnote{\url{http://e-diasporas.fr/wp/leclerc.html}}, E. Leclerc décrit avec précision le rôle de ces vastes sites protéiformes. La génèse de \textit{yabiladi.com} est similaire à celle de ces sites indiens apparus au début des années 2000 sous l'impulsion d'une diaspora influencée par le succès de certains de ses membres, nouveaux entrepreneurs de l'informatique faisant carrière aux USA \citep{leclerc_cyberespace_2012}. 

Un portail Web peut se voir comme une porte d'entrée ou un relai focalisé sur une thématique, une zone géographique ou régionale, une ethnie, une caste, un culte, ... et incarnant un double rôle : celui de l'intermédiaire entre une masse d'internautes et des grappes de sites inféodés à son autorité et celui du focalisateur d'une ou plusieurs communautés dont il se considère comme le représentant. 

\begin{figure*}
  \includegraphics[width=\linewidth]{graphics/portail}
  \caption{Dynamique de création d'un portail Web autour d'une communauté de sites}
  \label{fig:portail}
\end{figure*}

\noindent Un portail Web est donc dans la recherche constante d'une position d'autorité. Il commence par se mettre au service de sites existants en leur servant d'annuaire, en les pointant (Figure \ref{fig:portail}, a). Puis, une fois que les internautes l'ont identifié comme porte d'entrée unifiée, le portail peut prospérer sur le Web, fort du monopole acquis et bascule alors dans la seconde phase de son existence : la conservation de sa position et de son audience (Figure \ref{fig:portail}, b). Tout l'enjeu est alors de ne plus être seulement un site par lequel on passe, mais un espace où l'on reste. Pour ce faire, les portails s'adaptent aux exigences des internautes et proposent de nouveaux services (forum, petites annonces, radio, tv, ...), voire deviennent éditeur ou hébergeur de contenu. Un portail diasporique peut ainsi chercher à créer, en son sein, un véritable environnement dédié aux rencontres et aux discutions entre émigrés, un lieu de partage et d'entre-aide.  

Mais le lien qui nait entre l'internaute émigré et son portail Web devenu espace, ne suffit pas à comprendre la longévité de ces sites. Eux qui, contrairement aux blogs, ont su résister et s'adapter à l'arrivée des réseaux sociaux. Pour continuer à être la vitrine culturelle de sa communauté, le portail doit y insuffler du dynamisme en testant toutes sortes de nouvelles technologie créatrices de lien. De fait, un portail Web, peut être vu comme un parfait témoin de l'évolution du Web et de ses outils. En cela, la genèse de \textit{yabiladi.com} est tout à fait exemplaire. En interrogeant nos archives et en les associant aux collectages d'Internet Archives, nous créons la timeline de la figure \ref{fig:timeline} qui présente les transformations successives de \textit{yabiladi.com} au cours de son histoire : 

\begin{figure*}
  \includegraphics[width=\linewidth]{graphics/timeline}
  \caption{Timeline des évolutions successives de \textit{yabiladi.com}}
  \label{fig:timeline}
\end{figure*}

\begin{figure*}[hbtp]%
  \includegraphics[width=\linewidth]{graphics/catego-verti}
  \caption{Évolution des catégories du forum de \textit{yabiladi.com}, par date d'apparition}
  \label{fig:categories}
\end{figure*}


\noindent Yabiladi signifie littéralement \textit{Oh mon pays} (Ya biladi). Mis en ligne fin 2001, le portail inaugure, en 2002, une toute nouvelle section, un forum\footnote{\url{https://web.archive.org/web/20020615201721/http://www.yabiladi.com:80/forum/list.php?f=4}} contenant quatre espaces de discussions \textit{Général}, \textit{Femmes marocaines}, \textit{Investir au Maroc} et \textit{La corruption}. Il se présente alors comme le "\textit{Le premier portail destiné aux Marocains Résidant à l'Etranger (RME)!}" mettant en avant un annuaire complet et éditorialisé (classement par catégories) d'adresses Web marocaines\footnote{\url{https://web.archive.org/web/20020531015840/http://www.yabiladi.com:80/}}, ainsi que quelque rubriques mélangeant informations et divertissements. 

Suivre le fil des évolutions de \textit{yabiladi.com}, c'est remonter l'histoire du Web dans son intégralité. Avant 2005, le site est maintenu par sa propre équipe de développeurs PHP, puis dès 2006, un premier CMS est utilisé pour moderniser le forum\footnote{\url{https://web.archive.org/web/20070406192841/http://www.phorum.org:80}} et des éléments dynamiques de Javascript sont progressivement intégrés à partir de 2010. Les flux RSS\footnote{\url{https://web.archive.org/web/20051217073725/http://www.yabiladi.com:80/rubrik/rss.php}} sont ajoutés dès 2005 mais disparaissent 5 années plus tard; le search hiérarchique (à la Yahoo) est lui remplacé en 2007 par un search plein texte (à la Google). Et il faut attendre 2012 pour voir une redirection se faire vers les comptes Twitter et Facebook de \textit{yabiladi.com}, puis patienter jusqu'en 2014 pour pouvoir partager du contenu sur les réseaux sociaux. Des technologies maisons sont testées puis abandonnées au profit de solutions extérieurs : un service de chat \textit{Yabuzz}, un player radio intégré\footnote{\url{https://web.archive.org/web/20060717190039/http://www.yabiladi.com:80/radio/download.php}} et une plateforme de partage vidéo \textit{Babrio}\footnote{\url{http://babrio.com/}}.

À plusieurs reprises, le portail tente de proposer du contenu éditorialisé en complément de ses seuls articles Web. Une Web radio, une Web télé et une Web série\footnote{\url{https://web.archive.org/web/20081220014828/http://www.yabiladi.com:80/webserie/}, \url{https://web.archive.org/web/20101202113409/http://yabiladi.com/tv/}, \url{https://web.archive.org/web/20050411011853/http://www.yabiladi.com:80/radio/}} sont lancées. Deux magazines sont publiés~: le premier, un journal satirique et politique \textit{La Gachette du Maroc}\footnote{\url{https://web.archive.org/web/20050831073003/http://www.yabiladi.com:80/magazine/}} ne durera que 2 ans; le second \textit{Yabiladi Mag} plus orienté business et tourisme\footnote{\url{https://web.archive.org/web/20101118010916/http://www.yabiladi.com:80/mag}} sera édité pendant 4 années. 

Espace de rencontre en ligne\footnote{\url{https://web.archive.org/web/20030626013657/http://www.yabiladi.com:80/rencontres/}}, \textit{yabliadi.com} propose à ses membres de se croiser en dehors du site, via des cafés débats\footnote{\url{https://web.archive.org/web/20060209041225/http://www.yabiladi.com:80/cafe-debat.php}}. Mais ces moments d'échanges ne durent pas et les espaces d'analyses et d'opinions\footnote{\url{https://web.archive.org/web/20050711003647/http://www.yabiladi.com:80/sommaire-debat.html}}, présents depuis les débuts du portail, sont peu à peu fermés, remplacés par des sections plus généralistes : \textit{Divertissement}, \textit{Vie pratique}, \textit{Famille}, ... 

Le site évolue et se transforme, 6 changements visuels et structurels majeurs sont opérés successivement (Figure \ref{fig:timeline}, "\textit{nouveau CSS}"), impactant notamment le forum. Si l'organisation locale de cette section reste la même (les messages postés y sont triés par date de publication et associés à un sujet unique, formant ce que l'on appelle des \textit{threads}\footnote{Suite de messages inféodés à un même sujet de discussion}), sa structure générale, quant à elle, est régulièrement bouleversée. Les catégories de discussions qui prédéfinissent l'ordre et l'agencement des threads, changent, disparaissent ou sont réintégrées au grès des nombreuses mutations du site. 

En 16 années d'existence, 81 catégories successives viennent, une à une, donner une nouvelle orientation au forum (Figure \ref{fig:categories}). Des thématiques entières apparaissent (\textit{Grossesse, Maternité, Vie de Famille, ...}) ou perdent en influence (\textit{Opportunité au Maroc, Forum étudiant, Solidarité \& associations, ...}). Seules les catégories généralistes et légères : \textit{Général}, \textit{Halka}(ie: blagues) et \textit{Vacances} restent inchangées, alors que d'autres sont en perpétuelles mutations : \textit{Islam} devient \textit{Islam et religions}, puis \textit{Islam et pensées religieuses} et enfin \textit{Apprendre l'Islam}. L'orga\-nisation du forum se stabilise en 2010 et compte aujourd'hui 25 catégories différentes. Chaque jour, ce sont ainsi plusieurs dizaines de sujets de discussions et plusieurs de centaines de messages qui sont ouverts et échangés entre les membres du forum. 

\subsection{La manifestation du 20 février 2011}

\noindent Le \textbf{Printemps Arabe} est une suite de protestations et de soulèvements des populations du monde Arabe (Maghreb, Proche \& Moyen-Orient ), débutée en Décembre 2010. L'ampleur et la finalité des contestations varie d'un pays à l'autre, mais la contiguïté de leurs déclenchements contribue à véhiculer l'image d'un embrasement spontané de la région, traversée dans son intégralité par une vague révolutionnaire (Table \ref{tab:printemps}).  

\begin{table*}
  \label{tab:printemps}
  \begin{tabular}{lcl}
    \toprule
    pays&déclenchement des contestations&finalités\\
    \midrule
Tunisie&17 décembre 2010&fuite du président et changement de régime\\
Algérie&28 décembre 2010&promesses de réformes\\
Yémen&29 décembre 2010&départ du président\\
Jordanie&14 janvier 2011&démission du gouvernement\\
Mauritanie&17 janvier 2011&manifestations\\
Oman&17 janvier 2011&réforme constitutionnelle\\
Arabie saoudite&21 janvier 2011&réformes sociales\\
Liban&24 janvier 2011&manifestations\\
Égypte&25 janvier 2011&départ du président et changement de régime\\
Syrie&26 janvier 2011&guerre civile\\
Palestine&28 janvier 2011&manifestations\\
Maroc&30 janvier 2011&réforme constitutionnelle\\
Soudan&30 janvier 2011&manifestations\\
Djibouti&1er février 2011&manifestations\\
Bahreïn&14 février 2011&manifestations\\
Irak&10 février 2011&manifestations\\
Libye&13 février 2011&mort du dirigeant et guerre civile\\
Somalie&13 février 2011&manifestations\\
Koweït&18 février 2011&manifestations\\
  \bottomrule
  \end{tabular}
  \bigskip
  \caption{Liste des pays touchés par le Printemps arabe (source: \url{https://fr.wikipedia.org/wiki/Printemps_arabe})}
\end{table*}

\noindent Le printemps Arabe débute le 17 Décembre 2010, en Tunisie, dans la ville de Sidi Bouzid. Suite à la confiscation répétée de son matériel par l'administration locale, le vendeur ambulant M. Bouazizi, s'immole devant le siège du gouvernorat. Il décèdera deux semaines plus tard des suites de ses blessures. La portée de son geste trouve immédiatement un écho parmi la population et devient le déclencheur d'une vague de protestations inédite dans tout le pays. Le peuple tunisien descendant dans la rue, scandant son mécontentement d'une situation économique devenue invivable. Au ras le bol généralisé, contre le chômage et l'austérité, s'ajoute la soif de renouvellement démocratique d'une jeunesse tunisienne qui cristallise sa colère contre la personne de Z. Ben Ali, dirigeant en place depuis 1987 \citep{salmon_29_2016}. L'usage déterminant d'Internet, des réseaux sociaux et des téléphones portables, comme moyens directs d'organisation collective, permet au mouvement de gagner rapidement en ampleur \citep{lotan_arab_2011, khondker_role_2011}.    

La fuite de Ben Ali, un mois plus tard le 14 Janvier 2011, rapportée sur le Web et amplifiée par des canaux de diffusion comme Al Jazeera entraine la contagion des révoltes aux pays voisins marquant, de fait, le premier temps fort du Printemps Arabe. Et si la démission de H. Moubarak, le 11 Février suivant, après plusieurs semaines d'occupation populaire de la place Tahrir au Caire, est vécue comme une nouvelle victoire démocratique, les assauts contre-révolutionnaires à venir donneront un coup d'arrêt aux protestations. Courant 2011 et 2012, les soulèvements sont réprimés (Égypte, Yémen) et, pour certains, tournent à la guerre civile (Libye, Syrie). \\

\noindent Tout au long de cette période, le Maroc cultive une forme d'\textit{exception marocaine} vis à vis du Printemps Arabe. Suite à une première manifestation en demi teinte (le 30 Janvier 2011), de jeunes militants marocains des droits de l'homme voient, dans les mobilisations tunisiennes et égyptiennes, l'occasion de redonner de l'élan à d'anciennes revendications démocratiques. L'analyse stratégique de ces événements, les amène à déduire que les victoires de leurs voisins sont, d'abord, le fait de la mise en avant d'une jeunesse dépolitisée, évoluant sans leader clairement identifié et agissant sur la base d'actions spontanées et innovantes (sit-in, réseaux sociaux, comités locaux, ...) \citep{bennani-chraibi_dynamique_2012}. Partant de ce constat, un appel est alors lancé pour un grand rassemblement national le 20 février 2011, baptisé \textit{journée de la dignité}. Dans la foulée, le \textit{mouvement du 20 février} (M20F) voit le jour pour coordonner et incarner les différents aspects de la contestation à venir. Contrairement à ses voisins, la mobilisation marocaine n'est donc pas née d'un embrasement soudain, mais d'une construction méthodique, orchestrée par des acteurs hétérogènes.

En effet, plusieurs associations et organisations rejoignent rapidement le mouvement\footnote{\url{https://www.yabiladi.com/articles/details/4596/marche-fevrier-maroc} \url{-associations-defense.html}} qui déclenche déjà l'enthousiasme de militants historiques de la cause démocratique, comme le blogueur \textit{larbi.org}\footnote{\url{https://frama.link/cTMRPS5a}}. Une préavis unitaire est déposé le 17 février, agrégeant derrière le slogan "\textit{Liberté, Dignité, Equité}" une multitude de revendications nationales ou régionales : pour une justice indépendante, pour la reconnaissance de l'amazigh\footnote{Langue berbère } comme langue officielle et pour la dissolution du gouvernement, en passant par la lutte contre la corruption, contre la mauvaise gestion administrative et communale et contre le chômage \citep{bennafla_maroc_2011}. Bien que l'objectif affiché ne soit pas le renversement de la monarchie, les signataires n'en réclament pas moins de grands changements démocratiques et une modification en profondeur de la constitution marocaine, pour "\textit{un roi qui règne mais ne gouverne pas}"\footnote{\url{https://frama.link/V7R8y1SH}}.

Relayée localement par les comités du M20F, la mobilisation se prépare aussi, et avant tout, sur la toile : un site Web est mis en ligne et associés à des comptes Twitter et de groupes Facebook\footnote{\url{https://web.archive.org/web/20110220014401/http://mamfakinch.posterous.com/}, \url{https://twitter.com/mamfakinch}, \url{https://www.facebook.com/Movement20}} qui tiennent les internautes informés grâce aux hashtags \textit{\#Feb20 \& \#20Fev}. La diaspora s'empare elle aussi du sujet via les blogs et les canaux d'informations du M20F. Des manifestations de soutient sont organisées, notamment à Paris\footnote{\url{https://youtu.be/g7muGDOGoto}} et à Bruxelles. 

Mais une campagne de contre-communication est lancée en parallèle par le gouvernement. Les membres du M20F sont ciblés personnellement sur les réseaux, dénigrés et intimidés. Anti et pro 20 février s'affrontent par vidéos interposées sur Youtube \footnote{\url{https://www.youtube.com/user/mouvement20fevrier/feed} et \url{https://www.youtube.com/user/MoroccanYouth/videos}} et Dailymotion. À deux jours de l'échéance, le M20F publie une dernière vidéo, où de jeunes militants appellent, face caméra, à descendre dans la rue, incarnant à visage découvert l'élan d'une jeunesse marocaine qu'ils souhaitent voir défiler massivement\footnote{\url{https://youtu.be/6Y_J-2S_1m8}}.

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/morocco}
  \vspace*{0.2cm}  
  \caption{Principaux foyers de contestation, lors des manifestations du 20 février 2011 au Maroc (source : \url{ https://globalvoices.org/2011/02/20/morocco-across} \url{-the-nation-demonstration/}) }
  \label{fig:morocco}
\end{marginfigure} 

Le 20 février 2011, entre 37,000 (selon le ministère Marocain de l'intérieur) et 100,000 personnes défilent dans 53 localités différentes (Figure \ref{fig:morocco}). Si le succès de la journée est mitigé d'un point de vue quantitatif, le M20F marque symboliquement les esprits et veut pousser plus loin la mobilisation, prévoyant de redescendre dans la rue les \textit{20} de chaque mois à venir. Mais le gouvernement Marocain et le roi Mohammed VI prennent, à titre préventifs une série de mesures visant à apaiser les mécontentements \citep{desrues_mouvement_2012}. Le 9 Mars, Mohammed VI prononce un discours à la télévision et annonce une modification de la constitution, dont les détails\footnote{\url{https://frama.link/oB6K7-hJ}} sont donnés le 17 Juin : le premier ministre sera dorénavant désigné par des élections, mais le roi gardera le pouvoir de dissoudre le parlement qui, de son côté, gagne en indépendance. Par ailleurs, la langue berbère devient, comme l'arabe, langue officielle d'état. Ce faisant, le régime marocain arrive à gérer la menace contestataire et étouffe une possible amplification du mouvement en reprenant à son compte l'agenda politique des événements. 

La nouvelle constitution est approuvée par référendum le 1er Juillet 2011, malgré l'appel au boycott\footnote{\url{https://youtu.be/irbHL8Io--Q}} lancé par le M20F qui n'y voient que des changements de façade. Quelques manifestations émaillent le reste de l'année, mais le M20F penne à mobiliser et à retrouver sa légitimité. Lors des élections législatives de Novembre 2011, les conservateurs du Parti de la justice et du dévéloppement (PJD) arrivent en tête des suffrages, mettant ainsi un terme au volet marocain du Printemps Arabe.

\subsection{Topologie d'un site Web}

\noindent Comment évolue un site ? Comment s'étend-il sur le Web ? Comment naissent et disparaissent certaines de ses sections ? Peut-on voir un site croitre, à mesure que de nouvelles pages s'ajoutent à son arborescence~? 

Ces questions sortent du cadre stricte de notre étude sur le forum de \textit{yabildai.com}, mais nous souhaitons tout de même nous y attarder ici, afin d'ajouter une nouvelle brique à nos outils d'exploration. En effet, le Web est tout autant un flux continu d'informations qu'un espace en pleine expansion, dont nous voulons maintenant retracer la genèse (à l'échelle d'un site tout du moins). Aussi, plusieurs pistes de réflexion s'offrent à nous : Qu'est ce qu'un site Web, d'un point de vue topographique ? Comment le représenter ? Comment traduire ses dynamiques internes ? Comment rendre compte de l'évolution de ses diverses ramifications, partant d'un corpus d'archives Web ? Notre corpus suffit il, à lui seul, à couvrir toute la ramure d'un site ? 

Sur ce point, M. Toyoda et M. Kitsuregawa proposent de se baser sur les URLs d'un site pour en reconstituer le graphe \citep{toyoda_extracting_2003,toyoda_system_2005}. Chaque élément d'une URL (compris entre deux "\textit{/}") formant un nœud particulier du graphe, dont l'évolution est ici retranscrite par une série de captures successives. On appelle cette méthode, une série temporelle de graphes. C'est l'approche que nous avions utilisé pour notre prototype \textit{en oursin} (Figure \ref{fig:prototypes}, a), présenté en section \ref{sec:3_constituer}. Mais, avec un peu de recul, il nous semble difficile de comparer visuellement de telles séries, surtout lorsque les sites (comme \textit{yabiladi.com}) se révèlent être très verbeux en URLs à afficher. De plus, la période de capture et de représentation reste sujette à discussion et peut varier sensiblement d'un site à l'autre.

À l'opposé, la méthode employée par B. Fry, dans son application Anemone\footnote{\url{http://benfry.com/anemone/}} \citep[p.76-82]{fry_organic_2000}, permet de saisir de manière dynamique l'évolution d'un site, vu ici comme un être vivant en pleine croissance. Le système, conçu par Fry, part de la première page archi\-vée d'un site Web donné, à laquelle il ajoute toutes les $n$ secondes une nouvelle ramification, collant ainsi à l'arborescence du site telle qu'elle a été collectée (figure \ref{fig:anemone}) :

\begin{figure*}
  \includegraphics[width=\linewidth]{graphics/anemone}
  \caption{Fonctionnement du système de visualisation Anemone, pour un site fictif \textit{inter.net}}
  \label{fig:anemone}
\end{figure*}

\noindent Mais, si l'idée de croissance est bien restituée par Anemone, la spatialisation (en haut, en bas, à gauche, ...), de chaque branche est, quant à elle, codée pour suivre un comportement aléatoire : la structure du site reste la même, mais l'agencement local des pages varie à chaque relance de l'application. Ce procédé écarte malheureusement toute possibilité d'étude répétée et systématique. De plus, le temps est ici pensé comme une mécanique de visualisation (le temps sert à mettre à jour l'Anemone) et non comme une véritable dimension d'analyse.

\begin{figure*}
  \includegraphics[width=\linewidth]{graphics/minard}
  \caption{C.J. Minard, (1869), Carte figurative des pertes successives en hommes de l'Armée Française dans la campagne de Russie 1812-1813}
  \label{fig:minard}
\end{figure*}

\noindent Sur ce point, en matière de visualisation des dimensions spatio-tempo\-relles d'un processus historique, la célèbre \textit{Carte figurative des pertes successives en hommes de l'Armée Française dans la campagne de Russie 1812-1813} de C. J. Minard reste la meilleure source d'inspiration. Elle permet de suivre le trajet (tracé beige) de l'armée napoléonienne et ses pertes humaines (épaisseur du tracé), à travers la Russie (indications géographique), au cours de l'hiver 1812-1813 (temps en abscisse). Deux dimensions (temps et espaces) servent ici de cadre à l'étude de l'évolution d'une troisième (le nombre d'hommes). \\ 

\noindent Au regard de ces exemples, il nous semble judicieux de choisir de représenter un site comme un arbre d'URLs, se déployant depuis sa racine (la première page mise en ligne) et grandissant à mesure que de nouvelles pages y sont publiées. En effet, un arbre croît suivant une direction donnée qui peut, dans notre cas, être associée à un axe temporel, placé en abscisse à la manière d'une timeline \ref{fig:topo}. 

\begin{figure}
  \includegraphics[width=\linewidth]{graphics/topo}
  \caption{Proposition de visualisation de l'évolution de la structure d'un site fictif \textit{inter.net} dans le temps}
  \label{fig:topo}
\end{figure}

\noindent Chaque URL du site représente ici une branche de l'arbre et les pages Web font office de feuilles (Figure \ref{fig:topo}, points rouges). L'abscisse d'une URL est déterminée suivant la date de création (dans le meilleur des cas) ou de téléchargement (dans le pire des cas) de la page qu'elle adresse. Pour positionner les embranchements mécaniques\footnote{les portions d'URL entre deux "\textit{/}" ne correspondant à aucune page réelle} de notre arbre, nous leur attribuons arbitrairement une date médiane (Figure \ref{fig:topo}, $t(p_2) - t(p_1)$). L'ordonnée d'une URL est, quant à elle, fonction de son âge (du haut vers le bas, de la plus ancienne à la plus récente) et lui sert d'encrage autour de l'embranchement parent (Figure \ref{fig:ordonnee}). L'épaisseur d'une branche de l'arbre d'URLs est déterminée par son nombre d'enfants. La taille des marqueurs représentant les pages peut être ramenée à une mesure donnée, comme par exemple, un nombre de messages postés.

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/ordonnee}
  \vspace*{0.2cm}  
  \caption{Positionnement de 3 pages (rouge) autour d'un embranchement parent (bleu)}
  \label{fig:ordonnee}
\end{marginfigure} 

Ceci étant défini, nous développons alors un système\footnote{Open-source et téléchargeable ici \url{https://github.com/lobbeque/archive-viz/tree/master/stayingAlive}}, permettant de récupérer, depuis notre moteur d'exploration et depuis le corpus d'Internet Archive, l'ensemble des URLs archivées d'un site donné. En effet, afin de garantir une plus grande couverture de l'étendue d'un site nous choisissons de coupler plusieurs sources de données. Nous implémentons cette méthode et construisons une interface de visualisation Web. L'utilisateur peut y zoomer à volonté et sélectionner, s'il le souhaite, le détail d'une branche ou d'une feuille (Figure \ref{fig:reve}).

\begin{figure*}
  \includegraphics[width=\linewidth]{graphics/reve}
  \caption{Capture d'écran de l'évolution topologique de la section \textit{Interprétation des rêves, roqya, djinn} du forum de \textit{yabiladi.com}}
  \label{fig:reve}
\end{figure*}

\subsection{Revenir au 20 février}

\noindent La mobilisation de la jeunesse marocaine sur les réseaux sociaux a servi d'amorce aux manifestations du 20 février 2011. Le temps d'une journée, les plateformes Twitter et Facebook ont prolongé les rues de Casablanca, de Rabat et de Marrakech via les hashtags \textit{\#Feb20 \& \#20Fev} (Section \ref{sec:5_blogs}). Porté par une communication inédite, l'élan de ce mouvement a atteint la communauté des marocains de l'étranger, qui s'est orga\-nisée pour apporter son soutien \citep{desrues_mouvement_2012}. Parallèlement à ces événements, le portail en ligne \textit{yabiladi.com} s'est imposé comme un point de rassemblement et de discussion privilégié par la diaspora Marocaine sur le Web. 

Partant de là, notre exploration se tourne maintenant vers la masse des archives de \textit{yabiladi.com}. Dans quelle mesure, la mobilisation du 20 février 2011, a-elle pu impacter le quotidien de ce site et de ses utilisateurs ? Y-a-il eu une forme d'organisation collective, en amont de la manifestation ? Une communauté, même éphémère, de soutiens ou de détracteurs a-elle pu y voir le jour ?

Aussi, commençons par définir notre espace d'exploration. Nous partons d'un site Web unique, \textit{yabiladi.com}, dont nous possédons des collectages hebdomadaires, allant de Mars 2010 à Septembre 2014. Les corpus d'Internet Archive peuvent, ici, servir de données d'appoint ou complémentaires. \textit{Yabiladi.com} est un site vaste, possédant plusieurs sections, dont un forum divisé lui même en une dizaine ou une vingtaine de catégories (Figure \ref{fig:categories}). Chaque catégorie englobe plusieurs threads de messages, hébergés sur des pages Web dédiées. Si, d'un point de vue temporel, notre cible est connue, il s'agit de l'avant et de l'après 20 février 2011, le nombre de messages ou de threads s'y référant reste pour le moment inconnu. L'idée est donc : \textbf{1)} d'identifier les messages parlant de la mobilisation, \textbf{2)} de remonter aux utilisateurs qui les ont publiés, \textbf{3)} de voir si ces membre forment une communauté. \\   

\noindent Tout d'abord, nous construisons un arbre d'URLs qui servira de support à nos visualisations. Nous partons des 2,683,928 pages archivées du corpus e-Diasporas et des 887,981 pages collectées par Internet Archives, pour proposer une représentation, la complète possible, de la structure du site et de son évolution. Les données venant de nos archives sont associées, si possible, à une date de création; les pages fournies par Internet Archives sont, elles, datées par rapport à leur collecte. En cas de doublon, nous privilégions la datation la plus précise (Table \ref{tab:datation_2}).

Les archives DAFF de \textit{yabiladi.com} sont ensuite envoyées dans notre moteur d'exploration qui les fragmente et les indexe. Là, un filtre est appliqué, pour ne conserver que les pages appartenant à la seule section forum du site : celles contenant la mention "\textit{/forum/}" dans leurs URLs. Passé cette étape, nous retenons un groupe de 109,534 pages Web, associées à une date de création unique et segmentées en 422,906 fragments Web.

Mais, il ne nous semble pas nécessaire, d'analyser toutes les catégories du forum. Les sections \textit{Général} et \textit{Actualités} étant plus susceptibles de renfermer des threads et des messages associés au 20 février. Or, comme nous savons que la section \textit{Actualités} a subi de nombreux changements au cours du temps \ref{fig:categories} : \textit{Actualités}, \textit{Actualités du Maroc}, \textit{Actualités Marocaines},  \textit{Actualités internationales} et \textit{Actualité du Maroc et du Monde}, nous choisissons de les agréger en une seule et même section renommée \textit{Actualités}. 

Par ailleurs, en se basant sur la structure des URLs du forum, il est possible d'en filtrer les pages par catégorie. En effet, depuis la création du forum en 2002, les URLs suivent toutes le même schéma~:

\begin{figure}
  \includegraphics[width=\linewidth]{graphics/filtre}
\end{figure}  

\noindent Par exemple, la \textit{category\_id} de la section \textit{Général} est $1$, information qui peut être facilement extraite. Nous appliquons ainsi ce filtre et obtenons en sortie de notre moteur les résultats de la table \ref{tab:forum}.

\newpage

\begin{table}
  \label{tab:forum}
  \begin{tabular}{lrr}
    \toprule
    & Général & Actualités\\
    \midrule    
    Nombre de threads&17,025&20,553\\
    Nombre de messages&352,231&328,965\\
    Nombre de membres&19,745&10,819\\
	Premier message&Janv. 2002&Janv. 2002\\    	
	Max. messages par membre&5,019&6,041\\	
	Max. threads par membre&1,316&2,057\\
	Messages moyens par thread&21&16\\    
  \bottomrule
  \end{tabular}
  \bigskip
  \caption{Statistique des archives des sections \textit{Général} et \textit{Actualités} de \textit{yabiladi.com}}
\end{table} 

\noindent Pour construire cette table, nous demandons à notre moteur de nous retourner les fragments Web ayant la forme d'un message générique, publié sur le forum, soit : un nom d'utilisateur, une date d'édition et un ensemble d'éléments textuels postés au fil d'un thread. Dans la suite de cette section, et par abus de langage, fragments Web et messages publiés, seront confondus et utilisés de manière équivalente. Aussi, pour identifier les messages\footnote{Le forum est écrit en majorité en français} se référant aux événements du 20 février 2011, nous utilisons la recherche plein texte de notre moteur et partons de l'ensemble de mots clés suivants :

\begin{figure}
  \includegraphics[width=\linewidth]{graphics/20fev}
  \caption{Mot clés associés à la manifestation du 20 février 2011}
  \label{fig:mot}
\end{figure}  

\noindent Nous agrégeons, ensuite, par thread les fragments retournés. Après une validation manuelle, nous n'en conservons que 12, selon qu'ils traitent directement du 20 février ou de ses conséquences\footnote{réception par la diaspora, suites à donner au mouvement, discréditation d'un des organisateurs, ...}. Nous présentons, ci dessous, la liste des différents titres (ou sujets) de ces 12 threads~:

\begin{figure}
  \includegraphics[width=\linewidth]{graphics/20fev-topic}
  \caption{Titre des 12 threads directement associés au 20 février}
  \label{fig:20fevTopic}
\end{figure}  

\noindent Nous appelons $V_0$ ce groupe initial de 12 threads, consistant en un ensemble de 196 messages, écrits par un panel de 94 auteurs uniques nommé $E_0$.

\begin{figure*}[hbtp]%
  \includegraphics[width=\linewidth]{graphics/yabi-1}
  \caption{Évolution de la structure de \textit{yabiladi.com} et ensemble de threads $V_1$}
  \label{fig:yabi-1}
\end{figure*}

\noindent L'idée est maintenant d'élargir ce premier groupe aux messages écrits en amont et en aval de la manifestation. Pour ce faire, nous utilisons $E_0$ comme nouveau point de départ et filtre de sélection. Aussi, nous demandons à notre moteur de nous retourner tous les threads du forum dans lesquels au moins deux auteurs de $E_0$ ont publiés un message. En effet, c'est un collectif d'utilisateurs que nous cherchons ici à étudier. Il nous faut donc découvrir s'ils se connaissaient avant la manifestation et s'ils ont continué à collaborer par la suite, notamment en alimentant les même threads à plusieurs mains. Au final, nous obtenons un nouveau groupe de 343 threads appelé $V_1$. La figure \ref{fig:yabi-1} donne à voir la répartition de ces threads (points rouges) dans le temps, parmi les pages archivées du forum de \textit{yabiladi.com}.

\subsection{Des trajets de contributeurs}

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/trajet}
  \vspace*{0.2cm}  
  \caption{Trajet d'un utilisateur ayant posté des messages (bleu) sur deux threads (rouge)}
  \label{fig:trajet}
\end{marginfigure} 

\noindent Il faut ajouter à la figure \ref{fig:yabi-1} une nouvelle information : la trace du trajet d'un utilisateur, naviguant d'un thread à l'autre du forum. En effet, en utilisant le fragment Web (Section \ref{sec:4_derrida}) comme unité principale d'exploration, nous souhaitons réintégrer de l'humain au cœur des archives Web, revenir aux gestes de celles et ceux qui alimentent quotidiennement les sites collectés. Aussi, en nous servant des noms d'auteurs associés aux messages que nous manipulons comme moyen d'identification, nous pouvons afficher de manière dynamique le trajet d'un membre donné (Figure \ref{fig:trajet}). La figure \ref{fig:yabi-2} présente ainsi l'un de ces trajets individuels, agrégé ici au niveau des threads. 

Mais bien plus qu'un individu isolé, c'est une communauté que nous cherchons d'abord à caractériser. Or, le fragment Web nous permet justement de conjuguer le devenir historique de multiples lignes processuelles (Section \ref{sec:4_derrida}), afin de raisonner autour de moments singuliers, à l'image du 20 février 2011. Nous définissons ainsi l'ensemble $E_1$ comme l'agrégation des trajets empruntés par le groupe d'utilisateurs $E_0$ et le graphe $G=(V_1,E_1)$ comme un réseau de threads liés entre eux par les trajets de contributeurs communs (Figure \ref{fig:graphe_g}). 

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/graphe_g}
  \vspace*{0.2cm}  
  \caption{Le graphe $G=(V_1,E_1)$ pour $V_1$ contenant quatre threads de messages}
  \label{fig:graphe_g}
\end{marginfigure} 


Nous affichons donc l'ensemble des trajets d'utilisateurs, extraits de nos archives et associés au 20 février. Mais afin de faciliter l'analyse de ces chemins, nous les représentons en utilisant la méthode \textit{edge bundling} \citep{holten_hierarchical_2006}, qui permet de modifier localement la courbure d'un lien dans un graphe, pour le rapprocher visuellement de ces voisins. 

La figure \ref{fig:yabi-3} donne à voir ce graphe $G$ et la manière dont il se construit au cours du temps. Pour clarifier, une dernière fois, la lecture de cette figure, nous dissimulons l'ossature de l'arbre d'URLs de \textit{yabiladi.com} et n'affichons que les trajets $E_1$ (tracés noirs) parmi les threads $V_1$ (points rouges).

\begin{figure*}[hbtp]%
  \includegraphics[width=\linewidth]{graphics/yabi-2}
  \caption{Exemple de trajet individuel (tracé bleu clair) parmi 66 threads du forum \textit{yabiladi.com}}
  \label{fig:yabi-2}
\end{figure*}

\begin{figure*}[hbtp]%
  \includegraphics[width=\linewidth]{graphics/yabi-3}
  \caption{Trajets agrégés $E_1$ de l'ensemble des contributeurs des threads $V_1$ du forum \textit{yabiladi.com}}
  \label{fig:yabi-3}
\end{figure*}

\noindent Une première analyse de la figure \ref{fig:yabi-3}, révèle l'apparition de plusieurs \textit{points de fixations}. Ce sont des points précis du graphe $G$ où arcs et nœuds semblent converger tous au même endroit et même moment. Le plus visible de ces points, où le nombre de threads est le plus important et où les trajets sont les plus densément  concentrés, correspond aux voisinages directs du 20 février 2011 (Figure \ref{fig:yabi-3}, flèche rouge). Plus précisément,  ce sont $25\%$ des 343 threads de $V_1$ qui sont crées entre Janvier et Février 2011.

Cette première observation fait sens, au regard, notamment de la construction de notre groupe de départ $V_0$, centré sur la mobilisation marocaine. Mais, nous voyons également que les manifestations du 20 février ont agrégé, autour d'elles, une communauté de membres du forum qui communiquaient déjà en entre eux, en amont des événements (un autre point de fixation est visible en 2009 par exemple, Figure \ref{fig:yabi-3}).

Aussi, si nous divisons maintenant la figure \ref{fig:yabi-3} en deux parties, séparées par le 20 février et appelées respectivement parties \textit{pré-mouvement} et \textit{post-mouvement}, nous voyons que la partie pré-mouvement prend la forme d'un sous-graphe de $G$, clairsemé et étalé sur une période relativement importante, allant de début 2004 à février 2011.

\begin{figure}
  \includegraphics[width=\linewidth]{graphics/user-first}
  \caption{Répartition des utilisateurs du groupe $E_0$ par date de premier message}
  \label{fig:user-first}
\end{figure}  

\noindent Sur l'ensemble du groupe initial d'utilisateur $E_0$, $62\%$ écrivent leur premier message sur le forum durant la période pré-mouvement (Figure \ref{fig:user-first}). En particulier, $20\%$ d'entre eux, sont apparus sur \textit{yabiladi.com} en 2007-2008, suivant une vague de nouveaux arrivants\footnote{Ces auteurs ont notamment contribué au thread "\textit{Accueil des nouveaux yabis ! Marhaba !}"}. Ces premiers membres se sont alors réunis et retrouvés pour débattre ensemble dans des threads dédiés au 20 février et ont, semble-il, continué à communiquer entre eux après ces événements. En effet, les $38\%$ restant, du groupe d'utilisateurs $E_0$, ont contribué d'abord et en premier aux threads directement liés au 20 février. Avant cette date, ils n'apparaissent pas dans les archives de \textit{yabiladi.com}. Nous les retrouvons, néanmoins dans la partie post-mouvement de notre graphe. 


Ces observations sont ici purement topographiques et basées sur des corrélations temporelles, nous ne rentrons pas encore dans le détail des conversations. Mais à ce niveau, nous pouvons déjà affirmer être face à un double comportement : \textbf{1)} d'anciens utilisateurs du forum convergent collectivement vers les threads mentionnant les manifestations \textbf{2)} de nouveaux membres débarquent directement sur \textit{yabiladi.com} pour contribuer aux débats post-mouvement. En revanche, tous disparaissent et s'évanouissent soudainement du site au début de l'année 2012.

\subsection{Vers un embrasement}

\noindent Afin de mieux saisir la dynamique de convergence des anciens et des nouveaux utilisateurs du forum autour des événements du 20 février 2011, nous souhaitons maintenant affiner notre analyse du graphe $G$. Avec la figure \ref{fig:yabi-3}, la spatialisation du graphe, c'est à dire la manière dont nous le représentons, est uniquement fonction de la date à la laquelle chaque thread de $V_1$ a été crée. Mais d'autres types de spatialisation existent, d'autres manière de reprocher ou d'éloigner visuellement les nœuds d'un graphe peuvent être employées. 

Nous reprenons donc notre graphe et l'envoyons dans le logiciel de visualisation gephi\footnote{\url{https://gephi.org/}} où nous choisissons de le spatialiser en utilisant, cette fois ci, la méthode \textit{force atlas} \citep{jacomy_forceatlas2_2014}. Appartenant à la famille des algorithmes dits \textbf{force-directed}\footnote{\url{https://en.wikipedia.org/wiki/Force-directed_graph_drawing}}, force atlas suit une approche purement topographique, où les nœuds  
d'un graphe sont supposés être chargés d'une force de répulsion et où les arrêtes sont elles chargées de les retenir (Figure \ref{fig:force_atlas}). L'algorithme itère sur l'ensemble du graphe, déplaçant les nœuds et les rapports de force jusqu'à trouver un point d'équilibre global.

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/force_atlas}
  \vspace*{0.2cm}  
  \caption{Principe de base de Force Atlas, les noeuds d'un graphe sont chargés d'une force de répulsion et les arcs d'une force d'attraction}
  \label{fig:force_atlas}
\end{marginfigure} 

La figure \ref{fig:cluster-yabiladi} présente le graphe $G$ nouvellement spatialisé. De la même manière qu'avec la figure \ref{fig:yabi-3}, nous voyons que $G$ suit à nouveau une distribution longitudinale, la graphe est étiré confirmant, de fait, sont aspect temporel. 

Les membres du forum contribuent d'abord et avant tout aux threads nouvellement crées. Ils ne reviennent que dans de très infimes cas participer à des discussions passées, construisant leur trajet d'auteur d'un instant $t_i$ vers un instant $t_{i+1}$, sans retour arrière. Le forum est ainsi vécu comme un flux continu de nouveaux threads, demandant des utilisateurs qu'ils entretiennent une présence régulière sur le site. Le temps moyen espaçant deux threads auxquels participe un auteur est ainsi de 53 jours, la valeur médiane étant de 8 jours. Ils sont, en cela, peu nombreux à marquer de longues pauses entre deux interventions, seuls $20\%$ des trajets durent plus d'un mois, l'intervalle maximum étant de 6 ans et 2 mois.   

\begin{figure*}
  \includegraphics[width=\linewidth]{graphics/gephi}
  \caption{Spatialisation du graphe $G$ par \textit{force atlas} et clustering des threads par classes de modularité}
  \label{fig:gephi}
\end{figure*} 

\noindent Dans un second temps, nous appliquons à notre graphe un algorithme de clustering par classes de modularité\footnote{\url{https://fr.wikipedia.org/wiki/Modularité\_(réseaux)}} \citep{blondel_fast_2008}. Nous souhaitons grouper entre eux les threads du forum et ainsi analyser, sous une nouvelle perspective, les points de fixations précédemment révélés (Figure \ref{fig:yabi-3}). 

La \textbf{modularité} d'un graphe est un score $\in [-1,1]$ qui cherche à traduire le rapport entre le nombre d'arêtes réel et le nombre d'arêtes espéré (placées de manière aléatoire) d'un groupe de nœuds. Plus ce score est élevé, plus le groupe tend à se rapprocher topographiquement de la structure d'une communauté (Figure \ref{fig:modularite}).

\begin{marginfigure}%
  \includegraphics[width=\linewidth]{graphics/modularite}
  \vspace*{0.2cm}  
  \caption{Exemple de clustering d'un graphe en 4 classes de modularité (couleurs)}
  \label{fig:modularite}
\end{marginfigure} 

\begin{table*}
  \label{tab:threads}
  \begin{tabular}{clcl}
    \toprule
    Cluster & Thread & Degré & Date de création\\
    \midrule    
	1&des espions a yabiladi?&8&25-05-2009\\
	1&Un nouveau forum ? Suggestions ?&9&04-05-2009\\
	1&adopter un chat&10&14-07-2009\\
	1&Assumez-vous vos écrits depuis votre inscription?&10&26-07-2009\\
	1&Accueil des nouveaux yabis ! Marhaba !&16&25-06-2007\\
    \midrule 	
	2&Les arabes et les apparences....&9&13-07-2010\\
	2&Franciser son prénom&10&20-06-2010\\
	2&correction orthographique...&12&24-08-2010\\
	2&Double triple pseudo etc...&14&08-02-2010\\
	2&Quels sont les 3 mots que vous associez à la France?&15&13-08-2010\\
    \midrule 	
	3&les homo sont musulmans comme les autres&10&15-10-2010\\
	3&Mais qu'est ce qui retiens certains en Occident????&10&26-09-2010\\
	3&Fautes d'orthographe: Catastrophique&10&19-10-2010\\
	3&porter une barbe au maroc...terrorisme ?&12&15-09-2010\\
	3&Plus beau pays du monde??&12&03-09-2010\\
	\midrule 
	4&Bonne Année 2011 Meilleurs Voeux&12&28-11-2010\\
	4&Elections miss and mister yabi 2011.&14&04-11-2010\\
	4&Oh mon Dieu ! Qu'il est beau mon pays&15&16-11-2010\\
	4&L'accueil des MRE au Maroc&15&21-11-2010\\
	4&marocains et algériens&15&29-10-2010\\
    \midrule 	
	5&Arabe et Tamazight langues officielles !&12&28-01-2011\\
	5&Photos Des Manifestations&13&21-01-2011\\
	5&2 millions de manifestants a la place jame3 lfna&14&20-01-2011\\
	5&Le Couscous est il un plat raffiné ou populaire&14&16-01-2011\\
	5&Qui assistera le 20 février au Maroc....???&30&15-01-2011\\
    \midrule 
	6&Kadafi est juif&11&28-02-2011\\
	6&Images d'Algérie&12&04-03-2011\\
	6&Manifestation le 20 Mars au Maroc&12&15-02-2011\\
	6&Le "20 Fevrier" est il democratique&14&17-02-2011\\
	6&Si le Front national passe en 2012 ? ? ?&16&08-02-2011\\
    \midrule 	
	7&Oui ou Non pour la nouvelle constitution&10&20-05-2011\\
	7&J'aime notre Roi&10&29-06-2011\\
	7&Le roi du Maroc dans la boue !&10&24-05-2011\\
	7&Pourquoi un garçon a droit a plus d'héritages qu'une fille&11&27-08-2011\\
	7&Racisme biologique&12&07-04-2011\\
    \midrule 	
	8&Mariage à durée..déterminée !&10&07-09-2011\\
	8&Brandissons nos couleurs Rif......&10&19-11-2011\\
	8&J'ai peur de la présence musulmane au Québec&10&21-11-2011\\
	8&Bonne année 2961 et meilleurs voeux à tous!&10&12-12-2011\\
	8&La Grande Classe Marocaine....&11&22-09-2011\\  
  \bottomrule
  \end{tabular}
  \bigskip
  \caption{Liste des 5 premiers threads (par degré) de chacun des 8 clusters du graphe $G$}
\end{table*} 

Ceci étant fait, le graphe $G$ se voit divisé en 8 clusters distincts (Figure \ref{fig:gephi}, $\{\#1,...,\#8\}$), témoignant chacun d'un moment singulier de l'histoire de cette communauté. À nouveau, rappelons que cette segmentation ne prend pas en compte le contenu des threads, mais uniquement la structure du réseau. Le cluster \textit{\#1} couvre ainsi les premières années de $G$ (de Juillet 2004 à Septembre 2009) et le cluster \textit{\#5}, quant à lui, s'étend sur la période la plus courte, de Janvier à Septembre 2011. 

Afin de donner du sens à la lecture de ces clusters, nous procédons à une analyse manuelle du contenu de chacun des threads les composant. Avertissons simplement qu'il n'est pas aisé de résumer la diversité de ces conversations en une ou deux thématiques, cela n'a peut être même pas beaucoup de sens, aussi ce qui suit doit être pris avec précaution, de futures analyses viendront compléter cette première observation. La table \ref{tab:threads} présente pour chaque cluster, les sujets des 5 threads ayant agrégé le plus d'auteurs (en se basant sur le degré).

Le cluster \textit{\#1}, le plus long, est constitué de conversations sur les règles de fonctionnement internes au forum. Les groupes \textit{\#2} et \textit{\#3} agrègent des propos que l'on peut, faute de mieux, qualifier de quotidien pour ce forum. Le cluster \textit{\#4} marque le tournant des années 2010 à 2011, on note néanmoins une focalisation sur l'identité marocaine et des comparaisons entre le Maroc et ses voisins maghrébins. Mais soudainement, le cluster \textit{\#5} fait émerger une majorité de threads (19 sur 35) liés au Printemps Arabe et à la manifestation du 20 février. Puis, le cluster \textit{\#6} regroupe certains débats post-mouvement et diverses prises de position politiques. Le cluster \textit{\#7} traite de l'héritage politique du 20 février et du changement de constitution à venir. Enfin, avec le cluster \textit{\#8}, le forum retombe dans ses conversations quotidiennes.\\ 

\noindent Arrivée à cette étape, notre exploration des archives de \textit{yabiladi.com} nous suggère donc que le mouvement du 20 février 2011 n'a pas réellement été préparé de longue date sur le Web (ou tout du moins sur ce forum). Une étincelle a, soudainement et sans crier gare, embrasé une partie très minoritaire de \textit{yabiladi.com} : 94 utilisateurs actifs sur un total de 30,564. Cette vague a agrégé d'anciens membres établis du forum et de nouveaux venus en brisant leurs habitudes et la routine de leurs conversations quotidiennes. Mais, la mobilisation n'a pas duré dans le temps, ni donné lieu à une autre forme de protestation en ligne ou a une autre incarnation. La réforme de la constitution, menée par le roi du Maroc au printemps 2011, est venue clore ce moment révolutionnaire. Cette communauté d'utilisateurs est, en cela, une construction éphémère et aujourd'hui éteinte. 

En effet, la plupart de ses membres ont disparu du forum, au début de l'année 2012, quelques mois seulement après la première apparition de Twitter\footnote{\url{https://web.archive.org/web/20120627021244/www.yabiladi.com}} sur la page principale de \textit{yabiladi.com} (Figure \ref{fig:timeline}). Mais, sur les 94 auteurs de l'ensemble $E_0$, nous pouvons avancer qu'au moins 26 d'entre eux ont crée et utilisent aujourd'hui un compte Twitter avec le même avatar\footnote{Comptage et validation manuelle réalisés en Avril 2018}.

\section{Mener des recherches historiques depuis les archives Web}
\label{sec:6_moment}

\noindent Au regard des derniers éléments tirés de nos explorations, peut-être est il maintenant temps d'interroger le bien fondé d'études basées sur les archives Web comme matière historique. Qu'il s'agisse des auteurs de la blogosphère marocaine ou des membres de \textit{yabiladi.com}, tous ont, peu de temps  après le début des opérations de collecte de l'e-Diaspora, choisi de quitter ces territoires du Web pour migrer vers les réseaux sociaux. En explorant d'abord et avant tout ces corpus d'archives Web, ne sommes nous pas passés à côté d'un des aspects majeurs de notre problématique ?

\subsection{Face aux limites de l'archivage du Web}

\noindent La mise en place de notre moteur d'exploration (Section \ref{sec:4_moteur}) et la définition du fragment Web comme nouvelle unité d'analyse des archi\-ves Web (Section \ref{sec:5_fragment}) ont été guidées par l'idée, selon laquelle, un site Web devait faire l'objet de recherches historiques dédiées \citep{brugger_web_2017}. 

Ce faisant, nous montrons qu'en étudiant les archives Web sur une échelle de temps large (Section \ref{sec:6_blogs}) ou à travers un vaste espace d'explo\-ration (Section \ref{sec:6_printemps}), il est possible de comprendre et d'analyser historiquement le devenir de collectifs en ligne aujourd'hui disparus. Nous avons trouvé les indices d'une mutation des auteurs et des lecteurs du Web (blogueurs, commentateurs, membres de forum, ...) depuis les anciens blogs et les portails en ligne vers les nouvelles plateformes de réseaux sociaux (Facebook, Twitter, ...). 

Mais, partant de nos seuls corpus archivés, nous ne pouvons mettre en lumière l'intégralité du processus de mutation. Seules quelques traces éparses de cette transition (Section \ref{sec:6_blogs}), voire l'absence soudaine de ses dernières (Section \ref{sec:6_printemps}), restent aujourd'hui décelables. À ce niveau, force est de constater que nous touchons ici l'une des limites des archives Web. Il nous faut, dès lors, considérer l'idée que nos corpus peuvent être intrinsèquement incomplets et partiales. 

Cette incomplétude va même bien au delà des cécités et autres artéfacts de crawl hérités de la structure même des archives Web (Section \ref{sec:4_legacy}). Créées et pensées principalement au début des années 2000 \citep{masanes_web_2006}, les systèmes de préservation de la toile ont suivi avec succès les multiples évolutions du Web en tant que médium \citep{cho_evolution_1999,oita_archiving_2010,pop_archiving_2010}, mais pourtant, ils échouent encore à traduire le Web comme un écosystème \citep{brugger_website_2009}. 

Le Web vivant est un flot continu d'informations autant qu'un espace en perpétuel expansion. C'est un environnement de créations et de mutations où une multitude d'acteurs peuvent interagir organiquement. Les archives Web, elles, sont des collections figées de captures discrètes, stockées les unes à l'écart des autres. L'INA archive des données Twitter, la BNF collecte des blogs et Internet Archive préserve les premiers pas du Web mais sans jamais chercher à constituer des ponts entre chacun de ces corpus. Les archives des réseaux sociaux et les archives des sites Web classiques sont traitées comme deux entités séparées et irréconciliables. Quand bien même elles ont, toutes deux, été arrachées au même écosystème. 

Alors que nous cherchions à remonter la trace archivées des possibles conséquences du Printemps Arabe, les acteurs du Web (auteurs, lecteurs, visiteurs, ...) s'étaient, depuis longtemps déjà, détournés des blogs et des forums \citep{khondker_role_2011,lotan_arab_2011}. La problématique des collectifs en ligne éteints est, ainsi, moins une question de disparition, qu'une question de transition. Malheureusement, nos archives Web ne témoignent que du premier saut de cette transition.  

\subsection{Une micro-histoire}

\noindent Dans son \textit{Manifeste pour une pensée numérique de l'écrit}\footnote{Manifeste vidéo \url{https://youtu.be/8kFntt1l09o}} (2016), F. Bon questionne les mutations récentes de l'écriture sur support numérique, au regard notamment, des moments charnières qui ont jalonné l'histoire de l'écriture dite classique, celle du livre papier, des rouleaux et des tablettes. Ces "\textit{moments de transition où deux systèmes techniques se superposent}" et où le "\textit{saut mental}" induit est "\textit{mis à nu}"\footnote{Ibid, 5mn 30s} sont des axes de recherche privilégiés pour ressituer des textes et des œuvres numé\-riques dans l'histoire même de leur genèse. 

Aussi, l'histoire de l'écriture a-elle été ponctuée de moments clés, de moments de transition où tout l'écosystème de l'écrit (les modalités techniques, les supports techniques, les auteurs, les œuvres, les moyens de diffusion ...) pivote sur lui même et s'engage sur une nouvelle voie. L'historien L. Febvre note ainsi \citep{febvre_apparition_2013}, parmi d'autres moments : le passage de l'oral à l'écrit,  l'apparition des méta données sur les rouleaux de parchemins, l'invention de la recopie mécanique et de l'imprimerie, ...

Mais le Web et l'Internet en général possèdent-ils, eux aussi, de tels moments pivots ? Sur ce point, nous pouvons avancer que le Web s'inscrit déjà dans sa propre micro-histoire. À l'heure où ces lignes sont écrites, le Web fête ses 26 premières années d'existence (Section \ref{sec:2_web}) et son enfance tout autant que son adolescence ne nous ont jamais semblé aussi éloignées. La difficulté à étudier aujourd'hui les premières heures du Web n'est plus à démontrer \citep{gebeil_quand_2016} et, malgré les mesures de préservation mises en place, beaucoup de ressources originales (logiciels, matériels, documentations ...) sont désormais perdues \citep{helmond_historical_2017}. Le simple fait de vouloir remettre sur pied un site Web passé, dans les conditions matérielles de sa première mise en ligne, s'apparente de nos jours à un jeu de piste insoluble ou a un exercice d'archéologie expérimentale\footnote{\url{https://fr.wikipedia.org/wiki/Archéologie_expérimentale}} \citep{driscoll_searching_2017}. 

L'épopée de l'Arpanet \citep{russell_shadow_2014} et des réseaux antérieurs ou parallèles \citep{schafer_part_2015} à Internet font aujourd'hui figure de pré-histoire du Web. Dans leur enquête sur les pères fondateurs d'Internet, K. Hafner et M. Lyon mettent ainsi en lumière le devenir singulier de la liste de discussion \textit{MsgGroup} dans les années 80 :\\

\begin{fullwidth}
"\textit{The dialogue itself in the MsgGroup had always been more important than the results. Creating the mechanisms of e-mail mattered, of course, but the MsgGroup also created something else entirely -- a community of equal, many of whom had never met each other yet who carried on as if they had know each other all their lives. It was the first place they had found something they'd been looking for since the Arpanet came into existence. The MsgGroup was perhaps the first virtual community.}" --- \citep[p218]{hafner_where_1998}\\
\end{fullwidth} 

\noindent En outrepassant l'objet initial de cette liste\footnote{Le MsgGroup est une liste de diffusion créée en 1975, à laquelle souscrivait les spécialistes de l'e-mailing émergeant, dans le but de définir des bonnes pratiques, des normes, des standards, ...}, pour en faire un espace vibrant de discussions et d'échanges, les membres du MsgGroup ont préfiguré les forums, les communautés de blogs et les réseaux sociaux à venir \citep{stevenson_slashdot_2016,paloque-berges_quest-ce_2018}. L'histoire du Web est ainsi jalonnée de ces moments où tout bascule, de ces moments pivots : comme la mise en place des systèmes xDSL et ADSL\footnote{\url{https://fr.wikipedia.org/wiki/ADSL}} qui participent de la démocratisation du Web à la fin des années 90, ou l'émergence des systèmes de partages et d'échanges \textit{consumer gifts} \citep{giesler_consumer_2006} qui tracent des ponts entre économie du Web et du réel au début des années 2000, en passant par le développement des smartphones et du Web mobile au tournant des années 2010, ... 

\subsection{Les moments pivot du Web}

\noindent Si F. Bon ou J. Baschet utilisent tous les deux la notion de \textbf{moment} plutôt qu'\textit{instant} ou \textit{point} pour parler de moments de transitions entre systèmes ou de moments de rencontre de lignes processuelles, c'est que le moment est "\textit{une forme diversifiée du temps vécu}", une manière singulière de s'inscrire dans le temps et caractérisée par une "\textit{densité spécifique}" \citep[p.186]{baschet_defaire_2018}. 

Chez ces chercheurs, la notion de moment est issue de la conception chinoise du temps qui, contrairement à la pensée occidentale, envisage le temps comme une suite de moments (\textit{shí}) successifs, s'inscrivant dans la durée (\textit{jia}), à la fois distincts et liés entre eux \citep{chen_cerner_2011} et non comme un continuum abstrait de points élémentaires \citep[livre XI, chap. XV, 20, p.~195]{saint_augustin_confessions_1993}. Dans la pensée chinoise, le temps est un agencement de moments et de durées. Le moment s'éprouve ainsi qualitativement, ce n'est pas une valeur abstraite (un instant, une date, une heure, ...) mais le marqueur d'une temporalité concrète dont il faut faire l'expérience en la vivant (comme une saison ou une époque). L'intensité d'un moment (douce, tendue, brutale, ou diluée) varie tout autant que sa temporalité (brève, durable, étalée ou soudaine), permettant ainsi de différencier deux moments l'un de l'autre.

Mais pour nous, parler de moment, s'inscrit d'abord dans la continuité de notre discussion (Section \ref{sec:4_temporalite}) sur les diverses temporalités du Web et de ses archives. Nous avancions alors, qu'il pouvait être possible de conjuguer ensemble plusieurs fragments Web et de suivre leurs lignes processuelles, afin d'étudier de saisir l'histoire du Web et ses cristallisations autour de moments singuliers. Une histoire du Web qui s'attarderait sur ces moments, est une histoire qui s'attache à comprendre qualitativement des processus historiques d'étendue et de durée variable. Sur ce point, les sections \ref{sec:6_blogs} et \ref{sec:6_printemps} témoignent, à travers le prisme particulier des archives de l'e-Diaspora marocaine, du moment spécifique où certains acteurs de la toile ont choisi de quitter les territoires du Web 2.0 pour rejoindre le Web des plateformes de réseaux sociaux et satisfaire, par là même, leurs désirs et leurs besoins grandissant d'expression, d'organisation, de connexion et de partage de l'information en ligne. Nos archives Web (par leur contenu et par leur forme) sont ainsi chargée des derniers soubresauts d'un Web mourant \citep{stevenson_hypertext_2018} qui, lors de sa collecte, ne reflétait déjà plus la réalité de son temps \citep{helmond_platformization_2015}.

De fait, nous appelons \textbf{moment pivot du Web}, une époque de transition entre deux systèmes, un moment du Web où de nouveaux usages émergent et s'écartent des pratiques alors en vigueur sur la toile. C'est un moment d'instabilité où deux manières de vivre et d'expérimenter le Web se chevauchent. En cela, un moment pivot du Web nait de la rencontre de trois facteurs, soit : \textbf{(1)} la rencontre à un moment donné entre \textbf{(2)} une accélération technique et \textbf{(3)} des utilisateurs capables de s'en saisir et s'en emparer. La convergence de ces trois facteurs fait alors bifurquer le Web dans une direction inédite.

\begin{center}
	\textbf{***}
\end{center}

\noindent Là on se dit que l'exploration désagrégée c'est quand meme pas mal et que l'on peut étudier les archives autour de moments singuliers (Cf Section \ref{sec:5_dessous})

\noindent Là on commence à parler de la suite, du web que l'on souhaite archiver, de la neutralité des archives et de ce qui est archivé et des défis à venir de l'archivage. En fait 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Chapitre 7 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Au Delà des Archives Web}
\label{chap:7}

\noindent là on va essayer d'être synthétique, l'idée et de dire que contrairement au reste du manuscrit ces études sont toujours en cours et qu'elles sont moins "solitaires" qu'elles englobent plus de monde 

\section{Des archives centrées sur la navigation}

\noindent là on présente le travail fait à la BPI et notamment on mais l'accent sur une approche des archives par traces de navigation plutôt que traces des sites. On présente les 2 stages réalisés et les proto des étudiants paf (ou au moins les idées derrière) 

\noindent On décrit comment la trace de navigation sur certain sites nous permet de retrouver de l'information sur le public précère de la BPI

\section{Fouiller les archives du Web profond}

\noindent là on présente le travail fait pour calm, on insiste juste sur l'évolution de la base de données, qui nous apprend des choses sur les méthodes de travail de cette asso et de manière générale sur l'économie sociale et solidaire

\noindent on fait aussi un point sur les mots utilisés et le moment particulier de début septembre 2015 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Conclusion %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Conclusion}

\noindent bon et là il faudra penser à conclure, quand même (et à virer la mention chapitre 8)


%%
% The back matter contains appendices, bibliographies, indices, glossaries, etc.







\backmatter

\bibliography{biblio}
\bibliographystyle{apalike}


\printindex

\end{document}

